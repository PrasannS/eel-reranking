{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08cc2d4-f071-49a7-aee2-419f46a42521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 03:41:04.241566: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-31 03:41:04.241587: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from src.recom_search.model.beam_node_reverse import ReverseNode\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearLatticeBert, LinearPOSBert\n",
    "from encoding_utils import *\n",
    "import pickle\n",
    "import toy_helper as thelp\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from latmask_bert_models import LatticeBertModel\n",
    "import json\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from mask_utils import *\n",
    "from encoding_utils import *\n",
    "\n",
    "\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbart_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d571946-bf28-41bc-853e-897e78de7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Wrapper\n",
    "class LinearPOSBertV1(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = LatticeBertModel(AutoConfig.from_pretrained('bert-base-cased'))\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences, pos_ids=None, attmasks=None):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            if pos_ids==None:\n",
    "                word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "            else:\n",
    "                word_rep, sentence_rep = self.bert(sentences, position_ids=pos_ids, encoder_attention_mask=attmasks, attention_mask=attmasks, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "def prepare_dataset(resset):\n",
    "    x = []\n",
    "    y = []\n",
    "    for res in resset:\n",
    "        curinps = []\n",
    "        for r in res:\n",
    "            try:\n",
    "                toktmp = torch.tensor(bert_tok(clean_expanded(r)).input_ids)\n",
    "                #print(toktmp.shape)\n",
    "                if float(toktmp.shape[0])<MAX_LEN:\n",
    "                    toktmp = torch.cat([toktmp, torch.zeros(MAX_LEN-toktmp.shape[0])])\n",
    "                else:\n",
    "                    toktmp = toktmp[:MAX_LEN]\n",
    "                curinps.append(toktmp)\n",
    "            except:\n",
    "                print(\"weird error happened\") \n",
    "        print(len(curinps))\n",
    "        curouts = []\n",
    "        tinp = torch.stack(curinps).long().to(device)\n",
    "        print(tinp.shape)\n",
    "        y.append(posbmodel(tinp))\n",
    "        x.append(tinp)\n",
    "        \n",
    "        #print(\"error somewhere\")\n",
    "    return x, y\n",
    "\n",
    "def check_accuracy(setpred, setlabels):\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "    for i in range(0, len(setpred)):\n",
    "        ex = setpred[i]\n",
    "        for j in range(0, len(ex)):\n",
    "            if sum(setlabels[i][j])==0:\n",
    "                continue\n",
    "            elif torch.argmax(setlabels[i][j])==0:\n",
    "                continue\n",
    "            tot+=1\n",
    "            if torch.argmax(ex[j])==torch.argmax(setlabels[i][j]):\n",
    "                cor+=1\n",
    "    return cor/tot\n",
    "\n",
    "# correct posids\n",
    "def mod_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            if p[i]==0:\n",
    "                p[i] = i\n",
    "    return cop\n",
    "\n",
    "# set posids to default\n",
    "def def_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            p[i] = i\n",
    "    return cop\n",
    "\n",
    "def show_labels (pred):\n",
    "    res = []\n",
    "    for p in pred:\n",
    "        res.append(lablist[torch.argmax(p)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2989ad54-31ac-4880-968f-53f169904dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Load POS model, label vocabulary \n",
    "with open('./a3-distrib/lab_vocab.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "posbmodel = LinearPOSBertV1(len(list(labels.keys())))    \n",
    "t = torch.load(\"./a3-distrib/ckpt/posbert.pth\")\n",
    "posbmodel.load_state_dict(t)\n",
    "posbmodel.eval()\n",
    "print(torch.cuda.memory_allocated(\"cuda:2\"))\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e65225c6-cc3e-4560-b4b7-a832af033642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method that makes padding equal to 1\n",
    "def ones_padding(msk):\n",
    "    cop = msk\n",
    "    tmp = cop[0]<0\n",
    "    limit = tmp.nonzero()[0]\n",
    "    print(limit)\n",
    "    for i in range(0, len(msk)):\n",
    "        for j in range(limit, len(msk[0])):\n",
    "            cop[i][j] = 1\n",
    "            cop[j][i] = 1\n",
    "    return cop\n",
    "\n",
    "def run_pipeline(inplist):\n",
    "    # construct data structure for toy graph in format used on actual examples\n",
    "    toygraph = thelp.create_toy_graph(inplist, mbart_tok)\n",
    "\n",
    "    # get list of exploded candidates using same algorithm from numbers\n",
    "    exploded = fl.get_all_possible_candidates(toygraph)\n",
    "\n",
    "    # get a flattened version of toy lattice (same method as on actual examples)\n",
    "    flat_toy = fl.flatten_lattice(toygraph)\n",
    "\n",
    "    # generate mask (uses same method as actual examples), convert to -inf mask (seems to not do anything)\n",
    "    mask = connect_mat(flat_toy)\n",
    "    mask[mask==0] = -float('inf')\n",
    "    mask = ones_padding(mask)\n",
    "    \n",
    "\n",
    "    # get gold labels for the exploded set\n",
    "    dsetx, dsety = prepare_dataset([exploded])\n",
    "\n",
    "    assert len(dsetx)==1\n",
    "\n",
    "    # from encoding utils, get posids and relevant tokens\n",
    "    sents, posids = create_inputs([flat_toy])\n",
    "    \n",
    "    # get gold label dictionaries for tokens in example, based on averages of tokens on dsety\n",
    "    _ , tmaps = lattice_pos_goldlabels(dsetx, dsety, sents)\n",
    "\n",
    "    # generate gold y labels using tmaps and \n",
    "    latposylabels = tmap_pos_goldlabels(tmaps, sents)\n",
    "\n",
    "    # get generated labels for flattened lattice, def_posids can be used for default posids\n",
    "    # params start as (sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "    # posids, mask can be set to None to ablate to default\n",
    "    pred = posbmodel(sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "    #pred = posbmodel(sents.to(device), mod_posids(posids).to(device), None)\n",
    "    return pred, latposylabels, tmaps, sents, posids, dsetx, dsety, flat_toy, mask\n",
    "\n",
    "lablist = [k for k in labels.keys()]\n",
    "def print_results(CUTOFF):\n",
    "\n",
    "    # sanity check to look at flat lattice \n",
    "    p = flat_toy\n",
    "    tlist = fl.get_toklist(p)\n",
    "    res = \"\"\n",
    "    for s in tlist:\n",
    "        res = res+\" \"+bert_tok.decode(s)\n",
    "    decstr = res\n",
    "\n",
    "    # number of tokens, the tokens that are passed into model for lattice\n",
    "    print(\"INPUT\")\n",
    "    print(decstr)\n",
    "\n",
    "    print(\"PREDICTED\")\n",
    "    print(show_labels(pred[0])[:CUTOFF])\n",
    "    print(\"GOLD\")\n",
    "    print(show_labels(latposylabels[0])[:CUTOFF])\n",
    "    \n",
    "    # run explodeds through model\n",
    "    indivlabs = posbmodel(dsetx[0])\n",
    "    print(\"\")\n",
    "    print(\"Exploded paths\")\n",
    "    # show labels for s1, s2 when run through individually\n",
    "    for i in range(len(inputlist)):\n",
    "        print(inputlist[i])\n",
    "        print(show_labels(indivlabs[i])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a628828-df45-46ab-992b-fe662c079a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19])\n",
      "3\n",
      "torch.Size([3, 500])\n",
      "3\n",
      "0\n",
      "0\n",
      "Accuracy\n",
      "1.0\n",
      "INPUT\n",
      " The Fed raises interest rates . interest among the economists . the children of the future .\n",
      "PREDICTED\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', '.', 'NN', 'IN', 'DT', 'NNS', '.', 'DT', 'NNS', 'IN', 'DT', 'NN', '.', '.', '<pad>']\n",
      "GOLD\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', '.', 'NN', 'IN', 'DT', 'NNS', '.', 'DT', 'NNS', 'IN', 'DT', 'NN', '.', '.', '<pad>']\n",
      "\n",
      "Exploded paths\n",
      "The Fed raises interest rates.\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', '.', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "The Fed raises interest among the economists .\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'IN', 'DT', 'NNS', '.', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "The Fed raises the children of the future .\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'NN', '.', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# get a list of input strings of the format where the start w/ the same pre-fix but have different endings\n",
    "inputlist = [\n",
    "    \"The Fed raises interest rates.\",\n",
    "    \"The Fed raises interest among the economists .\",\n",
    "    \"The Fed raises the children of the future .\"\n",
    "]\n",
    "\n",
    "pred, latposylabels, tmaps, sents, posids, dsetx, dsety, flat_toy, mask = run_pipeline(inputlist)\n",
    "#mask[mask==0] = -float('inf')\n",
    "\n",
    "# accuracy (assumes that gold is good, which isn't confirmed here)\n",
    "print(\"Accuracy\")\n",
    "print(check_accuracy(pred, latposylabels))\n",
    "# input is number of toks to print\n",
    "print_results(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7d5853f-ee08-49f8-b274-362e5dca3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mask[0]<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7c11f-f40e-4a38-a176-3951ce26d9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c66d86d-eb3a-4d66-9090-f7d45cfd2b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., -inf, -inf, -inf, 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c485f-26e2-496d-b193-6d5f321b022b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
