{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08cc2d4-f071-49a7-aee2-419f46a42521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 15:56:55.518745: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-06 15:56:55.518771: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from src.recom_search.model.beam_node_reverse import ReverseNode\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearLatticeBert, LinearPOSBert\n",
    "from encoding_utils import *\n",
    "import pickle\n",
    "import toy_helper as thelp\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from latmask_bert_models import LatticeBertModel\n",
    "import json\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from mask_utils import *\n",
    "from encoding_utils import *\n",
    "\n",
    "\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbart_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d571946-bf28-41bc-853e-897e78de7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Wrapper\n",
    "class LinearPOSBertV1(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = LatticeBertModel(AutoConfig.from_pretrained('bert-base-cased'))\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences, pos_ids=None, attmasks=None):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            word_rep, sentence_rep = self.bert(sentences, position_ids=pos_ids, encoder_attention_mask=attmasks, attention_mask=attmasks, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "class LinearPOSBertCheck(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences, pos_ids=None, attmasks=None):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            word_rep, sentence_rep = self.bert(sentences, position_ids=pos_ids, encoder_attention_mask=attmasks, attention_mask=attmasks, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "def prepare_dataset(resset):\n",
    "    x = []\n",
    "    y = []\n",
    "    for res in resset:\n",
    "        \n",
    "        cleaned = [clean_expanded(r) for r in res]\n",
    "        inputs = bert_tok(cleaned, padding=\"max_length\", max_length=500, return_tensors='pt').to(device)\n",
    "\n",
    "        y.append(posbmodel(inputs.input_ids, attmasks = inputs.attention_mask))\n",
    "        x.append(inputs.input_ids)\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "def check_accuracy(setpred, setlabels):\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "    for i in range(0, len(setpred)):\n",
    "        ex = setpred[i]\n",
    "        for j in range(0, len(ex)):\n",
    "            if sum(setlabels[i][j])==0:\n",
    "                continue\n",
    "            elif torch.argmax(setlabels[i][j])==0:\n",
    "                continue\n",
    "            tot+=1\n",
    "            if torch.argmax(ex[j])==torch.argmax(setlabels[i][j]):\n",
    "                cor+=1\n",
    "    return cor/tot\n",
    "\n",
    "# correct posids\n",
    "def mod_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            if p[i]==0:\n",
    "                p[i] = i\n",
    "    return cop\n",
    "\n",
    "# set posids to default\n",
    "def def_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            p[i] = i\n",
    "    return cop\n",
    "\n",
    "def show_labels (pred):\n",
    "    res = []\n",
    "    for p in pred:\n",
    "        res.append(lablist[torch.argmax(p)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8391d221-08a9-428a-bcfa-15e37dce1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load POS model, label vocabulary \n",
    "with open('./lab_vocab.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13e8265-4ddb-43fe-a92c-166c27bf5dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c80034e-358c-4157-aea3-f8b331742738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867814400\n"
     ]
    }
   ],
   "source": [
    "posbmodel = LinearPOSBertCheck(len(list(labels.keys())))    \n",
    "t = torch.load(\"./a3distrib/ckpt/bertonewayv1.pth\")\n",
    "posbmodel.load_state_dict(t)\n",
    "posbmodel.eval()\n",
    "print(torch.cuda.memory_allocated(\"cuda:0\"))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65225c6-cc3e-4560-b4b7-a832af033642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method that makes padding equal to 1\n",
    "from mask_utils import ones_padding\n",
    "\n",
    "def run_pipeline(inplist, resarrs = None, flat = None):\n",
    "    # construct data structure for toy graph in format used on actual examples\n",
    "    if flat==None:\n",
    "        toygraph = thelp.create_toy_graph(inplist, mbart_tok)\n",
    "\n",
    "        # get list of exploded candidates using same algorithm from numbers\n",
    "        exploded = fl.get_all_possible_candidates(toygraph)\n",
    "\n",
    "        # get a flattened version of toy lattice (same method as on actual examples)\n",
    "        flat_toy = fl.flatten_lattice(toygraph)\n",
    "    else:\n",
    "        flat_toy = flat\n",
    "        exploded = resarrs\n",
    "\n",
    "    # generate mask (uses same method as actual examples), convert to -inf mask (seems to not do anything)\n",
    "    mask = connect_mat(flat_toy)\n",
    "    mask = torch.triu(mask)\n",
    "    #mask[mask==0] = -float('inf')\n",
    "    #mask = ones_padding(mask)\n",
    "    \n",
    "\n",
    "    # get gold labels for the exploded set\n",
    "    dsetx, dsety = prepare_dataset([exploded])\n",
    "\n",
    "    assert len(dsetx)==1\n",
    "\n",
    "    # from encoding utils, get posids and relevant tokens\n",
    "    sents, posids = create_inputs([flat_toy])\n",
    "    \n",
    "    # get gold label dictionaries for tokens in example, based on averages of tokens on dsety\n",
    "    _ , tmaps = lattice_pos_goldlabels(dsetx, dsety, sents)\n",
    "\n",
    "    # generate gold y labels using tmaps and \n",
    "    latposylabels = tmap_pos_goldlabels(tmaps, sents)\n",
    "\n",
    "    # get generated labels for flattened lattice, def_posids can be used for default posids\n",
    "    # params start as (sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "    # posids, mask can be set to None to ablate to default\n",
    "    pred = posbmodel(sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "    #pred = posbmodel(sents.to(device), None, None)\n",
    "    return pred, latposylabels, tmaps, sents, posids, dsetx, dsety, flat_toy, mask\n",
    "\n",
    "lablist = [k for k in labels.keys()]\n",
    "def print_results(CUTOFF):\n",
    "\n",
    "    # sanity check to look at flat lattice \n",
    "    p = flat_toy\n",
    "    tlist = fl.get_toklist(p)\n",
    "    res = \"\"\n",
    "    for s in tlist:\n",
    "        res = res+\" \"+bert_tok.decode(s)\n",
    "    decstr = res\n",
    "\n",
    "    # number of tokens, the tokens that are passed into model for lattice\n",
    "    print(\"INPUT\")\n",
    "    print(decstr)\n",
    "\n",
    "    print(\"PREDICTED\")\n",
    "    print(show_labels(pred[0])[:CUTOFF])\n",
    "    print(\"GOLD\")\n",
    "    print(show_labels(latposylabels[0])[:CUTOFF])\n",
    "    \n",
    "    # run explodeds through model\n",
    "    indivlabs = posbmodel(dsetx[0])\n",
    "    print(\"\")\n",
    "    print(\"Exploded paths\")\n",
    "    # show labels for s1, s2 when run through individually\n",
    "    for i in range(len(inputlist)):\n",
    "        print(inputlist[i])\n",
    "        print(show_labels(indivlabs[i])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a628828-df45-46ab-992b-fe662c079a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "0\n",
      "Accuracy\n",
      "0.03\n",
      "INPUT\n",
      " The Fed raises interest rates . interest among the economists . the children of the future .\n",
      "PREDICTED\n",
      "['NNS', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', ':', 'NN', 'IN', 'DT', 'NNS', ':', 'DT', 'NNS', 'IN', 'DT', 'JJ', '.', '``', '``', '``', '``', '``', '``', '``', '``']\n",
      "GOLD\n",
      "['NNP', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', '.', 'NN', 'IN', 'DT', 'NNS', '.', 'DT', 'NNS', 'IN', 'DT', 'JJ', '.', '.', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']\n",
      "\n",
      "Exploded paths\n",
      "The Fed raises interest rates.\n",
      "['IN', 'DT', 'NNP', 'VBZ', 'NN', 'NNS', 'IN', '``', '``', '``', '``', '``', 'RBS', 'RBS', 'RBS', 'RBS', 'JJS', 'JJS', 'JJS', 'JJS']\n",
      "The Fed raises interest among the economists .\n",
      "['IN', 'DT', 'NNP', 'VBZ', 'NN', 'IN', 'DT', 'NNS', 'IN', '``', 'IN', '``', '``', 'RBS', 'JJS', 'RBS', 'JJS', 'JJS', 'JJS', 'JJS']\n",
      "The Fed raises the children of the future .\n",
      "['IN', 'DT', 'NNP', 'VBZ', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'IN', '``', 'IN', '``', '``', '``', '``', '``', 'JJS', 'JJS', 'JJS']\n"
     ]
    }
   ],
   "source": [
    "# get a list of input strings of the format where the start w/ the same pre-fix but have different endings\n",
    "inputlist = [\n",
    "    \"The Fed raises interest rates.\",\n",
    "    \"The Fed raises interest among the economists .\",\n",
    "    \"The Fed raises the children of the future .\"\n",
    "]\n",
    "\n",
    "pred, latposylabels, tmaps, sents, posids, dsetx, dsety, flat_toy, mask = run_pipeline(inputlist)\n",
    "#mask[mask==0] = -float('inf')\n",
    "\n",
    "# accuracy (assumes that gold is good, which isn't confirmed here)\n",
    "print(\"Accuracy\")\n",
    "print(check_accuracy(pred, latposylabels))\n",
    "# input is number of toks to print\n",
    "print_results(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0565317b-bb42-409e-8244-974844eb952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNP'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lablist[torch.argmax(tmaps[0]['0'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a78d2d-6bd7-4ab6-8f71-573fb9dcae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlist = [\n",
    "    \"I like you\",\n",
    "    \"I like him\",\n",
    "]\n",
    "\n",
    "# construct data structure for toy graph in format used on actual examples\n",
    "toygraph = thelp.create_toy_graph(inputlist, mbart_tok)\n",
    "\n",
    "# get list of exploded candidates using same algorithm from numbers\n",
    "exploded = fl.get_all_possible_candidates(toygraph)\n",
    "\n",
    "# get a flattened version of toy lattice (same method as on actual examples)\n",
    "flat_toy = fl.flatten_lattice(toygraph)\n",
    "\n",
    "# generate mask (uses same method as actual examples), convert to -inf mask (seems to not do anything)\n",
    "mask = connect_mat(flat_toy)\n",
    "#mask[mask==0] = -float('inf')\n",
    "#mask = ones_padding(mask)\n",
    "\n",
    "\n",
    "# get gold labels for the exploded set\n",
    "dsetx, dsety = prepare_dataset([exploded])\n",
    "\n",
    "assert len(dsetx)==1\n",
    "\n",
    "# from encoding utils, get posids and relevant tokens\n",
    "sents, posids = create_inputs([flat_toy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3f7d1-3137-4a71-81e2-5318587b7876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_posids(posids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e5c128-0d18-4ba9-846d-563c4f82b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# get gold label dictionaries for tokens in example, based on averages of tokens on dsety\n",
    "_ , tmaps = lattice_pos_goldlabels(dsetx, dsety, sents)\n",
    "\n",
    "# generate gold y labels using tmaps and \n",
    "latposylabels = tmap_pos_goldlabels(tmaps, sents)\n",
    "\n",
    "# get generated labels for flattened lattice, def_posids can be used for default posids\n",
    "# params start as (sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "# posids, mask can be set to None to ablate to default\n",
    "pred = posbmodel(sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))#, mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "#pred = posbmodel(sents.to(device), mod_posids(posids).to(device), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b12621c-183e-49d7-a8e0-1d3026955e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-way sanity check\n",
    "\n",
    "def subset(inmasks, sets, pids, newdim):\n",
    "    inmasks = torch.tril(inmasks)\n",
    "    m = torch.stack([inmasks])[:, :newdim, :newdim].to(device)\n",
    "    s = sents[:, :newdim].to(device)\n",
    "    p = mod_posids(pids)[:, :newdim].to(device)\n",
    "    return m, s, p\n",
    "\n",
    "def get_norminp(inputsents, lim):\n",
    "    isents = torch.stack([inputsents[:lim]]).to(device)\n",
    "    ipids = torch.stack([torch.arange(0, lim)]).to(device)\n",
    "    imasks = torch.stack([torch.tril(torch.ones((lim, lim)))]).to(device)\n",
    "    return isents, ipids, imasks\n",
    "\n",
    "loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a08f8d-0ffd-4208-ac24-250c717b8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get with lattice\n",
    "slmask, slsent, slpid = subset(mask, sents, mod_posids(posids), 6)\n",
    "#slmask[0][4] = 0\n",
    "#slmask[0][4][0] = 1\n",
    "# get with other\n",
    "pred = posbmodel(slsent, slpid, slmask)#, mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "# get with normal model (1 example)\n",
    "ns, np, nm = get_norminp(dsetx[0][0], 5)\n",
    "normpred = posbmodel(ns, np, nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7b6251-5810-4cdc-9855-0a4694117e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns1, np1, nm1 = get_norminp(dsetx[0][1], 5)\n",
    "normpred1 = posbmodel(ns1, np1, nm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f01c05-aa48-46ac-94ca-0f09634946e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1176, 1128,  102]], device='cuda:0')\n",
      "tensor([[ 101,  146, 1176, 1128, 1140,  102]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ns)\n",
    "print(slsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfdd0f95-4980-4421-8089-e1e443d0816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 1., 0.],\n",
      "         [1., 1., 1., 1., 1., 1.]]], device='cuda:0')\n",
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(slmask)\n",
    "print(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ecb9235-7721-4db0-8adb-2a572b639e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check loss (should be near 0 or just 0)\n",
    "common = 4\n",
    "loss(normpred[:, :common], pred[:, :common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2654f0-e0b6-48b4-9956-1a10ed05e9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2220e+01, -8.8262e+00, -7.7302e+00, -8.5825e-01, -2.2456e+00,\n",
       "         2.0531e+00, -4.2497e+00,  3.1990e+00, -1.3949e+01, -3.5714e+00,\n",
       "        -1.3353e+00, -3.8779e+00,  3.3536e-03,  7.5576e-01, -7.3095e+00,\n",
       "        -1.0389e+01, -3.4822e+00,  2.4121e+00, -1.1164e+01, -9.7251e+00,\n",
       "         3.5669e-02, -3.0768e+00,  6.6038e+00, -8.7559e-01,  2.1208e+00,\n",
       "         1.7764e+00, -4.3281e+00, -2.5379e+00, -4.2568e+00, -4.4116e+00,\n",
       "        -9.9575e+00, -5.0940e+00, -3.9813e+00, -2.1043e+00, -1.1959e+01,\n",
       "        -7.3502e+00, -9.1080e+00, -1.1939e+01, -1.5407e+00, -3.1237e+00,\n",
       "        -7.1442e+00, -1.1285e+01, -1.8101e+01, -7.2481e+00, -4.3044e+00,\n",
       "        -1.3759e+01], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a366b6f-f8e6-4da7-a52b-85b05a2b718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2220e+01, -8.8262e+00, -7.7302e+00, -8.5825e-01, -2.2456e+00,\n",
       "         2.0531e+00, -4.2497e+00,  3.1990e+00, -1.3949e+01, -3.5714e+00,\n",
       "        -1.3353e+00, -3.8779e+00,  3.3541e-03,  7.5576e-01, -7.3095e+00,\n",
       "        -1.0389e+01, -3.4822e+00,  2.4121e+00, -1.1164e+01, -9.7251e+00,\n",
       "         3.5670e-02, -3.0768e+00,  6.6038e+00, -8.7559e-01,  2.1208e+00,\n",
       "         1.7764e+00, -4.3281e+00, -2.5379e+00, -4.2568e+00, -4.4116e+00,\n",
       "        -9.9575e+00, -5.0940e+00, -3.9813e+00, -2.1043e+00, -1.1959e+01,\n",
       "        -7.3502e+00, -9.1080e+00, -1.1939e+01, -1.5407e+00, -3.1237e+00,\n",
       "        -7.1442e+00, -1.1285e+01, -1.8101e+01, -7.2481e+00, -4.3044e+00,\n",
       "        -1.3759e+01], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normpred1[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e458c9f0-f4c9-4a22-9f4c-142ebd143301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 46])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normpred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76bbd2-f187-4f9a-9fb2-97e0e2ce504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb4ead3-9543-4b5c-adde-6793712675d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.2855,  -2.2870,  18.7948,  -2.2742,  -2.8717,   1.1202,   4.6252,\n",
      "         -2.1317,  -1.4885,   7.3869,  -7.4286,  -0.1274,  -2.4628,  -1.2159,\n",
      "         -5.3879,  -2.3283,  -6.0379,  -1.8584,  -8.7300,  10.5653,  -0.5551,\n",
      "         -8.0439, -15.2517, -23.2961,  -0.0896,  -5.2870,  -6.8535,  -3.0583,\n",
      "         -8.9975,  -5.2218,  -9.0417,  -6.5071, -11.6533,  -5.4137, -11.1530,\n",
      "        -10.1047, -11.4400,  -3.5278,  -4.4020,  -7.9905, -11.9993,  -2.5205,\n",
      "        -13.1899, -16.5361, -14.9493, -24.4979], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(normpred[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a95e4e-2241-4ec6-9256-d4a29c474226",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnobatch = posbmodel(shortsents.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08f40c95-3276-4802-b7a7-071ab921d5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pnobatch[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fdbcdee-1d3d-4f2a-8e6c-01479038885e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4797002-5274-4f73-b35a-6755f5a3f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = []\n",
    "for p in pred[0]:\n",
    "    pl.append(int(torch.argmax(p)))\n",
    "\n",
    "pnl = []\n",
    "for pn in pnobatch[0]:\n",
    "    pnl.append(int(torch.argmax(pn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f67d9f6-67b1-4564-9fdf-4470b7ffae4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pnl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mpnl\u001b[49m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss(pred[\u001b[38;5;241m0\u001b[39m][i], pnobatch[\u001b[38;5;241m0\u001b[39m][i]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pnl' is not defined"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "for i in range(len(pnl)):\n",
    "    print(loss(pred[0][i], pnobatch[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df0055fc-80f8-44bf-a554-e3a6aadab5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (2, 2), (3, 3), (18, 18), (5, 5), (10, 10), (17, 27), (5, 5), (1, 1), (2, 2), (10, 10), (17, 27), (2, 2), (10, 10), (1, 1), (2, 2), (23, 5), (17, 17), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(pl[:len(pnl)], pnl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1bf0e14-2076-40ba-8bdb-678ef5f1a2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 44])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5352b2e-2d65-4476-816e-9ee2cc5c1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "ids = bert_tok(\"How is it going my boy.\", padding=\"max_length\", max_length=500, return_tensors='pt').to(device)\n",
    "out = posbmodel(ids.input_ids, attmasks=ids.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb2ece0-6f75-4021-ae18-a7b28a915cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shids = bert_tok(\"How is it going my boy.\", return_tensors='pt').to(device)\n",
    "shout = posbmodel(shids.input_ids, attmasks=shids.attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18966840-6b9e-4397-8bbd-6aa92f298c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9396e-12, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(out[0][:9][0], shout[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218b54cf-8e76-4344-ae77-8900c01145cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mout\u001b[49m[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m9\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(shout[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "print(out[0][:9][0])\n",
    "print(shout[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7088b5-1f39-47c6-b1b6-685a4688fee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 44])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][:9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811237ae-7a7a-443b-9c78-03ec8b80894a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 44])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c379182-8604-4e01-b488-2af1320b5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pgraphs = None\n",
    "with open('./torchsaved/pgraphsall.pkl', 'rb') as file:\n",
    "    pgraphs = pickle.load(file)\n",
    "    \n",
    "resarrs = None\n",
    "with open('./torchsaved/resarrsall.pkl', 'rb') as file:\n",
    "    resarrs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d3bbff-06e6-44d5-99e1-6de696c79218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "0\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "missing token\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "IND = 87\n",
    "pred, latposylabels, tmaps, sents, posids, dsetx, dsety, flat_toy, mask = run_pipeline(None, resarrs[IND], pgraphs[IND])\n",
    "#mask[mask==0] = -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53e429-5277-4720-8e40-f6ba3aac0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_efficient(pgraph, lsplit, curind, tracking, res):\n",
    "    cur = pgraph[curind]\n",
    "    # base case\n",
    "    if len(cur['nexts'])==0:\n",
    "        res.append(bert_tok.decode(tracking))\n",
    "        return\n",
    "    \n",
    "    nextl = cur['nexts']\n",
    "    ind = curind\n",
    "    # do this to prevent stack overflow\n",
    "    while len(nextl)==1:\n",
    "        if pgraph[ind]['id']==nextl[0]:\n",
    "            res.append(pgraph[ind]['token_idx'])\n",
    "            nextl = \n",
    "    # reset after recursion, double check this\n",
    "    res = res[:ind]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "154e1d13-fb6b-4d57-a160-49ea8b013f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token list that tracks back a single path from start\n",
    "def find_backs(start, fgraph):\n",
    "    cur = start\n",
    "    nid = fgraph[cur]['id']\n",
    "    res = []\n",
    "    while cur>=0:\n",
    "        cur = cur-1\n",
    "        if nid in fgraph[cur]['nexts']:\n",
    "            res = [fgraph[cur]['token_idx']] + res\n",
    "            nid = fgraph[cur]['id']\n",
    "    return res\n",
    "\n",
    "# get token list that tracks canonical forward\n",
    "def find_fronts(start, fgraph):\n",
    "    nid = fgraph[start]['nexts'][0]\n",
    "    res = []\n",
    "    for i in range(start, len(fgraph)):\n",
    "        if nid == fgraph[i]['id']:\n",
    "            res = res + [fgraph[i]['token_idx']]\n",
    "            nextmp = fgraph[i]['nexts']\n",
    "            if len(nextmp)==0:\n",
    "                return res\n",
    "            nid = nextmp[0]\n",
    "\n",
    "    return res\n",
    "\n",
    "tgraph = pgraphs[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e096124a-7ff0-471d-8c6f-b0eec30fb748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgraph[1]['token_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "473ed92b-6896-469c-86da-0d030f2e6999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_idx': 1104,\n",
       " 'pos': 5,\n",
       " 'id': '1104 5',\n",
       " 'nexts': ['102 6', '1697 6', '20796 6', '1103 6', '25194 6', '19866 6'],\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgraph[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02b62a36-3d06-4d23-a678-68e05f87e30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'days before the trial of'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tok.decode(find_fronts(0, tgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d085be1d-1f8f-44b1-bd01-cfde19e367ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.384297520661157\n",
      "INPUT\n",
      " Two days before the trial of President Mohamed Mo ##rs ##i was to begin , they went down to the street . s . the street . s . , who has been deposed , they went ou ##sted President Mohamed Mo ##rs ##i began , the two men went down to the street . s . they were out on the street . s . went to the street . s . on the street . s . into the street . s . out on the took to the street . s . , the two men went down to they were out on the street . s . took to the street . s . went into the street . s . out on the the outgoing President Mohamed Mo ##rs ##i began , the , the two they were took went into out down to the the street . s . deposed President Mohamed Mo ##rs ##i opens , the two they have taken to the street . s . took went out was due to start , they took went open , they took went begin , the two they were took went out to open , they took went begin , the two opened , the two they were on the street . s . took went to on into out into the street . s . began they went down the street . s . , the outgoing deposed Mohamed Mo ##rs ##i , they went down President\n",
      "PREDICTED\n",
      "['NNP', 'CD', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNS', 'NN', 'VBD', 'TO', 'VB', ',', 'PRP', 'VBD', 'IN', 'TO', 'DT', 'NN', 'NN', 'POS', 'NN', 'DT', 'NN', '.', 'IN', 'NN', ',', 'WP', 'VBZ', 'VBN', 'VBN', ',', 'PRP', 'VBD', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', ',', 'DT', 'CD', 'NNS', 'VBD', 'IN', 'TO', 'DT', 'NN', '.', 'POS', '.', 'PRP', 'IN', 'IN', 'IN', 'DT', 'NN', 'NN', 'POS', '.', 'VBD', 'TO', 'DT', 'NN', '.', 'POS', 'NN', 'IN', 'DT', 'NN', '.', 'POS', 'NN', 'IN', 'DT', 'NN', '.', 'POS', 'NN', 'IN', 'IN', 'DT', 'VBD', 'TO', 'DT', 'NN', '.', 'POS', '.', ',', 'DT', 'CD', 'NNS', 'VBD', 'IN', 'TO', 'PRP', 'VBD', 'IN', 'IN', 'DT', 'NN', 'NN', 'POS', '.', 'VBD', 'TO', 'DT', 'NN', '.', 'POS', '.', 'VBD', 'IN', 'DT', 'NN', '.', 'POS', '.', 'IN', 'IN', 'DT', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', ',', 'DT', ',', 'DT', 'CD', 'PRP', 'VBD', 'VBD', 'VBD', 'IN', 'RP', 'IN', 'TO', 'DT', 'DT', 'NN', '.', 'POS', '.', 'VBN', 'NNP', 'NNP', 'NNP', 'NNS', 'NN', 'NNS', ',', 'DT', 'CD', 'PRP', 'VB', 'VBN', 'TO', 'DT', 'NN', 'NNP', 'NNS', '.', 'VBD', 'VBD', 'IN', 'VBD', 'IN', 'TO', 'VB', ',', 'PRP', 'VBD', 'VBD', 'JJ', ',', 'PRP', 'VBD', 'VBD', 'VB', ',', 'DT', 'CD', 'PRP', 'VBN', 'IN', 'VBD', 'IN', 'TO', 'JJ', ',', 'PRP', 'VBN', 'VBD', 'VBP', ',', 'DT', 'CD', 'VBD', ',', 'DT', 'CD', 'PRP', 'IN', 'IN', 'DT', 'NN', 'NNP', 'POS', '.', 'VBD', 'VBD', 'TO', 'IN', 'IN', 'IN', 'IN', 'DT', 'NN', ':', 'POS', ':', 'VBD', 'PRP', 'VBD', 'RB', 'DT', 'NN', '.', 'POS', ':', ',', 'DT', 'JJ', 'VBN', 'NNP', 'NNP', 'NNS', 'NNP', ',', 'PRP', 'VBD', 'IN', 'NNP', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``', '``']\n",
      "GOLD\n",
      "['IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', 'TO', 'VB', ',', 'PRP', 'VBD', 'IN', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'DT', 'NN', 'VBD', '<pad>', 'VBD', ',', 'WP', 'VBZ', 'VBN', 'VBN', ',', 'PRP', 'VBD', 'JJ', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', ',', 'DT', 'CD', 'NNS', 'VBD', 'IN', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'PRP', 'VBD', 'IN', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'IN', 'IN', 'DT', 'VBD', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', ',', 'DT', 'CD', 'NNS', 'VBD', 'IN', 'TO', 'PRP', 'VBD', 'IN', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'IN', 'IN', 'DT', 'DT', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', ',', 'DT', ',', 'DT', 'CD', 'PRP', 'VBD', 'VBD', 'VBD', 'IN', 'IN', 'IN', 'TO', 'DT', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VBZ', ',', 'DT', 'CD', 'PRP', 'VBP', 'VBN', 'TO', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'VBD', 'IN', 'VBD', 'VBG', 'TO', 'VB', ',', 'PRP', 'VBD', 'VBD', 'VB', ',', 'PRP', 'VBD', 'VBD', 'VB', ',', 'DT', 'CD', 'PRP', 'VBD', 'VBD', 'VBD', 'IN', 'TO', 'VB', ',', 'PRP', 'VBD', 'VBD', 'VB', ',', 'DT', 'CD', 'VBD', ',', 'DT', 'CD', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'VBD', 'TO', 'IN', 'IN', 'IN', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', 'VBD', 'PRP', 'VBD', 'IN', 'DT', 'NN', 'VBD', '<pad>', 'VBD', ',', 'DT', 'JJ', 'VBN', 'NNP', 'NNP', 'NNP', 'NNP', ',', 'PRP', 'VBD', 'IN', 'NNP', 'VBD', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP']\n",
      "\n",
      "Exploded paths\n",
      "The Fed raises interest rates.\n",
      "['IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'TO', 'VB', ',', 'PRP', 'VBD', 'IN', 'TO']\n",
      "The Fed raises interest among the economists .\n",
      "['IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'TO', 'VB', ',', 'PRP', 'VBD', 'IN', 'TO']\n",
      "The Fed raises the children of the future .\n",
      "['IN', 'DT', 'NNS', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'TO', 'VB', ',', 'PRP', 'VBD', 'RP', 'DT']\n"
     ]
    }
   ],
   "source": [
    "# accuracy (assumes that gold is good, which isn't confirmed here)\n",
    "print(\"Accuracy\")\n",
    "print(check_accuracy(pred, latposylabels))\n",
    "# input is number of toks to print\n",
    "print_results(298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1086a8f0-e826-43d7-b556-61146594b9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lablist[torch.argmax(tmaps[0]['119'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7550a3eb-58ff-4f13-abb7-5fd07e617f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]   <cls>\n",
      "Two   CD\n",
      "days   NNS\n",
      "before   IN\n",
      "the   DT\n",
      "trial   NN\n",
      "of   IN\n",
      "President   NNP\n",
      "Mohamed   NNP\n",
      "Mo   NNP\n",
      "##rs   NNP\n",
      "##i   NNP\n",
      "was   VBD\n",
      "to   TO\n",
      "begin   VB\n",
      ",   ,\n",
      "they   PRP\n",
      "went   VBD\n",
      "down   IN\n",
      "street   NN\n",
      ".   .\n",
      "[SEP]   <sep>\n",
      "[PAD]   <pad>\n",
      "streets   NNS\n",
      "who   WP\n",
      "has   VBZ\n",
      "been   VBN\n",
      "deposed   JJ\n",
      "ou   NN\n",
      "##sted   VBN\n",
      "began   VBD\n",
      "two   CD\n",
      "men   NNS\n",
      "were   VBD\n",
      "out   IN\n",
      "on   IN\n",
      "into   IN\n",
      "took   VBD\n",
      "outgoing   JJ\n",
      "opens   VBZ\n",
      "have   VBP\n",
      "taken   VBN\n",
      "due   VBG\n",
      "start   VB\n",
      "open   VB\n",
      "opened   VBD\n"
     ]
    }
   ],
   "source": [
    "for t in tmaps[0].keys():\n",
    "    print(bert_tok.decode(int(t)), \" \", lablist[torch.argmax(tmaps[0][t])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2bb74b-fcc0-4953-a800-ef5d18e6230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8066215a-ece9-4881-b3f1-2cde263e4e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for l in latposylabels[0]:\n",
    "    \n",
    "    \n",
    "    #print(cnt, \" \", lablist[torch.argmax(l)] )\n",
    "    if torch.argmax(l)==0:\n",
    "        print(bert_tok.decode(sents[0][23]))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a812b-18df-4846-9704-3c7f4e494d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
