{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08cc2d4-f071-49a7-aee2-419f46a42521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 07:33:07.956023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-31 07:33:07.956046: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from src.recom_search.model.beam_node_reverse import ReverseNode\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearLatticeBert, LinearPOSBert\n",
    "from encoding_utils import *\n",
    "import pickle\n",
    "import toy_helper as thelp\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from latmask_bert_models import LatticeBertModel\n",
    "import json\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from mask_utils import *\n",
    "from encoding_utils import *\n",
    "\n",
    "\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbart_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db5b195-d574-43ed-a978-5f86709ae22e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     labels \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m     81\u001b[0m posbmodel \u001b[38;5;241m=\u001b[39m LinearPOSBertV1(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(labels\u001b[38;5;241m.\u001b[39mkeys())))\n\u001b[0;32m---> 82\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./a3distrib/ckpt/posbert.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     83\u001b[0m posbmodel\u001b[38;5;241m.\u001b[39mload_state_dict(tmp)\n\u001b[1;32m     84\u001b[0m posbmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Model Wrapper\n",
    "class LinearPOSBertV1(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = LatticeBertModel(AutoConfig.from_pretrained('bert-base-cased'))\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences, pos_ids=None, attmasks=None):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            if pos_ids==None:\n",
    "                word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "            else:\n",
    "                word_rep, sentence_rep = self.bert(sentences, position_ids=pos_ids, encoder_attention_mask=attmasks, attention_mask=attmasks, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "def prepare_dataset(resset):\n",
    "    x = []\n",
    "    y = []\n",
    "    for res in resset:\n",
    "        curinps = []\n",
    "        for r in res:\n",
    "            try:\n",
    "                toktmp = torch.tensor(bert_tok(clean_expanded(r)).input_ids)\n",
    "                #print(toktmp.shape)\n",
    "                if float(toktmp.shape[0])<MAX_LEN:\n",
    "                    toktmp = torch.cat([toktmp, torch.zeros(MAX_LEN-toktmp.shape[0])])\n",
    "                else:\n",
    "                    toktmp = toktmp[:MAX_LEN]\n",
    "                curinps.append(toktmp)\n",
    "            except:\n",
    "                print(\"weird error happened\") \n",
    "        print(len(curinps))\n",
    "        curouts = []\n",
    "        tinp = torch.stack(curinps).long().to(device)\n",
    "        print(tinp.shape)\n",
    "        y.append(posbmodel(tinp))\n",
    "        x.append(tinp)\n",
    "        \n",
    "        #print(\"error somewhere\")\n",
    "    return x, y\n",
    "\n",
    "def check_accuracy(setpred, setlabels):\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "    for i in range(0, len(setpred)):\n",
    "        ex = setpred[i]\n",
    "        for j in range(0, len(ex)):\n",
    "            if sum(setlabels[i][j])==0:\n",
    "                continue\n",
    "            elif torch.argmax(setlabels[i][j])==0:\n",
    "                continue\n",
    "            tot+=1\n",
    "            if torch.argmax(ex[j])==torch.argmax(setlabels[i][j]):\n",
    "                cor+=1\n",
    "    return cor/tot\n",
    "\n",
    "# correct posids\n",
    "def mod_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            if p[i]==0:\n",
    "                p[i] = i\n",
    "    return cop\n",
    "\n",
    "# set posids to default\n",
    "def def_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            p[i] = i\n",
    "    return cop\n",
    "\n",
    "# Load POS model, label vocabulary \n",
    "with open('./a3distrib/lab_vocab.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "posbmodel = LinearPOSBertV1(len(list(labels.keys())))\n",
    "tmp = torch.load(\"./a3distrib/ckpt/posbert.pth\")\n",
    "posbmodel.load_state_dict(tmp)\n",
    "posbmodel.eval()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(\"cuda:2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a628828-df45-46ab-992b-fe662c079a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2 input strings of the format where the start w/ the same pre-fix but have different endings\n",
    "s1 = \"The Fed raises interest rates\"\n",
    "s2 = \"The Fed raises interest him\"\n",
    "\n",
    "# construct data structure for toy graph in format used on actual examples\n",
    "toygraph = thelp.create_toy_graph(s2, s1, mbart_tok)\n",
    "\n",
    "# get list of exploded candidates using same algorithm from numbers\n",
    "exploded = fl.get_all_possible_candidates(toygraph)\n",
    "\n",
    "# get a flattened version of toy lattice (same method as on actual examples)\n",
    "flat_toy = fl.flatten_lattice(toygraph)\n",
    "\n",
    "# generate mask (uses same method as actual examples), convert to -inf mask (seems to not do anything)\n",
    "mask = connect_mat(flat_toy)\n",
    "#mask[mask==0] = -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c5bdf-ae5b-461c-a2ed-5a8dcffef9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get gold labels for the exploded set\n",
    "dsetx, dsety = prepare_dataset([exploded])\n",
    "\n",
    "assert len(dsetx)==1\n",
    "\n",
    "# from encoding utils, get posids and relevant tokens\n",
    "sents, posids = create_inputs([flat_toy])\n",
    "\n",
    "# get gold label dictionaries for tokens in example, based on averages of tokens on dsety\n",
    "_ , tmaps = lattice_pos_goldlabels(dsetx, dsety, sents)\n",
    "\n",
    "# generate gold y labels using tmaps and \n",
    "latposylabels = tmap_pos_goldlabels(tmaps, sents)\n",
    "\n",
    "# get generated labels for flattened lattice, def_posids can be used for default posids\n",
    "# params start as (sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "# posids, mask can be set to None to ablate to default\n",
    "pred = posbmodel(sents.to(device), mod_posids(posids).to(device), torch.stack([mask]).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412dd25-9660-48b6-a334-68342357a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy (assumes that gold is good, which isn't confirmed here)\n",
    "check_accuracy(pred, latposylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110d1a5-5db6-459b-ab63-0a11743fe720",
   "metadata": {},
   "outputs": [],
   "source": [
    "lablist = [k for k in labels.keys()]\n",
    "CUTOFF = 10\n",
    "\n",
    "def show_labels (pred):\n",
    "    res = []\n",
    "    for p in pred:\n",
    "        res.append(lablist[torch.argmax(p)])\n",
    "    return res\n",
    "\n",
    "# sanity check to look at flat lattice \n",
    "p = flat_toy\n",
    "tlist = fl.get_toklist(p)\n",
    "decstr = bert_tok.decode(tlist)\n",
    "\n",
    "# number of tokens, the tokens that are passed into model for lattice\n",
    "print(len(tlist))\n",
    "print(decstr)\n",
    "\n",
    "print(\"PREDICTED\")\n",
    "print(show_labels(pred[0])[:CUTOFF])\n",
    "print(\"GOLD\")\n",
    "print(show_labels(latposylabels[0])[:CUTOFF])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211410b-8afd-4cd0-9f02-8552b88db575",
   "metadata": {},
   "outputs": [],
   "source": [
    "indivlabs = posbmodel(dsetx[0])\n",
    "\n",
    "# show labels for s1, s2 when run through individually\n",
    "print(s1)\n",
    "print(show_labels(indivlabs[0])[:8])\n",
    "print(s2)\n",
    "print(show_labels(indivlabs[1])[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d24a8-e14b-4464-b96f-65c0434341f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00173714-0b8f-49da-8f55-4e6f115d0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb2aa9-6a63-4a7c-8f69-d46d6c4e3adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
