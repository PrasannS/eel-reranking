{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee963e1b-b8cf-41d7-9135-00b38d78a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 09:01:11.885530: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-22 09:01:11.885555: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "from rerank_score_cands_new import load_cands\n",
    "import numpy as np\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "#from distill_comet import XLMCometRegressor\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "xlm_tok = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "class XLMCometRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        # TODO should we be freezing layers?\n",
    "        super().__init__()\n",
    "        \n",
    "        self.xlmroberta = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "        # Num labels 1 should just indicate regression (?)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.xlmroberta.config.hidden_size, 1), \n",
    "        )\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        # don't finetune xlmroberta model\n",
    "        #with torch.no_grad():\n",
    "        word_rep, sentence_rep = self.xlmroberta(input_ids, attention_mask=attention_masks, encoder_attention_mask=attention_masks, return_dict=False)\n",
    "        # use the first <s> token as a CLS token, TODO experiment with using the sum of \n",
    "        # ensure padding not factored in\n",
    "        word_rep = word_rep*(input_ids>0).unsqueeze(-1)\n",
    "        outputs = self.regressor(torch.sum(word_rep, 1))\n",
    "        #print(\"Shape: \", outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "def load_cometqe_data():\n",
    "    with open('processeddata/germanlatmasks.pkl', 'rb') as f:\n",
    "        masks = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/germanlatinps.pkl', 'rb') as f:\n",
    "        xinps = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/germanlatlabels.pkl', 'rb') as f:\n",
    "        yinps = pickle.load(f)\n",
    "    return masks, xinps, yinps\n",
    "\n",
    "mdata, xdata, ydata = load_cometqe_data()\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41843b0-3bd4-4c2d-affe-4077a3398819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, masks):\n",
    "        assert len(sentences) == len(labels)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.labels[i], self.masks[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "def collate_custom(datafull):\n",
    "    #print(len(datafull[0]))\n",
    "    data = [torch.tensor(d[0]) for d in datafull]\n",
    "    masdata=  [d[2] for d in datafull]\n",
    "    labels = [d[1] for d in datafull]\n",
    "    max_len = max([x.squeeze().numel() for x in data])\n",
    "    data = [torch.nn.functional.pad(x, pad=(0, max_len - x.numel()), mode='constant', value=0) for x in data]\n",
    "    data = torch.stack(data).to(device)\n",
    "    # TODO just a normal mask for now\n",
    "    #masdata = [torch.ones_like(m) for m in masdata]\n",
    "    masdata = [torch.nn.functional.pad(x, pad=(0, max_len - x[0].numel(), 0, max_len - x[0].numel()), mode='constant', value=0) for x in masdata]\n",
    "    masdata = torch.stack(masdata).to(device)\n",
    "    return data, torch.tensor(labels).to(device), masdata\n",
    "\n",
    "subset = 32*100\n",
    "xtmp = xdata[:subset]\n",
    "ytmp = ydata[:subset]\n",
    "mtmp = mdata[:subset]\n",
    "\n",
    "trainloader = DataLoader(RegressionDataset(xtmp, ytmp, mtmp), batch_size=32, shuffle=False, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5bce46-1aac-4c8f-ba67-7203384e3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "ite = iter(trainloader)\n",
    "i = ite.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "693224c2-b55f-4c9b-97f3-168f28818f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(i[2][0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45eecfc-edeb-4e3b-8c8b-85c1deff45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "def train(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2):\n",
    "    print(\"Total steps :\", epochs*len(train_dataloader))\n",
    "    best_loss = 1e10\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch%1==0:\n",
    "            print(\"EPOCH \", epoch)\n",
    "            print(\"-----\")\n",
    "            print(best_loss)\n",
    "        model.train()\n",
    "        cbest = 1e10\n",
    "        lostot = 0\n",
    "        loscnt = 0\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            batch_inputs, batch_labels, batch_masks = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "            loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "            lostot+=loss\n",
    "            loscnt+=1\n",
    "            if step%5==0:\n",
    "                #print(loss)  \n",
    "                if loscnt>0:\n",
    "                    print(lostot/loscnt)\n",
    "                    cbest = min(float(lostot/loscnt), cbest)\n",
    "                    best_loss = min(best_loss, cbest)\n",
    "                    #print(\"cbest, \", cbest)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        cbest = min(float(lostot/loscnt), cbest)\n",
    "        best_loss = min(best_loss, cbest)\n",
    "        print(\"cbest, \", cbest)\n",
    "        if epoch%5==0:\n",
    "            #torch.save(model.state_dict(), \"torchsaved/germantest\"+str(epoch)+\".pt\")\n",
    "            \"\"\n",
    "    return model\n",
    "\n",
    "model = XLMCometRegressor(drop_rate=0.1)\n",
    "#model.load_state_dict(torch.load(\"./torchsaved/germanlat9.pt\"))\n",
    "\n",
    "print(\"model loaded\")\n",
    "\n",
    "vmask = (torch.triu(torch.ones(32, 32))*2-torch.ones(32, 32))*-1\n",
    "vmask = vmask.to(device)\n",
    "mse = nn.MSELoss()\n",
    "def rank_loss(preds, golds):\n",
    "    totloss = 0\n",
    "    for i in range(1, len(preds)):\n",
    "        # for margin\n",
    "        margin = (golds - torch.roll(golds, i))*vmask[i]\n",
    "        diff = ((preds - torch.roll(preds, i))-margin)*vmask[i]\n",
    "        diff[diff<0] = 0\n",
    "        totloss+=torch.sum(diff)\n",
    "    return totloss #+ mse(preds, golds)\n",
    "\n",
    "def rank_easy(preds, golds):\n",
    "    totloss = 0\n",
    "    for i in range(1, len(preds)):\n",
    "        # for margin\n",
    "        #margin = (golds - torch.roll(golds, i))*vmask[i]\n",
    "        diff = (preds - torch.roll(preds, i))*vmask[i]\n",
    "        diff[diff<0] = 0\n",
    "        totloss+=torch.sum(diff)\n",
    "    return totloss #+ mse(preds, golds)\n",
    "\n",
    "def run_model_train_params(learn_r, epochs, loader, mod, loss):\n",
    "    optimizer = AdamW(mod.parameters(),\n",
    "                      lr=learn_r,\n",
    "                      eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
    "                     num_warmup_steps=0, num_training_steps=epochs*len(loader))\n",
    "    model = train(mod, optimizer, scheduler, loss, epochs, \n",
    "                  loader, device, clip_value=2)\n",
    "\n",
    "print(\"start training\")\n",
    "print(ydata)\n",
    "# converge on this first\n",
    "#run_model_train_params(1e-5, 2, trainloader, model, mse)\n",
    "#run_model_train_params(1e-5, 10, trainloader, model, rank_easy)\n",
    "run_model_train_params(1e-5, 200, trainloader, model, rank_easy)\n",
    "#run_model_train_params(1e-5, 100, trainloader, model, rank_easy)\n",
    "\n",
    "#run_model_train_params(5e-5, 5, trainloader, model, rank_easy)\n",
    "#run_model_train_params(1e-5, 10, trainloader, model, rank_easy)\n",
    "#run_model_train_params(1e-5, 20, trainloader, model, rank_loss)\n",
    "\n",
    "#run_model_train_params(1e-5, 40, trainloader, model, rank_loss)\n",
    "# nohup python -u async_train_model.py &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
