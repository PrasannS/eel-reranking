{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c7bdfb-27bd-4fdf-8a3c-953d9e5ff5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 12:08:36.222739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-10 12:08:36.222764: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODE_NORMAL\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflatten_lattice\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearPOSBertV1\n",
      "File \u001b[0;32m/mnt/data1/prasann/latticegen/lattice-generation/flatten_lattice.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m frenbase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtn1_fr-en_bfs_recom_4_-1_False_0.4_True_False_4_5_rcb_0.902_0.0_0.9/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m bert_tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m mbart_tok \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/mbart-large-50-many-to-one-mmt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m mbart_tok\u001b[38;5;241m.\u001b[39msrc_lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# method that takes in # of stops\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:451\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1741\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1739\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1861\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1862\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1863\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/mbart50/tokenization_mbart50_fast.py:133\u001b[0m, in \u001b[0;36mMBart50TokenizerFast.__init__\u001b[0;34m(self, vocab_file, src_lang, tgt_lang, tokenizer_file, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m    129\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    130\u001b[0m     code \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m FAIRSEQ_LANGUAGE_CODES \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    131\u001b[0m ]\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_code_to_id \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m     lang_code: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(lang_code) \u001b[38;5;28;01mfor\u001b[39;00m lang_code \u001b[38;5;129;01min\u001b[39;00m FAIRSEQ_LANGUAGE_CODES\n\u001b[1;32m    151\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:110\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:950\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo converter was found. Currently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    948\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:479\u001b[0m, in \u001b[0;36mSpmConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m--> 479\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Tokenizer assemble\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mnormalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto)\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:440\u001b[0m, in \u001b[0;36mSpmConverter.tokenizer\u001b[0;34m(self, proto)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, proto):\n\u001b[1;32m    439\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mtrainer_spec\u001b[38;5;241m.\u001b[39mmodel_type\n\u001b[0;32m--> 440\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     unk_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_id(proto)\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:635\u001b[0m, in \u001b[0;36mMBart50Converter.vocab\u001b[0;34m(self, proto)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m, proto):\n\u001b[1;32m    629\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    630\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m    631\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m    632\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m    633\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m    634\u001b[0m     ]\n\u001b[0;32m--> 635\u001b[0m     vocab \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(piece\u001b[38;5;241m.\u001b[39mpiece, piece\u001b[38;5;241m.\u001b[39mscore) \u001b[38;5;28;01mfor\u001b[39;00m piece \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpieces\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m]\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     vocab \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mar_AR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcs_CZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde_DE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124met_EE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfi_FI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgu_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mja_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkk_KZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mko_KR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlt_LT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlv_LV\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_MM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mne_NP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnl_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mro_RO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mru_RU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msi_LK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr_TR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvi_VN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh_CN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maf_ZA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maz_AZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfa_IR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_IL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhr_HR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mka_GE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkm_KH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmk_MK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmn_MN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmr_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl_PL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mps_AF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msv_SE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msw_KE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mta_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mte_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth_TH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtl_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muk_UA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mur_PK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxh_ZA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgl_ES\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msl_SI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lzma import MODE_NORMAL\n",
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearPOSBertV1\n",
    "from encoding_utils import *\n",
    "import pickle\n",
    "from mask_utils import *\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "bert_tok = fl.bert_tok\n",
    "mbart_tok = fl.mbart_tok\n",
    "\n",
    "# -1 for whole graph, otherwise # of lattice segments to use\n",
    "STOPS = 1\n",
    "# v3 - first whole lattice\n",
    "# v4 - first single lattice\n",
    "# v5 - single lattice with fixes\n",
    "VNUM = 5\n",
    "MOD_NAME = 'bertonewayv1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bcaa7a-4fe3-4bac-9658-13500a47a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies files for pre-loading\n",
    "LOADED = {\n",
    "    'amasks': 'attmasksallv'+str(VNUM)+'.pt',\n",
    "    'tmaps': 'tmapsmaskedv'+str(VNUM)+'/'\n",
    "}\n",
    "\n",
    "# Code needed when changing set of gold labels\n",
    "def prepare_dataset(resset, model):\n",
    "    x = []\n",
    "    y = []\n",
    "    allmasks = []\n",
    "    for res in resset:\n",
    "        curinps = []\n",
    "        curmasks = []\n",
    "        for r in res:\n",
    "            try:\n",
    "                msk = torch.zeros(MAX_LEN, MAX_LEN)\n",
    "                toktmp = torch.tensor(bert_tok(clean_expanded(r)).input_ids)\n",
    "                msk[:len(toktmp), :len(toktmp)] = torch.ones(len(toktmp), len(toktmp))\n",
    "                msk = torch.tril(msk)\n",
    "                #msk = msk[:MAX_LEN, :MAX_LEN]\n",
    "                #print(toktmp.shape)\n",
    "                if float(toktmp.shape[0])<MAX_LEN:\n",
    "                    dlen = MAX_LEN-toktmp.shape[0]\n",
    "                    toktmp = torch.cat([toktmp, torch.zeros(dlen)])\n",
    "                else:\n",
    "                    toktmp = toktmp[:MAX_LEN]\n",
    "                curinps.append(toktmp)\n",
    "                curmasks.append(msk)\n",
    "            except:\n",
    "                print(\"weird error happened\") \n",
    "        print(len(curinps))\n",
    "        tinp = torch.stack(curinps).long().to(device)\n",
    "        print(tinp.shape)\n",
    "        # not taking in 1-way mask \n",
    "        y.append(model(tinp, attmasks=torch.stack(curmasks).long().to(device)))\n",
    "        x.append(tinp)\n",
    "        del tinp\n",
    "        allmasks.append(curmasks)\n",
    "        \n",
    "    return x, y, allmasks\n",
    "\n",
    "def get_labset_partial(explodeds, startind, amt, model):\n",
    "    dsetx, dsety, _ = prepare_dataset(explodeds[startind:startind+amt], model)\n",
    "    print(len(dsetx))\n",
    "    assert len(dsetx)==amt\n",
    "    latposylabels, tmaps = lattice_pos_goldlabels(dsetx, dsety, sents[startind:startind+amt])\n",
    "    del dsetx, dsety\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return latposylabels, tmaps\n",
    "\n",
    "def get_biglabset(split, model):\n",
    "    for i in range(0, int(len(resarrs)/split)):\n",
    "        print(\"SUBSET - \", i)\n",
    "        r, tmap = get_labset_partial(resarrs, i*split, split, model)\n",
    "        torch.cuda.empty_cache()\n",
    "        file = open('./torchsaved/tmapsmaskedv'+str(VNUM)+'/tmaps_'+str(i*split)+'.pkl', 'wb')\n",
    "        # dump information to that file\n",
    "        pickle.dump(tmap, file)\n",
    "\n",
    "        # close the file\n",
    "        file.close()\n",
    "        del r\n",
    "        del tmap\n",
    "        torch.cuda.empty_cache()\n",
    "        print(torch.cuda.memory_allocated(\"cuda:0\"))\n",
    "\n",
    "def load_model(labels):\n",
    "    # load model, same for gold generation and inference\n",
    "    posbmodel = LinearPOSBertV1(len(list(labels.keys())))\n",
    "    t = torch.load(\"./a3distrib/ckpt/\"+MOD_NAME)\n",
    "    posbmodel.load_state_dict(t)\n",
    "    posbmodel.eval()\n",
    "    del t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"GPU Mem Used = \", torch.cuda.memory_allocated(\"cuda:0\"))\n",
    "\n",
    "    return posbmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05745c8-6015-47e1-9c4c-d0bb4239328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num examples:  101\n",
      "using loaded masks\n"
     ]
    }
   ],
   "source": [
    "# First we want to generate flattened version of the graph\n",
    "processedgraphs = fl.get_processed_graph_data(fl.frenbase, -1, STOPS)\n",
    "\n",
    "# extra step for greedy \n",
    "if STOPS==1:\n",
    "    processedgraphs = filter_greedy(processedgraphs)\n",
    "\n",
    "# get exploded candidates to generate gold labels\n",
    "resarrs = [fl.get_cover_paths(p)[0] for p in processedgraphs]\n",
    "\n",
    "# ensure no empty examples\n",
    "clean_empty(resarrs, processedgraphs)\n",
    "print(\"num examples: \", len(resarrs))\n",
    "\n",
    "# TODO should I add an example?\n",
    "\n",
    "# get attention masks, make if they don't exist allready\n",
    "if os.path.exists('./torchsaved/'+LOADED['amasks']):\n",
    "    print(\"using loaded masks\")\n",
    "    attmasks = torch.load('./torchsaved/'+LOADED['amasks']).to(device)\n",
    "else:\n",
    "    print(\"creating new masks\")\n",
    "    masktmp = [connect_mat(p) for p in processedgraphs]\n",
    "    attmasks = torch.stack(masktmp).to(device)\n",
    "    torch.save(attmasks, './torchsaved/'+LOADED['amasks'])\n",
    "\n",
    "# convert to backwards-only mask\n",
    "# TODO some sort of bug causing NaN predictions\n",
    "attmasks = torch.tril(attmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229314ac-396a-4a3b-81a0-aceb40124608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Mem Used =  534572544\n",
      "Average, max nodes:  (22.396039603960396, 61)\n"
     ]
    }
   ],
   "source": [
    "# credit to tutorial by https://pageperso.lis-lab.fr/benoit.favre/pstaln/09_embedding_evaluation.html for \n",
    "# input / pre-processing setup\n",
    "# load labels\n",
    "with open('./a3distrib/lab_vocab.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "posbmod = load_model(labels)\n",
    "\n",
    "# create inputs\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "\n",
    "print(\"Average, max nodes: \", avg_nodes(sents))\n",
    "\n",
    "# generate token label maps if they don't exist\n",
    "if os.path.exists('./torchsaved/'+LOADED['tmaps']+'tmaps_0.pkl')==False:\n",
    "    with torch.no_grad():\n",
    "        get_biglabset(1, posbmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae056076-bb22-4109-8865-1781146015cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The US President was to receive Iraqi Prime Minister Nouri Al Maliki Friday, November 1, 2013 in an effort to seek US assistance in fighting the worst wave of violence in five years. [SEP]\n",
      "The US President was to receive Iraqi Prime Minister Nouri Al Maliki Friday, November 1, 2013 in an effort to seek US assistance in fighting the worst wave of violence in five years.\n"
     ]
    }
   ],
   "source": [
    "print(bert_tok.decode(sents[0][:40]))\n",
    "print(resarrs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53a7de-b479-4d2f-8094-edf7530d3090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65463b21-aa86-4606-b261-a1edee23f842",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latposylabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tot)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnt\u001b[38;5;241m/\u001b[39mtot\n\u001b[0;32m---> 46\u001b[0m flex_acc(\u001b[43mlatposylabels\u001b[49m, pred1, tmaps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latposylabels' is not defined"
     ]
    }
   ],
   "source": [
    "def get_acc(ylabs, thepred):\n",
    "    # simplify prediction tensors\n",
    "    ysimp = ylabs\n",
    "    psimp = torch.argmax(thepred, dim=2)\n",
    "    # clean up labels\n",
    "    sm = subword_mask_all(sents)\n",
    "    ysimp[sents==0] = 0\n",
    "    ysimp[sents==102] = 0\n",
    "    ysimp[sm==0] = 0\n",
    "    ysimp[:, 0] = 0\n",
    "    # apply cleanaup to x \n",
    "    psimp[ysimp==0] = 0\n",
    "    # apply cleanaup to x \n",
    "    psimp[ysimp==0] = 0\n",
    "    # compute accuracy\n",
    "    acc = 1 - ((ysimp-psimp).count_nonzero())/ysimp.count_nonzero()\n",
    "    return acc, ysimp, psimp\n",
    "\n",
    "def flex_acc(ylabs, thepred, tmaps):\n",
    "    ysimp = ylabs\n",
    "    psimp = torch.argmax(thepred, dim=2)\n",
    "    # clean up labels\n",
    "    sm = subword_mask_all(sents)\n",
    "    ysimp[sents==0] = 0\n",
    "    ysimp[sents==102] = 0\n",
    "    ysimp[sm==0] = 0\n",
    "    ysimp[:, 0] = 0\n",
    "    # apply cleanaup to x \n",
    "    psimp[ysimp==0] = 0\n",
    "    # apply cleanup to sents\n",
    "    scop = torch.clone(sents)\n",
    "    scop[ysimp==0]=0\n",
    "    tot=0\n",
    "    cnt = 0\n",
    "    # go through valid tokens\n",
    "    for pos in torch.nonzero(scop):\n",
    "        tk = str(int(scop[pos[0], pos[1]]))\n",
    "        if int(psimp[pos[0], pos[1]]) in tmaps[pos[0]][tk]:\n",
    "            cnt+=1\n",
    "        else:\n",
    "            print(int(psimp[pos[0], pos[1]]), \" \", tmaps[pos[0]][tk])\n",
    "        tot+=1\n",
    "    print(tot)\n",
    "    return cnt/tot\n",
    "            \n",
    "flex_acc(latposylabels, pred1, tmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a92e7f7-b75d-4259-8e9c-44b2b67ceced",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, ysimp, psimp = get_acc(latposylabels, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fca6132b-4460-4b00-b8f2-2318c87eaab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ysimp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     errp[diff\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m errg, errp\n\u001b[0;32m---> 11\u001b[0m errg, errp \u001b[38;5;241m=\u001b[39m \u001b[43mget_err_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36mget_err_tensors\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_err_tensors\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     diff \u001b[38;5;241m=\u001b[39m \u001b[43mysimp\u001b[49m\u001b[38;5;241m-\u001b[39mpsimp\n\u001b[1;32m      4\u001b[0m     diff \u001b[38;5;241m=\u001b[39m diff\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m      5\u001b[0m     errg \u001b[38;5;241m=\u001b[39m ysimp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ysimp' is not defined"
     ]
    }
   ],
   "source": [
    "lablist = [l for l in labels.keys()]\n",
    "def get_err_tensors():\n",
    "    diff = ysimp-psimp\n",
    "    diff = diff.abs().bool().int()\n",
    "    errg = ysimp\n",
    "    errp = psimp\n",
    "    errg[diff==0] = 0\n",
    "    errp[diff==0] = 0\n",
    "    return errg, errp\n",
    "\n",
    "errg, errp = get_err_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f63c671-a155-4ca0-bf0c-359a9f0f9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load token label maps, use to get y-labels\n",
    "N_EX = 101\n",
    "tmaps = []\n",
    "for i in range(0, N_EX):\n",
    "    with open('./torchsaved/'+LOADED['tmaps']+'tmaps_'+str(i)+'.pkl', 'rb') as file:\n",
    "        tmaps.append(pickle.load(file)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a194519-9bac-4b2e-bcf4-72c0050fa202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO y-labels are wrong currently, go back in to get rid of pad / subword stuff\n",
    "latposylabels = tmap_pos_goldlabels(tmaps, sents)    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# save data to run later\n",
    "outputdata = {}\n",
    "outputdata['tmaps'] = tmaps\n",
    "outputdata['masks'] = attmasks\n",
    "outputdata['pgraphs'] = processedgraphs\n",
    "\n",
    "with open('./torchsaved/outputv'+str(VNUM)+'.pkl', 'wb') as file:\n",
    "    # dump information to that file\n",
    "    pickle.dump(outputdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e85ae-b190-4c61-b7b1-cf452e8177e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all predictions with ablations\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "pred1 = posbmod(sents, mod_posids(posids), attmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d9c0f5-cecc-4603-94ce-799705a8c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8899, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sents, posids = create_inputs(processedgraphs)\n",
    "pred2 = posbmod(sents, None, attmasks)\n",
    "print(check_accuracy(pred2, latposylabels, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae02243-db6c-4bc4-9ae7-3a491e5eee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8894, device='cuda:0')\n",
      "tensor(0.8899, device='cuda:0')\n",
      "tensor(0.9635, device='cuda:0')\n",
      "tensor(0.9894, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(pred1, latposylabels, sents))\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "pred2 = posbmod(sents, fix_posids(posids), attmasks)\n",
    "print(check_accuracy(pred2, latposylabels, sents))\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "pred3 = posbmod(sents, mod_posids(posids), None)\n",
    "print(check_accuracy(pred3, latposylabels, sents))\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "pred4 = posbmod(sents, fix_posids(posids), None)\n",
    "print(check_accuracy(pred4, latposylabels, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c44821c-92db-4afa-a139-c591e25a5c87",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [124]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m sents, posids \u001b[38;5;241m=\u001b[39m create_inputs(processedgraphs)\n\u001b[0;32m----> 2\u001b[0m mpids \u001b[38;5;241m=\u001b[39m \u001b[43mmod_posids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sents, posids \u001b[38;5;241m=\u001b[39m create_inputs(processedgraphs)\n\u001b[1;32m      4\u001b[0m fpids \u001b[38;5;241m=\u001b[39m fix_posids(posids)\n",
      "File \u001b[0;32m/mnt/data1/prasann/latticegen/lattice-generation/encoding_utils.py:151\u001b[0m, in \u001b[0;36mmod_posids\u001b[0;34m(pids)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(p)):\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m             p[i] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cop\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sents, posids = create_inputs(processedgraphs)\n",
    "mpids = mod_posids(posids)\n",
    "sents, posids = create_inputs(processedgraphs)\n",
    "fpids = fix_posids(posids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ae799-1c73-4682-9838-4050cdfcc9d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "be56a92c-57a3-4bf0-bb23-c678d0fc844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, posids = create_inputs(processedgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748eff26-3029-4eae-8c34-4cb133c41f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "            try:\n",
    "                msk = torch.zeros(MAX_LEN, MAX_LEN)\n",
    "                toktmp = torch.tensor(bert_tok(clean_expanded(r)).input_ids)\n",
    "                msk[:len(toktmp), :len(toktmp)] = torch.ones(len(toktmp), len(toktmp))\n",
    "                msk = torch.tril(msk)\n",
    "                #msk = msk[:MAX_LEN, :MAX_LEN]\n",
    "                #print(toktmp.shape)\n",
    "                if float(toktmp.shape[0])<MAX_LEN:\n",
    "                    dlen = MAX_LEN-toktmp.shape[0]\n",
    "                    toktmp = torch.cat([toktmp, torch.zeros(dlen)])\n",
    "                else:\n",
    "                    toktmp = toktmp[:MAX_LEN]\n",
    "                curinps.append(toktmp)\n",
    "                curmasks.append(msk)\n",
    "            except:\n",
    "                print(\"weird error happened\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c890f6-6e34-46ec-a697-73915c35fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetx, dsety, cmasks = prepare_dataset(resarrs, posbmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23a7f4-9606-49e6-b1df-8221041a911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(len(attmasks)):\n",
    "    print(torch.sum(cmasks[a][0].to(device)-attmasks[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1e61a-e603-456f-858b-b239f5787e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd3e0e82-efd5-4769-9271-66c8308bbda8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msum((\u001b[43mcmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mattmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "torch.sum((cmasks[0]-attmasks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f567d698-c419-4f7e-b77a-bfafed8d0f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsetx[0][0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c833f743-dcd4-408d-bfa1-8c6dbf04abf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents[0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6813f5ee-7fa9-44b6-be03-622b3001efc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf4dae1b-6247-41c2-8c78-4bd9f3133422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attmasks[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "037dccd7-84be-4306-adde-864d3a882466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist = []\n",
    "for i in range(len(dsetx)):\n",
    "    isdiff = torch.sum((dsetx[i][0]-sents[i]).bool().int())>0\n",
    "    if isdiff:\n",
    "        dlist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b7c4c7-86c6-4256-a2e6-31e4e3cc6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decoded(toks):\n",
    "    nt = []\n",
    "    for t in toks:\n",
    "        if t==100 or t==101 or t==102 or t==0:\n",
    "            continue\n",
    "        nt.append(t)\n",
    "    #print(tok.decode(nt))\n",
    "    return bert_tok.decode(nt)\n",
    "\n",
    "sentdiff = [print_decoded(sents[i]) for i in dlist]\n",
    "golddiff = [print_decoded(dsetx[i][0]) for i in dlist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9799ed96-c2dd-4fce-8fe8-9027c5313588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Americans  risk can be double.', 'The'), ('Men were being killed in the street or even in their homes, and security forces were frequently attacked as well.', 'Men'), ('I think there are two levels of response by the French Government.', 'I'), ('The return process will be the same in return.', 'The')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(sentdiff, golddiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffcc520d-90c5-4256-8137-2a6836dae51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'The Americans  risk can be double.'],\n",
       " ['Men',\n",
       "  'Men were',\n",
       "  'Men were being killed in the street or even in their homes, and security forces were frequently attacked as well.'],\n",
       " ['I', 'I think there are two levels of response by the French Government.'],\n",
       " ['The', 'The return process will be the same in return.']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[resarrs[i] for i in dlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d239dba6-3099-4c94-83e3-3c13ca94a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "yval = torch.argmax(dsety[0], dim=2)[0][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5b4965e-bfca-48d4-97df-61566f4c767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = torch.argmax(, dim=2)[0][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bdfa1d5f-85c3-44ef-8e6f-aaf0a557a157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5,   0,   0,   0,   0,   0,   0,   0,   0,   0, -13,   0,   0,   0,\n",
       "          0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,  18,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval-pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a25c832-d4ff-47e0-b858-bd8a965720e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(ind):\n",
    "    pred = posbmod(sents[ind:ind+1], pos_ids=None, attmasks=attmasks[ind:ind+1])\n",
    "    yval = dsety[ind]\n",
    "    print(torch.sum(pred-yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea86572b-2aae-4ae7-8013-4945c498dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "check_pred(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b442c1a-da96-41a5-ba83-d409648c096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_posids(posids[0:0+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad30ab-33f2-4b1e-9952-734989ed5a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
