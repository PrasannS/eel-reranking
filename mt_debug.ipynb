{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd758ae-aabd-4aae-87cc-f94295f34075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from typing import List\n",
    "import json\n",
    "#from transformers import AutoTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f1cd9c-23e0-4a75-b41b-708152ee8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "cometqe_dir = \"./cometqemodel\"\n",
    "cometqe_model = \"wmt20-comet-qe-da\"\n",
    "cometmodel = \"wmt20-comet-da\"\n",
    "batch_size = 64\n",
    "def load_cands(fname):\n",
    "    data = []\n",
    "    with open(fname, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            #print(line)\n",
    "            # if line is empty\n",
    "            # end of file is reached\n",
    "            if not line or len(line)<3:\n",
    "                break\n",
    "\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def process_cands(cand_data):\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    srcs = []\n",
    "    clen = len(cand_data['scores'])\n",
    "    # return refs, hyps, srcs\n",
    "    return [cand_data['ref']]*clen, cand_data['cands'], [cand_data['inp']]*clen\n",
    "        \n",
    "\n",
    "def get_average_score(cand_data):\n",
    "    scoresum = 0\n",
    "    scoretot = 0\n",
    "    for c in cand_data:\n",
    "        scoresum+=sum(c['scores'])\n",
    "        scoretot+=len(c['scores'])\n",
    "    return scoresum/scoretot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bbcfb0-a04e-4caf-b51c-1cf477773167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out average token length\n",
    "#lentok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\", src_lang=\"de_DE\")\n",
    "beam_cands = load_cands(\"./candoutputs/beam40en_de.jsonl\")\n",
    "lat_cands = load_cands(\"./candoutputs/latoutputdenew.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5c3ca1-7397-4dea-ae51-b2540677e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = lentok(beam_cands[0]['ref'])\n",
    "def get_len_distr(cands):\n",
    "    maxlen = 0\n",
    "    totlen = 0\n",
    "    vals = []\n",
    "    for c in cands:\n",
    "        try:\n",
    "            tmp = len(lentok(c['ref'])['input_ids'])\n",
    "        except:\n",
    "            print(\"issue\")\n",
    "            tmp = maxlen\n",
    "        maxlen = max(maxlen, tmp)\n",
    "        totlen+=tmp\n",
    "        vals.append(tmp)\n",
    "    return vals\n",
    "        \n",
    "#len_dist = get_len_distr(beam_cands)\n",
    "#plt.hist(len_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a57ec7-93f4-4b1a-a1d9-795b6738d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_cand(ind):\n",
    "\n",
    "    print(cand_data[ind]['ref'])\n",
    "    for c in cand_data[ind]['cands']:\n",
    "        print(c)\n",
    "        \n",
    "def find_common(c1, c2list):\n",
    "    for c in c2list:\n",
    "        if c['ref']==c1['ref']:\n",
    "            return c\n",
    "        \n",
    "def get_cometqe_scores(hyps, srcs):\n",
    "    cometqe_input = [{\"src\": src, \"mt\": mt} for src, mt in zip(srcs, hyps)]\n",
    "    # sentence-level and corpus-level COMET\n",
    "    outputs = model.predict(\n",
    "        cometqe_input, batch_size=40, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs\n",
    "\n",
    "def get_comet_scores(hyps, srcs, refs):\n",
    "    cometqe_input = [{\"src\": src, \"mt\": mt, \"ref\":ref} for src, mt, ref in zip(srcs, hyps, refs)]\n",
    "    # sentence-level and corpus-level COMET\n",
    "    outputs = comet.predict(\n",
    "        cometqe_input, batch_size=40, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs\n",
    "\n",
    "def test_cometqe(hyp, src):\n",
    "    cqe_input = [{'src':src, 'mt':hyp}]\n",
    "    outputs = model.predict(\n",
    "        cqe_input, batch_size=1, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ef7162-1934-44df-ac02-b18da11ea368",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lat_cands' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(get_comet_scores([lat_cands[ind][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcands\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]], [lat_cands[ind][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minp\u001b[39m\u001b[38;5;124m'\u001b[39m]], [lat_cands[ind][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mcomp_debug_cand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mcomp_debug_cand\u001b[0;34m(ind, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomp_debug_cand\u001b[39m(ind, k):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlat_cands\u001b[49m[ind][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcands\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lat_cands' is not defined"
     ]
    }
   ],
   "source": [
    "#print(beam_cands[0]['cands'][0])\n",
    "def comp_debug_cand(ind, k):\n",
    "    if len(lat_cands[ind]['cands'])==0:\n",
    "        return\n",
    "    print(\"INPUT\")\n",
    "    print(lat_cands[ind]['inp'])\n",
    "    print(\"REF\")\n",
    "    print(lat_cands[ind]['ref'])\n",
    "    print(\"LATTICE\")\n",
    "    for i in range(0, k):\n",
    "        try:\n",
    "            print(lat_cands[ind]['scores'][i])\n",
    "            print(lat_cands[ind]['cands'][i])\n",
    "        except:\n",
    "            #print(\"NONE\")\n",
    "            \"\"\n",
    "    print(\"BEAM\")\n",
    "    common = find_common(lat_cands[ind], beam_cands)\n",
    "    #print(common)\n",
    "    for i in range(0, k):\n",
    "        print(common['scores'][i])\n",
    "        print(common['cands'][i])\n",
    "    print(\"BEAM\")\n",
    "    print(get_comet_scores([common['cands'][0]], [lat_cands[ind]['inp']], [lat_cands[ind]['ref']]))\n",
    "    print(\"LATTICE\")\n",
    "    print(get_comet_scores([lat_cands[ind]['cands'][0]], [lat_cands[ind]['inp']], [lat_cands[ind]['ref']]))\n",
    "\n",
    "for i in range(1, 20):\n",
    "    comp_debug_cand(i, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b60c24-6d2a-4df3-86d3-1c344240a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-da is already in cache.\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "comet_path = download_model(cometmodel, \"./cometmodel\")\n",
    "comet = load_from_checkpoint(comet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774229aa-00d4-42b7-bb3c-dd8017827e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\", tgt_lang=\"de_DE\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4af83adb-bb45-4dcf-9079-e243661145a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f6c26a3-9cde-44f3-aa27-b00ec35b90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbart_nll(cand, ind, tok, mod):\n",
    "\n",
    "    inp = cand['inp']\n",
    "    out = cand['cands'][ind]\n",
    "    \n",
    "\n",
    "    inputs = tokenizer(inp, return_tensors=\"pt\").to(device)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(out, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    # forward pass\n",
    "    output = model(**inputs, labels=labels.input_ids)\n",
    "    #print(type(labels))\n",
    "    #print(labels.attention_mask)\n",
    "    return output.loss\n",
    "\n",
    "def test_cand(cand, ind):\n",
    "    print(get_mbart_nll(cand, ind))\n",
    "    print(cand['scores'][ind])\n",
    "    \n",
    "def rescore_cands(c_list):\n",
    "    device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mname = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "    src_l = \"en_XX\"\n",
    "    tgt_l = \"de_DE\"\n",
    "    tok = tokenizer\n",
    "    mod = model\n",
    "    # tok = AutoTokenizer.from_pretrained(mname, src_lang=src_l, tgt_lang=tgt_l)\n",
    "    # mod = AutoModelForSeq2SeqLM.from_pretrained(mname)\n",
    "    # mod.to(device)\n",
    "    i = 0\n",
    "    for c in c_list:\n",
    "        c['oldsco'] = c['scores']\n",
    "        c['scores'] = []\n",
    "        for can in range(0, len(c['cands'])):\n",
    "            c['scores'].append(float(get_mbart_nll(c, can, tok, mod)))\n",
    "        c['sco_ranks'] = list(numpy.argsort(c['scores']))\n",
    "        print(i)\n",
    "        i+=1\n",
    "    # del tok\n",
    "    # del mod\n",
    "    return c_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36640cde-aa4c-4bf7-85eb-599c858f02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resc \u001b[38;5;241m=\u001b[39m \u001b[43mrescore_cands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat_cands\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mrescore_cands\u001b[0;34m(c_list)\u001b[0m\n\u001b[1;32m     35\u001b[0m c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m can \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcands\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m---> 37\u001b[0m     c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_mbart_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msco_ranks\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(numpy\u001b[38;5;241m.\u001b[39margsort(c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resc = rescore_cands(lat_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a61e95-8b02-4889-9da1-80d98f156559",
   "metadata": {},
   "outputs": [],
   "source": [
    "resc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04de376a-2292-46c1-9cb4-ab81baae3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_cands[0]['scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c761d22a-ec55-4ea6-a238-26c8f4531817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42018b-308b-49e9-aaec-2a03da7390a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1c49a-9660-4167-8c15-d959c2b104d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
