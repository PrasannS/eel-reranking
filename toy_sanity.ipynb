{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08cc2d4-f071-49a7-aee2-419f46a42521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-29 05:54:01.220619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-29 05:54:01.220641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from src.recom_search.model.beam_node_reverse import ReverseNode\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearLatticeBert, LinearPOSBert\n",
    "from encoding_utils import *\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from mask_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a628828-df45-46ab-992b-fe662c079a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbart_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "#TODO things to sanity check\n",
    "# - reversing graphs\n",
    "# - flattening graphs\n",
    "# - getting masks from graphs\n",
    "# - running flattened versions (how do masks / posids fit into the picture)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4c7d30-b45e-4088-b93a-2483b95712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"The Fed raises interest rates\"\n",
    "s2 = \"The Fed raises interest him\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b626ef45-f5dd-477c-a745-9d1c8fd5e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_from_idx(t1, ind):\n",
    "    return ReverseNode(None, {'uid':str(t1)+str(ind), 'token_idx':t1, 'token_str':bert_tok.decode(t1), 'prob':0})\n",
    "\n",
    "# take 2 comparable strings that split at some point, make a simulated lattice \n",
    "def create_toy_graph(s1, s2, tok):\n",
    "    toks1 = tok(s1).input_ids\n",
    "    toks2 = tok(s2).input_ids\n",
    "    #print(toks1)\n",
    "    #print(toks2)\n",
    "    assert len(toks1) == len(toks2)\n",
    "    prev = [None, None]\n",
    "    merged = False\n",
    "    gdict = {}\n",
    "    for i in range(len(toks1)):\n",
    "        \n",
    "        cur1 = node_from_idx(toks1[i], i)\n",
    "        if i==0:\n",
    "            gdict['root'] = cur1\n",
    "        gdict[cur1.uid] = cur1\n",
    "        #print(toks1[i] is not toks2[i])\n",
    "        #print(merged)\n",
    "        \n",
    "        if (toks1[i] != toks2[i]) or merged:\n",
    "            #print(\"in\")\n",
    "            if prev[1] is None:\n",
    "                prev[1] = prev[0]\n",
    "            cur2 = node_from_idx(toks2[i], i)\n",
    "            prev[1].nextlist.append(cur2)\n",
    "            prev[1].next_scores.append(cur2.prob)\n",
    "            prev[1].next_ids.append(cur2.uid)\n",
    "            prev[1] = cur2\n",
    "            gdict[cur2.uid] = cur2\n",
    "            merged = True\n",
    "        if prev[0]:\n",
    "            prev[0].nextlist.append(cur1)\n",
    "            prev[0].next_scores.append(cur1.prob)\n",
    "            prev[0].next_ids.append(cur1.uid)\n",
    "        prev[0] = cur1\n",
    "    return gdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2effc891-703e-4b3c-beb9-63581724c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "toygraph = create_toy_graph(s1, s2, mbart_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "000b1434-6971-4f14-8d35-0678421be8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = fl.get_all_possible_candidates(toygraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f09c7d-3e07-45e9-bc38-7c30056b9de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_toy = fl.flatten_lattice(toygraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf008e3f-1317-4589-890a-7c16cdb2a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "The Fed raises interest him rates\n"
     ]
    }
   ],
   "source": [
    "p = flat_toy\n",
    "tlist = fl.get_toklist(p)\n",
    "decstr = bert_tok.decode(tlist)\n",
    "print(len(tlist))\n",
    "#for node in p:\n",
    "    #print(node['token_idx'], \" \", node['pos'])\n",
    "print(decstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0bc5f1-05ec-4796-bc70-6d4bd2337ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = connect_mat(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af829e6d-5564-42cd-99f8-ee8c64b558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from latmask_bert_models import LatticeBertModel\n",
    "\n",
    "#TODO fix bug with misaligned 101, 102\n",
    "class LinearPOSBertV1(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = LatticeBertModel(AutoConfig.from_pretrained('bert-base-cased'))\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences, pos_ids=None, attmasks=None):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            if pos_ids==None:\n",
    "                word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "            else:\n",
    "                print('YO')\n",
    "                word_rep, sentence_rep = self.bert(sentences, position_ids=pos_ids, encoder_attention_mask=attmasks, attention_mask=attmasks, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "def prepare_dataset(resset):\n",
    "    x = []\n",
    "    y = []\n",
    "    for res in resset:\n",
    "        curinps = []\n",
    "        for r in res:\n",
    "            try:\n",
    "                toktmp = torch.tensor(bert_tok(clean_expanded(r)).input_ids)\n",
    "                #print(toktmp.shape)\n",
    "                if float(toktmp.shape[0])<MAX_LEN:\n",
    "                    toktmp = torch.cat([toktmp, torch.zeros(MAX_LEN-toktmp.shape[0])])\n",
    "                else:\n",
    "                    toktmp = toktmp[:MAX_LEN]\n",
    "                curinps.append(toktmp)\n",
    "            except:\n",
    "                print(\"weird error happened\") \n",
    "        print(len(curinps))\n",
    "        curouts = []\n",
    "        tinp = torch.stack(curinps).long().to(device)\n",
    "        print(tinp.shape)\n",
    "        y.append(posbmodel(tinp))\n",
    "        x.append(tinp)\n",
    "        \n",
    "        #print(\"error somewhere\")\n",
    "    return x, y\n",
    "\n",
    "def fix_posids(pids):\n",
    "    cop = pids\n",
    "    for p in cop:\n",
    "        for i in range(0, len(p)):\n",
    "            p[i] = i\n",
    "    return cop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e985bb7-ad3c-49e5-b88b-68c331232903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433909248\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./a3-distrib/lab_vocab.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "posbmodel = LinearPOSBertV1(len(list(labels.keys())))\n",
    "posbmodel.load_state_dict(torch.load(\"./a3-distrib/ckpt/posbert.pth\"))\n",
    "posbmodel.eval()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(\"cuda:2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "120c5bdf-ae5b-461c-a2ed-5a8dcffef9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 500])\n",
      "LatticeBertModel ATT MASK SHAPE\n",
      "LatticeBertEncoder ATT MASK SHAPE\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "1\n",
      "2\n",
      "0\n",
      "YO\n",
      "LatticeBertModel ATT MASK SHAPE\n",
      "LatticeBertEncoder ATT MASK SHAPE\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n",
      "LatticeBertAttention ATT MASK SHAPE\n",
      "ATT MASK SHAPE\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from encoding_utils import *\n",
    "\n",
    "# get gold labels for the exploded set\n",
    "dsetx, dsety = prepare_dataset([exploded])\n",
    "print(len(dsetx))\n",
    "#print(torch.cuda.memory_allocated(\"cuda:2\"))\n",
    "assert len(dsetx)==1\n",
    "sents, posids = create_inputs([flat_toy])\n",
    "latposylabels, tmaps = lattice_pos_goldlabels(dsetx, dsety, sents)\n",
    "\n",
    "#latposylabels = tmap_pos_goldlabels(tmaps, sents)\n",
    "\n",
    "# get generated labels\n",
    "pred2 = posbmodel(sents.to(device), fix_posids(posids).to(device), torch.stack([mask]).to(device))\n",
    "\n",
    "# check to see if there are errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a512f5e-0be4-42e2-90ac-cec1f0e317ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 44])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a412dd25-9660-48b6-a334-68342357a7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_accuracy(setpred, setlabels):\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "    for i in range(0, len(setpred)):\n",
    "        ex = setpred[i]\n",
    "        for j in range(0, len(ex)):\n",
    "            if sum(setlabels[i][j])==0:\n",
    "                continue\n",
    "            elif torch.argmax(ex[j])==0:\n",
    "                continue\n",
    "            tot+=1\n",
    "            if torch.argmax(ex[j])==torch.argmax(setlabels[i][j]):\n",
    "                cor+=1\n",
    "    return cor/tot\n",
    "\n",
    "check_accuracy(pred2, latposylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7110d1a5-5db6-459b-ab63-0a11743fe720",
   "metadata": {},
   "outputs": [],
   "source": [
    "lablist = [k for k in labels.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cee29f3a-4d34-43aa-a651-264478f90169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'RB', 'NNS', '.']\n",
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'PRP', 'NNS', '.']\n"
     ]
    }
   ],
   "source": [
    "def show_labels (pred):\n",
    "    res = []\n",
    "    for p in pred:\n",
    "        res.append(lablist[torch.argmax(p)])\n",
    "    return res\n",
    "\n",
    "print(show_labels(pred2[0])[:8])\n",
    "print(show_labels(latposylabels[0])[:8])\n",
    "\"The fed raises interest rates him.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a47a1-6b7f-4c78-b7df-5fb4bfe26725",
   "metadata": {},
   "outputs": [],
   "source": [
    "indivlabs = posbmodel(dsetx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c211410b-8afd-4cd0-9f02-8552b88db575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'DT', 'NNP', 'VBZ', 'NN', 'PRP', '.', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(show_labels(indivlabs[0])[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd671e-8dad-44ef-910a-dba5174cca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
