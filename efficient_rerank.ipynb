{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58ca6e-bcd0-4f8e-9c3b-3dfb947ade77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearPOSBertV1\n",
    "from encoding_utils import *\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "from mask_utils import *\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from distill_comet import XLMCometEmbeds, XLMCometRegressor\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "xlm_tok = fl.bert_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e66a54-cc6a-4e12-963b-fbef8fdca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V12 first attempt with inputs prepended\n",
    "#V14 final pipeline first attempt\n",
    "VNUM = 14\n",
    "MOD_NAME = 'bertonewayv1.pth'\n",
    "\n",
    "# specifies files for pre-loading\n",
    "LOADED = {\n",
    "    'amasks': 'attmasksallv'+str(VNUM)+'.pt',\n",
    "    'tmaps': 'tmapsmaskedv'+str(VNUM)+'/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baeca25-5712-4a9e-9623-ca376cae506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_input(pgraph, inp):\n",
    "    \n",
    "    inptoks = xlm_tok(inp).input_ids\n",
    "    # add in the <s> token\n",
    "    # we don't need the </s> token by how the mask works\n",
    "    inptoks.append(0)\n",
    "    posadd = len(inptoks)\n",
    "    inpflat = []\n",
    "    ind = 0\n",
    "    for i in range(len(inptoks)):\n",
    "        nl = []\n",
    "        inp = inptoks[i]\n",
    "        if i<(len(inptoks)-1):\n",
    "            nl.append(str(inptoks[i+1])+\" \"+str(ind+1))\n",
    "        inpflat.append({\n",
    "            'token_idx':inp, \n",
    "            'pos':ind,\n",
    "            'id': str(inp)+\" \"+str(ind),\n",
    "            'nexts':nl,\n",
    "            'score':0,\n",
    "        })\n",
    "        ind+=1\n",
    "    inpflat[-1]['nexts'].append(pgraph[0]['id'].split()[0]+\" \"+str(posadd))\n",
    "    \n",
    "    inpflat.extend(pgraph)\n",
    "    for i in range(posadd, len(inpflat)):\n",
    "        extok = inpflat[i]\n",
    "        extok['pos']+=posadd\n",
    "        extok['id']= str(extok['token_idx'])+\" \"+str(extok['pos'])\n",
    "        for j in range(len(extok['nexts'])):\n",
    "            newpos = int(extok['nexts'][j].split()[1])+posadd\n",
    "            extok['nexts'][j] = extok['nexts'][j].split()[0]+\" \"+str(newpos)\n",
    "    return inpflat, posadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18febff-5025-40fa-b962-188a9b6f5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPS = -1\n",
    "\n",
    "# Get examples (just use the normal lattice examples ig?)\n",
    "processedgraphs, inps, refs = fl.get_processed_graph_data(fl.frenbase, -1, STOPS)\n",
    "\n",
    "# get exploded candidates to generate gold labels\n",
    "resarrs = [fl.get_cover_paths(p)[0] for p in processedgraphs]\n",
    "\n",
    "# extra step for greedy \n",
    "if STOPS==1:\n",
    "    processedgraphs = filter_greedy(processedgraphs)\n",
    "    \n",
    "\n",
    "# ensure no empty examples\n",
    "clean_empty(resarrs, processedgraphs)\n",
    "\n",
    "ppinput = [prepend_input(processedgraphs[i], inps[i]) for i in range(len(processedgraphs))]\n",
    "processedgraphs = [p[0] for p in ppinput]\n",
    "posadds = [p[1] for p in ppinput]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3c6d1-199e-4c8e-bc39-daaeafe6a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask (pgraph, padd):\n",
    "    start = connect_mat(processedgraphs[0])\n",
    "    start[:, :padd] = 1\n",
    "    start[:padd, padd:] = 0 \n",
    "    start[padd:, padd:] = torch.tril(start[padd:, padd:])\n",
    "    return start\n",
    "\n",
    "def get_allamasks():\n",
    "    attmasks = []\n",
    "    for i in range(len(posadds)):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        attmasks.append(causal_mask(processedgraphs[i], posadds[i]-1))\n",
    "    return attmasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55cfa2-2fbd-4d41-8cf1-45c6bf1536e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedgraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e53838-fb2c-4323-9fc4-155582cac86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validnext (pos, nlist):\n",
    "    retval = \"\"\n",
    "    for n in nlist:\n",
    "        if pos< int(n.split()[1]):\n",
    "            retval = n\n",
    "            if \"2 \" not in n:\n",
    "                return retval\n",
    "    if len(retval)>0:\n",
    "        return retval\n",
    "    print(\"no valid\")\n",
    "    print(pos)\n",
    "    print(nlist)\n",
    "    return \"\"\n",
    "\n",
    "def p_wnext(pgraph):\n",
    "    nid = '0 0'\n",
    "    for tokd in pgraph:\n",
    "        if tokd['id']==nid:\n",
    "            print(xlm_tok.decode(tokd['token_idx']))\n",
    "            nid = get_validnext(tokd['pos'], tokd['nexts'])\n",
    "            \n",
    "            #print(tokd)\n",
    "    print(nid)\n",
    "            \n",
    "p_wnext(processedgraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7b26a-215f-45d9-b9be-c0c8b4b342b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask code (TODO needs some updating)\n",
    "if os.path.exists('./torchsaved/'+LOADED['amasks']):\n",
    "        print(\"using loaded masks\")\n",
    "        attmasks = torch.load('./torchsaved/'+LOADED['amasks']).to(device)\n",
    "else:\n",
    "    print(\"creating new masks\")\n",
    "    masktmp = get_allamasks()\n",
    "    attmasks = torch.stack(masktmp).to(device)\n",
    "    torch.save(attmasks, './torchsaved/'+LOADED['amasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29dc3b-7840-43ed-97d9-183e322747c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenized inputs with posids (TODO needs an update for src/tgt format)\n",
    "sents, posids = create_inputs(processedgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe0183-9808-4c17-b9ca-df7951591072",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents.to(device)\n",
    "posids.to(device)\n",
    "attmasks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b4139-d8de-4936-bf1b-061fcf3fbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "# Returns Token Scores For Each (Sum becomes Regression)\n",
    "class XLMCometEmbeds(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        # TODO should we be freezing layers?\n",
    "        super().__init__()\n",
    "        \n",
    "        self.xlmroberta = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "        # Num labels 1 should just indicate regression (?)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.xlmroberta.config.hidden_size, 1), \n",
    "        )\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, positions, attention_masks):\n",
    "        # don't finetune xlmroberta model\n",
    "        #with torch.no_grad():\n",
    "        word_rep, sentence_rep = self.xlmroberta(input_ids, position_ids = positions, attention_mask=attention_masks, encoder_attention_mask=attention_masks, return_dict=False)\n",
    "        # use the first <s> token as a CLS token, TODO experiment with using the sum of \n",
    "        # ensure padding not factored in\n",
    "        #word_rep = word_rep*(input_ids>0).unsqueeze(-1)\n",
    "        res = []\n",
    "        for w in word_rep:\n",
    "            res.append(self.regressor(w))\n",
    "            \n",
    "        word_rep = word_rep*(input_ids>0).unsqueeze(-1)\n",
    "        #outputs = self.regressor(torch.sum(word_rep, 1))\n",
    "        #print(\"Shape: \", outputs.shape)\n",
    "        return word_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a262f3-467c-4b45-b847-694c3d79f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2d299-0240-415e-aa5d-fcdbd893c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLMCometEmbeds(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/maskedcont3.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd5c94-9d65-41bf-a505-717150931e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.regressor[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e26ee-f559-4565-acfa-dd2856ab6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.regressor[1].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17444fb-2374-4c20-8128-536fb0ae1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outs = model(sents, posids, attmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1db79-a788-4ead-8deb-5b1cb9dadfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.inner(outs, model.regressor[1].weight).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "949da490-312f-4db8-be82-9f78bdce0571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0b7c4aef-2098-43d0-812f-f5c6a582c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "55\n",
      "499\n",
      "55\n",
      "498\n",
      "57\n",
      "499\n",
      "57\n",
      "498\n",
      "82\n",
      "499\n",
      "82\n",
      "498\n",
      "90\n",
      "499\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# set scores computed for each token by the model\n",
    "def set_pgscores(pgraphs, scores):\n",
    "    #idlist = get_idlist(pgraph)\n",
    "    for p in range(len(pgraphs)):\n",
    "        pgraph = pgraphs[p]\n",
    "        for i in range(min(len(pgraph), 500)):\n",
    "            pgraph[i]['score'] = scores[p][i]\n",
    "            if pgraph[i]['token_idx']>0:\n",
    "                if pgraph[i]['score']==0:\n",
    "                    print(i)\n",
    "                    print(p)\n",
    "    return pgraphs\n",
    "\n",
    "# topological sort the graphs, make sure that nodes that are next always come next in the list\n",
    "def topo_sort_pgraph(pgraph):\n",
    "    # reverse ordering\n",
    "    pgraph.reverse()\n",
    "    # for all tokens\n",
    "    i = 0\n",
    "    while i < min(len(pgraph), 500):\n",
    "        ns = pgraph[i]\n",
    "        # check if any tokens that come after actually should be before\n",
    "        for j in range(i, min(len(pgraph), 500)):\n",
    "            # if so, re-insert right before in list\n",
    "            if pgraph[j]['id'] in ns:\n",
    "                tmp = pgraph[j]\n",
    "                del pgraph[j]\n",
    "                pgraph.insert(i, tmp)\n",
    "                i+=1\n",
    "        i+=1\n",
    "        \n",
    "def prepare_pgraphs(pgraphs, scores):\n",
    "    res = []\n",
    "    # make a deep copy of processed graphs\n",
    "    for p in pgraphs:\n",
    "        res.append([x for x in p])\n",
    "    # set scores for stuff\n",
    "    set_pgscores(res, scores)\n",
    "    # do topological sorting\n",
    "    for r in res:\n",
    "        topo_sort_pgraph(r)\n",
    "    return res\n",
    "    \n",
    "    \n",
    "\n",
    "prepared_pgraphs = prepare_pgraphs(processedgraphs, scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8de87057-2400-454b-ab96-6659b555872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1371, device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# given a list of sub-scores (topological flattening of the graph), use dp to get the highest scoring path\n",
    "# idlist has the corresponding graph ids for \n",
    "# would need to do a sort on pgrapaps that makes sure that no next node is before in the linear ordering\n",
    "# reverse since we're using nexts\n",
    "# TODO simplify code to not need so many data structures\n",
    "def dp_best_path(pgraphs, graph):\n",
    "    bplist = []\n",
    "    bsco_list =[]\n",
    "    idlist = get_idlist(pgraphs)\n",
    "    for i in range(len(idlist)):\n",
    "        bpath = []\n",
    "        cur = pgraphs[i]\n",
    "            \n",
    "        # get the highest prev from ahead to use\n",
    "        mval = -10\n",
    "        maxnext = None\n",
    "        for n in cur['nexts']:\n",
    "            try:\n",
    "                if graph[n]['bestsco']>mval:\n",
    "                    mval = graph[n]['bestsco']\n",
    "                    maxnext = graph[n]\n",
    "            except:\n",
    "                \"\"\n",
    "\n",
    "        # add in scores / path from that prev\n",
    "        if maxnext==None:\n",
    "            bpath.append(i)\n",
    "            bplist.append(bpath)\n",
    "            bsco_list.append(cur['score'])\n",
    "            # check if this is how things work in python\n",
    "            graph[cur['id']]['bestsco'] = cur['score']\n",
    "            graph[cur['id']]['plist'] = bpath\n",
    "            continue\n",
    "        bpath.extend(maxnext['plist']+[i])\n",
    "        bplist.append(bpath)\n",
    "        bsco_list.append(cur['score']+mval)\n",
    "        graph[cur['id']]['bestsco'] = cur['score']+mval\n",
    "        graph[cur['id']]['plist'] = bpath\n",
    "        #print(bpath)\n",
    "    return bplist[-1], bsco_list[-1]\n",
    "\n",
    "def get_idlist(pgraph):\n",
    "    return [p['id'] for p in pgraph]\n",
    "\n",
    "def dp_pgraph(pgraph):\n",
    "    graph = {}\n",
    "    for p in pgraph:\n",
    "        # TODO check if scores are negative number compatible\n",
    "        p['bestsco'] = 0\n",
    "        p['plist'] = []\n",
    "        graph[p['id']] = p\n",
    "    bestpath, bso = dp_best_path(pgraph, graph)\n",
    "    print(bsco)\n",
    "    bestpath.reverse()\n",
    "    return [pgraph[x]['token_idx'] for x in bestpath]\n",
    "        \n",
    "    \n",
    "bp = dp_pgraph(prepared_pgraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8ef365f4-ffa7-477a-901e-e0781980f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1371, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "<s> Mais le coût du nouveau vaccin devrait être bien inférieur car il transforme les cellules du foie en usines à anticorps.</s><s> But the new vaccine should cost much less because it turns liver cells into antibody factories.\n",
      "But the cost of the new vaccine is likely to be far lower, because it turns liver cells into antibody factories.\n"
     ]
    }
   ],
   "source": [
    "examplenum = 21\n",
    "print(xlm_tok.decode(dp_pgraph(prepared_pgraphs[examplenum])))\n",
    "print(refs[examplenum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "75d64186-97d3-41c5-b8a3-a6416c002f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_idx': 0, 'pos': 0, 'id': '0 0', 'nexts': ['636 1'], 'score': tensor(0., device='cuda:1', grad_fn=<SelectBackward0>), 'bestsco': tensor(0., device='cuda:1', grad_fn=<SelectBackward0>), 'plist': [364]}\n",
      "{'token_idx': 0, 'pos': 49, 'id': '0 49', 'nexts': ['581 50'], 'score': tensor(0., device='cuda:1', grad_fn=<SelectBackward0>), 'bestsco': tensor(0., device='cuda:1', grad_fn=<SelectBackward0>), 'plist': [315]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for t in processedgraphs[0]:\n",
    "    if t['token_idx']==0:\n",
    "        print(t)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da7ecc-d06a-491d-8507-c61bfcc93fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ea641-ce74-4cb7-9c5d-e8dafb4ba9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedgraphs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc6a49-81b8-4005-90e1-3fd0ba81715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107809d6-014b-4d85-9463-2adfcbb8db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "' </s> en_XX The US President was to receive Iraq i Prime Minister No uri Al Malik i Friday , November 1 , 2013 in an effort to se ek US assistance in fighting the worst wa ve of violence in five years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f68d6-96dd-4311-893e-99e8298bb5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
