{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a58ca6e-bcd0-4f8e-9c3b-3dfb947ade77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 08:32:16.283049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-24 08:32:16.283071: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import flatten_lattice as fl\n",
    "import torch\n",
    "from bert_models import LinearPOSBertV1\n",
    "from encoding_utils import *\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "from mask_utils import *\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "xlm_tok = fl.bert_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e66a54-cc6a-4e12-963b-fbef8fdca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "VNUM = 11\n",
    "MOD_NAME = 'bertonewayv1.pth'\n",
    "\n",
    "# specifies files for pre-loading\n",
    "LOADED = {\n",
    "    'amasks': 'attmasksallv'+str(VNUM)+'.pt',\n",
    "    'tmaps': 'tmapsmaskedv'+str(VNUM)+'/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18febff-5025-40fa-b962-188a9b6f5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPS = -1\n",
    "\n",
    "# Get examples (just use the normal lattice examples ig?)\n",
    "processedgraphs = fl.get_processed_graph_data(fl.frenbase, -1, STOPS)\n",
    "\n",
    "# get exploded candidates to generate gold labels\n",
    "resarrs = [fl.get_cover_paths(p)[0] for p in processedgraphs]\n",
    "\n",
    "# extra step for greedy \n",
    "if STOPS==1:\n",
    "    processedgraphs = filter_greedy(processedgraphs)\n",
    "\n",
    "# ensure no empty examples\n",
    "clean_empty(resarrs, processedgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cfc9c06-aef0-43dc-ae74-2da45ce94680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedgraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3dc5a7-19a5-4318-914e-923759308394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using loaded masks\n"
     ]
    }
   ],
   "source": [
    "# Attention mask code (TODO needs some updating)\n",
    "if os.path.exists('./torchsaved/'+LOADED['amasks']):\n",
    "        print(\"using loaded masks\")\n",
    "        attmasks = torch.load('./torchsaved/'+LOADED['amasks']).to(device)\n",
    "else:\n",
    "    print(\"creating new masks\")\n",
    "    masktmp = [connect_mat(p) for p in processedgraphs]\n",
    "    attmasks = torch.stack(masktmp).to(device)\n",
    "    torch.save(attmasks, './torchsaved/'+LOADED['amasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139afa72-166d-4226-90ec-4b87d608f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenized inputs with posids (TODO needs an update for src/tgt format)\n",
    "sents, posids = create_inputs(processedgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d64186-97d3-41c5-b8a3-a6416c002f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_idx': 0, 'pos': 0, 'id': '0 0', 'nexts': ['581 1'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 15, 'id': '0 15', 'nexts': ['41626 16'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['1210 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['24159 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 15, 'id': '0 15', 'nexts': ['98 16'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 17, 'id': '0 17', 'nexts': ['106 18'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 19, 'id': '0 19', 'nexts': ['1210 20'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['33938 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['142 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 19, 'id': '0 19', 'nexts': ['24159 20'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['191618 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['23 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['10 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 17, 'id': '0 17', 'nexts': ['6 18'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 18, 'id': '0 18', 'nexts': ['106 19'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['191618 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 18, 'id': '0 18', 'nexts': ['7582 19'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['6138 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['24159 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['6 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['23 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['1210 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['81887 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['71834 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['7413 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['4358 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 31, 'id': '0 31', 'nexts': ['30641 32'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 31, 'id': '0 31', 'nexts': ['78431 32'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 33, 'id': '0 33', 'nexts': ['129058 34'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 33, 'id': '0 33', 'nexts': ['37515 34'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 33, 'id': '0 33', 'nexts': ['130481 34'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['40 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['24159 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['191618 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['36442 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['8060 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['47 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['23 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['4358 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['127557 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['30641 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['181952 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['23 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 24, 'id': '0 24', 'nexts': ['142 25'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 24, 'id': '0 24', 'nexts': ['33938 25'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['127557 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 31, 'id': '0 31', 'nexts': ['47 32'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 32, 'id': '0 32', 'nexts': ['30641 33'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 32, 'id': '0 32', 'nexts': ['78431 33'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 31, 'id': '0 31', 'nexts': ['23 32'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 32, 'id': '0 32', 'nexts': ['181952 33'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['14098 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['36442 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['127557 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['4358 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 29, 'id': '0 29', 'nexts': ['7082 30'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 29, 'id': '0 29', 'nexts': ['14098 30'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 34, 'id': '0 34', 'nexts': ['130481 35'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 34, 'id': '0 34', 'nexts': ['37515 35'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['345 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['7082 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['8060 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['23 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['47 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['36442 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 29, 'id': '0 29', 'nexts': ['30641 30'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 29, 'id': '0 29', 'nexts': ['78431 30'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 27, 'id': '0 27', 'nexts': ['127557 28'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 20, 'id': '0 20', 'nexts': ['6 21'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['23 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['10 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['33938 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['142 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 21, 'id': '0 21', 'nexts': ['1210 22'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['47 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['237 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 24, 'id': '0 24', 'nexts': ['221 25'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 24, 'id': '0 24', 'nexts': ['40 25'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['70 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['10 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['100 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['47 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 26, 'id': '0 26', 'nexts': ['40 27'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['23295 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 28, 'id': '0 28', 'nexts': ['7082 29'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['33938 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 25, 'id': '0 25', 'nexts': ['345 26'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 22, 'id': '0 22', 'nexts': ['191618 23'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['127557 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['4358 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['14098 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['345 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['1919 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 23, 'id': '0 23', 'nexts': ['142 24'], 'score': 0}\n",
      "{'token_idx': 0, 'pos': 24, 'id': '0 24', 'nexts': ['81887 25'], 'score': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "for t in processedgraphs[0]:\n",
    "    if t['token_idx']==0:\n",
    "        print(t)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33da7ecc-d06a-491d-8507-c61bfcc93fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tok.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c0ea641-ce74-4cb7-9c5d-e8dafb4ba9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_idx': 0, 'pos': 0, 'id': '0 0', 'nexts': ['581 1'], 'score': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedgraphs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f24299a5-de76-4ab9-a7bf-808567e56a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "testgraph = None\n",
    "# debug the flatten_lattice method\n",
    "def get_proc_graph_data(lanbase, stop=-1, msplits=-1):\n",
    "    # variable tracks during lattice decoding and stops after a certain point\n",
    "    global max_splits\n",
    "    global splits_hit\n",
    "    global testgraph\n",
    "    max_splits = msplits\n",
    "    base = fl.GBASE+lanbase\n",
    "    paths = os.listdir(base)\n",
    "    result = []\n",
    "    if stop==-1:\n",
    "        stop = len(paths)\n",
    "    # get flattened version of lattice for each lattice in directory\n",
    "    for i in range(0, stop):\n",
    "        splits_hit=0\n",
    "        curgraph = fl.load_graph(base+paths[i])\n",
    "        testgraph = curgraph\n",
    "        # get rid of duplicate nodes TODO [happens surprisingly often?]\n",
    "        result.append(fl.remove_duplicates(flatten_lat(curgraph)))\n",
    "        break\n",
    "    return result\n",
    "\n",
    "# flatten out lattice \n",
    "def flatten_lat(graph):\n",
    "    tokdicts = []\n",
    "    visited = []\n",
    "    prev_contig = []\n",
    "    greedy_flatten(tokdicts, visited, graph['root'], 0, prev_contig, set())\n",
    "    #greedy_flat_old(tokdicts, visited, graph['root'], 0)\n",
    "    return tokdicts\n",
    "\n",
    "max_splits = -1\n",
    "splits_hit = 0\n",
    "# flattens graph by position, ignores </s> and en_XX tokens for greater BERT compatibility\n",
    "# TODO set up to use mbart tokenization\n",
    "def greedy_flatten(tdicts, visited, node, pos, prev_cont, added_ids, branch_start=None):\n",
    "    global splits_hit\n",
    "    if node.uid in visited:\n",
    "        return\n",
    "    if node.token_idx==2 or node.token_idx==250004:\n",
    "        npos = pos\n",
    "    else:\n",
    "        node.pos = pos\n",
    "        \n",
    "        visited.append(node.uid)\n",
    "        npos = pos+1\n",
    "        s = node.token_str\n",
    "        prev_cont.append(node)\n",
    "    \n",
    "    olen = len(tdicts)\n",
    "    # we're hitting a branch or an ending, update to bert tokenization and add to visited\n",
    "    # should be ok to do this since branching / merging only happens at word boundaries (presumably)\n",
    "    branched = (len(node.next_scores)>1)\n",
    "    end = (len(node.next_scores)==0)\n",
    "    merge = end==False and node.nextlist[0].uid in visited\n",
    "    if branched or merge or end:\n",
    "        #print(branched, \" \", merge, \" \", end)\n",
    "        if len(prev_cont)>0:\n",
    "            errorflag = False\n",
    "            \n",
    "            prev_update = []\n",
    "            for p in prev_cont:\n",
    "                if p.uid in added_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    prev_update.append(p)\n",
    "                    added_ids.add(p.uid)\n",
    "            \n",
    "            if len(prev_update)>0:\n",
    "                toktmp = fl.get_toklist(prev_update)\n",
    "                for i in range(1, len(prev_update)):\n",
    "                    if prev_update[-(i+1)].pos>=prev_update[-i].pos:\n",
    "                        errorflag = True\n",
    "                        break\n",
    "                decstr = mbart_tok.decode(toktmp)\n",
    "                if errorflag:\n",
    "                    #print(decstr)\n",
    "                    #print([p.pos for p in prev_update])\n",
    "                    \"\"\n",
    "                bert_toks = bert_tok(decstr).input_ids\n",
    "                curpos = prev_update[0].pos\n",
    "                # TODO add logic that tracks scores / next nodes\n",
    "                otdlen = len(tdicts)\n",
    "                for bind in range(0, len(bert_toks)):\n",
    "                    b = bert_toks[bind]\n",
    "                    if b==0 or b==2:\n",
    "                        continue\n",
    "                    nid = str(b)+\" \"+str(curpos)\n",
    "                    # if we're at the start, add this node to next of branch node\n",
    "                    if len(tdicts)==otdlen and branch_start is not None:\n",
    "                        branch_start['nexts'].append(nid)\n",
    "\n",
    "                    if bind<len(bert_toks)-1:\n",
    "                        tdicts.append({\n",
    "                            'token_idx':b,\n",
    "                            'pos':curpos, \n",
    "                            'id': nid,\n",
    "                            'nexts': [str(bert_toks[bind+1])+\" \"+str(curpos+1)], \n",
    "                            'score': 0\n",
    "                        })\n",
    "                    else:\n",
    "                        tdicts.append({\n",
    "                            'token_idx':b,\n",
    "                            'pos':curpos, \n",
    "                            'id': str(b)+\" \"+str(pos),\n",
    "                            'nexts': [], \n",
    "                            'score': 0\n",
    "                        })\n",
    "                    curpos+=1\n",
    "                if merge or end:\n",
    "                    splits_hit+=1\n",
    "            \n",
    "    if len(tdicts)>olen:\n",
    "        del prev_cont\n",
    "        prev_cont = []\n",
    "        \n",
    "    # end things early if we want to limit paths\n",
    "    if max_splits>=0 and splits_hit>=max_splits:\n",
    "        return \n",
    "    \n",
    "    scosort = list(np.argsort(node.next_scores))\n",
    "    if branched and len(tdicts)>0:\n",
    "        branch_start=tdicts[-1]\n",
    "    # TODO check which direction we need to go from argsort\n",
    "    for i in range(0, len(scosort)):\n",
    "        greedy_flatten(tdicts, visited, node.nextlist[scosort[i]], npos, prev_cont, added_ids, branch_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b25c8c76-588b-4384-a503-9f7637bbfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'Friday']\n",
      "[',', '1']\n",
      "['November', '1']\n",
      "[',', 'st']\n",
      "['2013,', '2013', 'in', 'seeking']\n",
      "['in', 'seeking', 'as', 'to']\n",
      "['search', 'an', 'a', 'the', 'his']\n",
      "['US', 'United', 'help', 'assistance', 'U']\n",
      "['assistance', 'aid', 'support', 'help']\n",
      "['to', 'in']\n",
      "['fight', 'combat']\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "cur = testgraph['root']\n",
    "while len(cur.nextlist)>0:\n",
    "    if len(cur.nextlist)>1:\n",
    "        print([c.token_str for c in cur.nextlist])\n",
    "    s+=\" \"+cur.token_str\n",
    "    cur = cur.nextlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddbc6a49-81b8-4005-90e1-3fd0ba81715a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 0, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tok(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107809d6-014b-4d85-9463-2adfcbb8db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "' </s> en_XX The US President was to receive Iraq i Prime Minister No uri Al Malik i Friday , November 1 , 2013 in an effort to se ek US assistance in fighting the worst wa ve of violence in five years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f68d6-96dd-4311-893e-99e8298bb5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
