{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad833c3b-f011-491e-a028-f79b9f053bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 10:39:01.986431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-15 10:39:01.986453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570a72d9-ebec-4a3f-8645-bd1d7a259028",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBASE = \"./reverse_graphs/\"\n",
    "endebase = \"mt1n_en-de_bfs_recom_4_80_False_0.4_True_False_4_5_rcb_0.9_0.0_0.9/\"\n",
    "frenbase = \"mtn1_fr-en_bfs_recom_4_-1_False_0.4_True_False_4_5_rcb_0.91_0.0_0.9/\"\n",
    "\n",
    "def load_graph(fname):\n",
    "    return pickle.load(open(fname,'rb'))\n",
    "\n",
    "def flatten_lattice(graph):\n",
    "    tokdicts = []\n",
    "    visited = []\n",
    "    poslist = []\n",
    "    greedy_flatten(tokdicts, visited, graph['root'], 0)\n",
    "    return tokdicts\n",
    "    \n",
    "def greedy_flatten(tdicts, visited, node, pos):\n",
    "    if node.uid in visited:\n",
    "        print(\"cycle here\")\n",
    "        return\n",
    "    node.pos = pos\n",
    "    tdicts.append(node)\n",
    "    visited.append(node.uid)\n",
    "    \n",
    "    scosort = list(np.argsort(node.next_scores))\n",
    "    \n",
    "    # TODO check which direction we need to go from argsort\n",
    "    for i in range(0, len(scosort)):\n",
    "        greedy_flatten(tdicts, visited, node.nextlist[scosort[i]], pos+1)\n",
    "        \n",
    "def get_processed_graph_data(lanbase, stop=-1):\n",
    "    base = GBASE+lanbase\n",
    "    paths = os.listdir(base)\n",
    "    print(len(paths))\n",
    "    result = []\n",
    "    if stop==-1:\n",
    "        stop = len(paths)\n",
    "    for i in range(0, stop):\n",
    "        curgraph = load_graph(base+paths[i])\n",
    "        result.append(flatten_lattice(curgraph))\n",
    "    return result\n",
    "\n",
    "def greedy_path(flat):\n",
    "    prev = -1\n",
    "    res = []\n",
    "    for f in flat:\n",
    "        if f.pos>prev:\n",
    "            res.append(f)\n",
    "            prev = f.pos\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb15518-48b0-4b8e-b79d-f74736842a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbart_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "mbart_tok.src_lang = \"en_XX\"\n",
    "\n",
    "def find_paths(root):\n",
    "    global nodeset\n",
    "    #print(root.token_str)\n",
    "    if len(root.nextlist) == 0:\n",
    "        yield [root]\n",
    "\n",
    "    scosort = list(np.argsort(root.next_scores))\n",
    "    \n",
    "    seen = []\n",
    "    for s in scosort:\n",
    "        child = root.nextlist[s]\n",
    "        if child.uid in seen:\n",
    "            continue\n",
    "        nodeset.add(child.uid)\n",
    "        #if len(seen)>1:\n",
    "            #print(\"maybe not bug\")\n",
    "        seen.append(child.uid)\n",
    "        for path in find_paths(child):\n",
    "            yield [root] + path\n",
    "            \n",
    "def get_plist_sco(plist):\n",
    "    res = []\n",
    "    for p in plist:\n",
    "        res.append(p.prob)\n",
    "    return res\n",
    "\n",
    "def get_plist_str(plist):\n",
    "    res = []\n",
    "    for p in plist:\n",
    "        res.append(p.token_idx)\n",
    "    val =  mbart_tok.decode(res)\n",
    "    #print(val)\n",
    "    return val\n",
    "\n",
    "nodeset = set()\n",
    "STOP = 1000\n",
    "def get_all_possible_candidates(graph):\n",
    "    global nodeset\n",
    "    scores =  []\n",
    "    cands = []\n",
    "    fullplist = []\n",
    "    generated = 0\n",
    "    \n",
    "    for p in find_paths(graph['root']):\n",
    "        if generated == STOP:\n",
    "            break\n",
    "        fullplist.append(p)\n",
    "        generated+=1\n",
    "    print(\"num nodes\")\n",
    "    print(len(nodeset))\n",
    "    nodeset = set()\n",
    "    #fullplist = remove_dups(fullplist)\n",
    "    print(\"candidates\")\n",
    "    print(len(fullplist))\n",
    "    for plist in fullplist:\n",
    "        #scores.append(get_plist_sco(plist))\n",
    "        cands.append(get_plist_str(plist))\n",
    "    \n",
    "    # TODO some kind of filtration that prevents super similar or bad stuff from being used\n",
    "    return cands\n",
    "    \n",
    "def get_allcands(lanbase, stop=-1, res=[]):\n",
    "    base = GBASE+lanbase\n",
    "    paths = os.listdir(base)\n",
    "    print(len(paths))\n",
    "    if stop==-1:\n",
    "        stop = len(paths)\n",
    "    for i in range(0, stop):\n",
    "        try:\n",
    "            curgraph = load_graph(base+paths[i])\n",
    "            res.append(get_all_possible_candidates(curgraph))\n",
    "        except:\n",
    "            print(\"hit recursion limit\")\n",
    "            res.append([])\n",
    "    return res\n",
    "        #result.append(flatten_lattice(curgraph))\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451865b-68f1-4f4c-9d54-1f9eaa6afccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resarrs = []\n",
    "get_allcands(frenbase, -1, resarrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedgraphs = get_processed_graph_data(frenbase, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8420b188-e8d8-446e-a923-426326a3dc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama receives Iraq’s prime minister in the midst of escalating violence.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resarrs[4][1][10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f21a676-b1fa-40dc-b954-839909d5b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = AutoTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991b14d0-028c-4150-9b86-8a83dd52d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>en_XX I think there are two levels of response by the French Government.. on the part of the French Government.'s response. from the French Government believe there are two that there are two levels of response by the French Government.. on the part of the French Government. from the French Government In my view there are two, the French Government has responded at two levels. two levels. of response to this from the Commission. by the French Government.. on the French Government.’s part.'s part. part of the Government. of France. French authorities. Government:</s></s>. from the Government. of France. French Government: there are two levels to the French Government. response of the French from the of reply from the French the response from French reaction on the part from the French answer on the part from the French opinion there are two, the French there are\n",
      "</s>   </s>\n",
      "en_XX   en_XX\n",
      "I   I\n",
      "think   think\n",
      "there   there\n",
      "are   are\n",
      "two   two\n",
      "levels   levels\n",
      "of   of\n",
      "response   response\n",
      "by   by\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      ".   .\n",
      "on   on\n",
      "the   the\n",
      "part   part\n",
      "of   of\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      "'   '\n",
      "s   s\n",
      "response   response\n",
      ".   .\n",
      "from   from\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      "believe   believe\n",
      "there   there\n",
      "are   are\n",
      "two   two\n",
      "that   that\n",
      "there   there\n",
      "are   are\n",
      "two   two\n",
      "levels   levels\n",
      "of   of\n",
      "response   response\n",
      "by   by\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      ".   .\n",
      "on   on\n",
      "the   the\n",
      "part   part\n",
      "of   of\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      "from   from\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      "In   In\n",
      "my   my\n",
      "view   view\n",
      "there   there\n",
      "are   are\n",
      "two   two\n",
      ",   ,\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      "has   has\n",
      "responde   responde\n",
      "d   d\n",
      "at   at\n",
      "two   two\n",
      "levels   levels\n",
      ".   .\n",
      "two   two\n",
      "levels   levels\n",
      ".   .\n",
      "of   of\n",
      "response   response\n",
      "to   to\n",
      "this   this\n",
      "from   from\n",
      "the   the\n",
      "Commission   Commission\n",
      ".   .\n",
      "by   by\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      ".   .\n",
      "on   on\n",
      "the   the\n",
      "French   French\n",
      "Government   Government\n",
      ".   .\n",
      "’   ’\n",
      "s   s\n",
      "part   part\n",
      ".   .\n",
      "'   '\n",
      "s   s\n",
      "part   part\n",
      ".   .\n",
      "part   part\n",
      "of   of\n",
      "the   the\n",
      "Government   Government\n",
      ".   .\n",
      "of   of\n",
      "France   France\n",
      ".   .\n",
      "French   French\n",
      "authorities   authorities\n",
      ".   .\n",
      "Government   Government\n",
      ":   :\n",
      "</s>   </s>\n",
      "</s>   </s>\n",
      ".   \n",
      "from   .\n",
      "the   from\n",
      "Government   the\n",
      ".   Government\n",
      "of   .\n",
      "France   of\n",
      ".   France\n",
      "French   .\n",
      "Government   French\n",
      ":   Government\n",
      "there   :\n",
      "are   there\n",
      "two   are\n",
      "levels   two\n",
      "to   levels\n",
      "the   to\n",
      "French   the\n",
      "Government   French\n",
      ".   Government\n",
      "response   .\n",
      "of   response\n",
      "the   of\n",
      "French   the\n",
      "from   French\n",
      "the   from\n",
      "of   the\n",
      "reply   of\n",
      "from   reply\n",
      "the   from\n",
      "French   the\n",
      "the   French\n",
      "response   the\n",
      "from   response\n",
      "French   from\n",
      "reaction   French\n",
      "on   reaction\n",
      "the   on\n",
      "part   the\n",
      "from   part\n",
      "the   from\n",
      "French   the\n",
      "answer   French\n",
      "on   answer\n",
      "the   on\n",
      "part   the\n",
      "from   part\n",
      "the   from\n",
      "French   the\n",
      "opinion   French\n",
      "there   opinion\n",
      "are   there\n",
      "two   are\n",
      ",   two\n",
      "the   ,\n",
      "French   the\n",
      "there   French\n",
      "are   there\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_toklist(revnodes):\n",
    "    res = []\n",
    "    for r in revnodes:\n",
    "        res.append(r.token_idx)\n",
    "    return res\n",
    "\n",
    "def check_encsame(flat):\n",
    "    tlist = get_toklist(flat)\n",
    "    decstr = mbart_tok.decode(get_toklist(flat))\n",
    "    re_encoded = mbart_tok(decstr).input_ids\n",
    "    print(decstr)\n",
    "    for i in range(0, len(tlist)):\n",
    "        print(mbart_tok.decode(tlist[i]), \" \", mbart_tok.decode(re_encoded[i+1]))\n",
    "        if tlist[i]==re_encoded[i+1]:\n",
    "            continue\n",
    "        #print(tlist[i])\n",
    "        #print(re_encoded[i+1])\n",
    "        #return False\n",
    "    return True\n",
    "\n",
    "check_encsame(processedgraphs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9db895f8-39cd-4d55-8c5c-c4dc9f6fba3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbart_tok.decode(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5db6f67f-83f6-419e-b79d-fd8f1ec5580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>  -  0\n",
      "en_XX  -  1\n",
      "According  -  2\n",
      "to  -  3\n",
      "Google  -  4\n",
      ",  -  5\n",
      "the  -  6\n",
      "most  -  7\n",
      "wanted  -  8\n",
      "dis  -  9\n",
      "gu  -  10\n",
      "ises  -  11\n",
      "are  -  12\n",
      "zombie  -  13\n",
      "s  -  14\n",
      ".  -  15\n",
      "Batman  -  16\n",
      ",  -  17\n",
      "pirat  -  18\n",
      "es  -  19\n",
      "and  -  20\n",
      "wit  -  21\n",
      "ches  -  22\n",
      ".  -  23\n",
      "ches  -  24\n",
      ".  -  25\n",
      "wit  -  26\n",
      "ches  -  27\n",
      ".  -  28\n"
     ]
    }
   ],
   "source": [
    "def print_proctoks(revnodes):\n",
    "    for rev in revnodes:\n",
    "        print(rev.token_str, \" - \", rev.pos)\n",
    "\n",
    "print_proctoks(greedy_path(processedgraphs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ef2fb-6adc-4109-89ad-1677afb42794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c50528-ed67-471f-bd55-357971c75169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to work with\n",
    "text = (\n",
    "   'Hello, how are you? I am Romeo.\\n'\n",
    "   'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "   'Nice meet you too. How are you today?\\n'\n",
    "   'Great. My baseball team won the competition.\\n'\n",
    "   'Oh Congratulations, Juliet\\n'\n",
    "   'Thanks you Romeo'\n",
    ")\n",
    "\n",
    "# pre-processing text (all technically in one string)\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "# split by spaces\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57887b59-eb3e-477e-abfa-85d47054e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding special tokens\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "# create vocabulary dictionaries (word->token id mapping)\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "    number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "    vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62557b95-28b4-46bf-afa9-35c43c8a0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    \n",
    "    # keep things balanced? \n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # get 2 random sentences, we want to check if they're contiguous or not\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) \n",
    "        \n",
    "        # not sure where token_list comes from \n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        # create a list with all the appropriate token ids for the input formatting\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "        # handle segment embedding?\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        # get num of tokens to mask\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        \n",
    "        # randomly select 15% of tokens to mask\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            \n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            # usually just mask\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            # sometimes just put a random word in\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbbf9aa-bfdd-47a4-ac44-3146389710dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.arange(30, dtype=torch.long).expand_as(input_ids))\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fccc8ba-a91d-4cc5-82dd-65f396f26471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0464df-879b-4a06-8297-fd4af74596d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908eb183-766d-4942-993d-bae0cd302ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples = [\"Barack Obama receives Iraq’s prime minister in the midst of escalating violence.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a39cadb-8ba5-4047-a97e-e0d6d686a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:46:29.695833: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-15 07:46:29.695855: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant modules\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda:2\"\n",
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-base-cased',\n",
    "           output_hidden_states = True,).to(device)\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate\n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787d823f-f601-4918-a88a-a3baa6714c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(device)\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f143582a-452b-484e-a39c-54f82d966a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_embeddings = []\n",
    "\n",
    "for text in examples:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    # Find the position 'bank' in list of tokens\n",
    "    word_index = tokenized_text.index('Obama')\n",
    "    # Get the embedding for bank\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd15771d-5745-4282-88c4-66e36d66121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127978e-371a-49a2-84db-97fbb4cbe198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
