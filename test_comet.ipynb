{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27fd736-d0cb-49ef-b7e4-89f5521e05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from typing import List\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a9ef29-dd90-4f02-b3aa-7cb440bd4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cands(fname):\n",
    "    data = []\n",
    "    with open(fname, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            #print(line)\n",
    "            # if line is empty\n",
    "            # end of file is reached\n",
    "            if not line or len(line)<3:\n",
    "                break\n",
    "\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def process_cands(cand_data):\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    srcs = []\n",
    "    clen = len(cand_data['scores'])\n",
    "    # return refs, hyps, srcs\n",
    "    return [cand_data['ref']]*clen, cand_data['cands'], [cand_data['inp']]*clen\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e00ad29b-403d-4f4e-823b-3e60d86b973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache dir for cometqe model\n",
    "cometqe_dir = \"./cometqemodel\"\n",
    "cometqe_model = \"wmt20-comet-qe-da\"\n",
    "cometmodel = \"wmt20-comet-da\"\n",
    "batch_size = 64\n",
    "\n",
    "def get_flat(hyp_lists, srcs):\n",
    "    flat_hyps = [hyp for sent_hyps in hyp_lists for hyp in sent_hyps]\n",
    "    dup_srcs = [src for src in srcs for _ in range(len(hyp_lists[0]))]\n",
    "    return flat_hyps, dup_srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7487972-2b94-45b7-bf2c-ca26ea8bf400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-qe-da is already in cache.\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████| 654/654 [02:01<00:00,  5.39it/s]\n",
      "wmt20-comet-da is already in cache.\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Predicting: 100%|█████████████████████████████████████████████████████████████████| 17/17 [00:10<00:00,  1.56it/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Predicting: 100%|█████████████████████████████████████████████████████████████████| 17/17 [00:10<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3829936766940018\n",
      "0.33833002571916543\n"
     ]
    }
   ],
   "source": [
    "# script to run entire pipeline from a candidate jsonl file\n",
    "\n",
    "cometqe_path = download_model(cometqe_model, cometqe_dir)\n",
    "model = load_from_checkpoint(cometqe_path)\n",
    "cand_data = load_cands(\"./candoutputs/nucleuscand40.jsonl\")\n",
    "rerank_info = get_reranked_cands(cand_data)\n",
    "storererank = {}\n",
    "storererank['data'] = rerank_info\n",
    "f = open('nucleusrerank.json', 'w')\n",
    "json.dump(storererank, f)\n",
    "del model\n",
    "del cometqe_path\n",
    "comet_path = download_model(cometmodel, \"./cometmodel\")\n",
    "comet = load_from_checkpoint(comet_path)\n",
    "rrinfo = comet_rerank_info(rerank_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b1e5084-36ad-4f55-9017-1a82cc323f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-da is already in cache.\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "#cometqe_path = download_model(cometqe_model, cometqe_dir)\n",
    "#model = load_from_checkpoint(cometqe_path)\n",
    "comet_path = download_model(cometmodel, \"./cometmodel\")\n",
    "comet = load_from_checkpoint(comet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb5d52cc-2d83-4796-b4d4-be02e6293661",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = True\n",
    "allscoressaved = []\n",
    "def get_reranked_cands(c_data):\n",
    "    global allscores\n",
    "    reranked_info = []\n",
    "    ex_len = len(c_data[0]['scores'])\n",
    "    if load_data:\n",
    "        allhyps = []\n",
    "        allsrcs = []\n",
    "        \n",
    "        for i in range(0, len(c_data)):\n",
    "            r, h, s = process_cands(c_data[i])\n",
    "            allhyps.extend(h)\n",
    "            allsrcs.extend(s)\n",
    "\n",
    "        allscores = get_cometqe_scores(allhyps, allsrcs)\n",
    "    for i in range(0, len(c_data)):\n",
    "        tmp = {}\n",
    "        r, h, s = process_cands(c_data[i])\n",
    "        tmp['ref'] = r[0]\n",
    "        tmp['src'] = s[0]\n",
    "        tmp['topinitial'] = h[0]\n",
    "        #scoretmp = get_cometqe_scores(h, s)[0]\n",
    "        #print(scoretmp)\n",
    "        scoretmp = allscores[0][i*ex_len:ex_len*(i+1)]\n",
    "        comind = scoretmp.index(max(scoretmp))\n",
    "        tmp['reranked'] = h[comind]\n",
    "        tmp['initialsco'] = scoretmp[0]\n",
    "        tmp['rersco'] = scoretmp[comind]\n",
    "        reranked_info.append(tmp)\n",
    "    return reranked_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c87285f5-724f-44e1-9abf-6a6748624ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allscores[0])/800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29c40b8e-def5-4eed-ab2e-11a64ce08598",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_info = get_reranked_cands(cand_data)\n",
    "storererank = {}\n",
    "storererank['data'] = rerank_info\n",
    "f = open('rerankdata.json', 'w')\n",
    "json.dump(storererank, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db118f2d-7eee-4fae-8401-aacb4c5f0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rercomettmp = []\n",
    "firstcomettmp = []\n",
    "def comet_rerank_info(rrk_info):\n",
    "    global rercomettmp\n",
    "    global firstcomettmp\n",
    "    tophyps = []\n",
    "    rerhyps = []\n",
    "    refs = []\n",
    "    srcs = []\n",
    "    for r in rrk_info:\n",
    "        tophyps.append(r['topinitial'])\n",
    "        rerhyps.append(r['reranked'])\n",
    "        refs.append(r['ref'])\n",
    "        srcs.append(r['src'])\n",
    "    rer_comet_sco = get_comet_scores(rerhyps, srcs, refs)\n",
    "    first_comet_sco = get_comet_scores(tophyps, srcs, refs)\n",
    "    rercomettmp = rer_comet_sco\n",
    "    firstcomettmp = first_comet_sco\n",
    "    print(rer_comet_sco[1])\n",
    "    print(first_comet_sco[1])\n",
    "    return rer_comet_sco, first_comet_sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6edde-dee2-4ebe-9587-d3ae4e31fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_rerank_info(rerank_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8ec083d-2466-4c7a-9b36-e1bb15f62dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cometqe_scores(hyps, srcs):\n",
    "    cometqe_input = [{\"src\": src, \"mt\": mt} for src, mt in zip(srcs, hyps)]\n",
    "    # sentence-level and corpus-level COMET\n",
    "    outputs = model.predict(\n",
    "        cometqe_input, batch_size=40, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs\n",
    "\n",
    "def get_comet_scores(hyps, srcs, refs):\n",
    "    cometqe_input = [{\"src\": src, \"mt\": mt, \"ref\":ref} for src, mt, ref in zip(srcs, hyps, refs)]\n",
    "    # sentence-level and corpus-level COMET\n",
    "    outputs = comet.predict(\n",
    "        cometqe_input, batch_size=40, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs\n",
    "\n",
    "def test_cometqe(hyp, src):\n",
    "    cqe_input = [{'src':src, 'mt':hyp}]\n",
    "    outputs = model.predict(\n",
    "        cqe_input, batch_size=1, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5cd7d87-47f2-46e9-8227-6e9449b0e8a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m test_srcs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am doing very well today\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      5\u001b[0m test_hyps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     [\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeute geht es mir sehr gut\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# should be good\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMir geht es heute sehr schlecht\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# should be bad\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     ], \n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ft_hyps, ft_srcs \u001b[38;5;241m=\u001b[39m \u001b[43mget_flat\u001b[49m(test_hyps, test_srcs)\n\u001b[1;32m     13\u001b[0m get_cometqe_scores(ft_hyps, ft_srcs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_flat' is not defined"
     ]
    }
   ],
   "source": [
    "test_srcs = [\n",
    "    \"I am doing very well today\",\n",
    "]\n",
    "\n",
    "test_hyps = [\n",
    "    [\n",
    "        \"Heute geht es mir sehr gut\", # should be good\n",
    "        \"Mir geht es heute sehr schlecht\", # should be bad\n",
    "    ], \n",
    "]\n",
    "\n",
    "ft_hyps, ft_srcs = get_flat(test_hyps, test_srcs)\n",
    "get_cometqe_scores(ft_hyps, ft_srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f4d31b-0c56-48a8-852b-7bb51223eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def get_sample_set (num):\n",
    "    with open(\"translation_data/news-commentary-v15.de-en.tsv\") as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        i = 0\n",
    "        res = []\n",
    "        for f in tsv_file:\n",
    "            if i == num:\n",
    "                break\n",
    "            res.append(f)\n",
    "            i = i+1\n",
    "    tmpdf = pd.DataFrame(res)\n",
    "    tmpdf['de'] = tmpdf[0]\n",
    "    tmpdf['en'] = tmpdf[1]\n",
    "    del tmpdf[0]\n",
    "    del tmpdf[1]\n",
    "    return tmpdf\n",
    "\n",
    "data = get_sample_set(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be71c70-f850-4b67-9ca2-a5beec1aca7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steigt Gold auf 10.000 Dollar?</td>\n",
       "      <td>$10,000 Gold?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – Es war noch nie leicht, ein ra...</td>\n",
       "      <td>SAN FRANCISCO – It has never been easy to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In letzter Zeit allerdings ist dies schwierige...</td>\n",
       "      <td>Lately, with gold prices up more than 300% ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erst letzten Dezember verfassten meine Kollege...</td>\n",
       "      <td>Just last December, fellow economists Martin F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Und es kam, wie es kommen musste.</td>\n",
       "      <td>Wouldn’t you know it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Following its targeted killing of Iran's secon...</td>\n",
       "      <td>Following its targeted killing of Iran's secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>And that shift would occur at a time of growin...</td>\n",
       "      <td>And that shift would occur at a time of growin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>The targeted killing by the United States of o...</td>\n",
       "      <td>The targeted killing by the United States of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>After all, Iran and the US have already been a...</td>\n",
       "      <td>After all, Iran and the US have already been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>As in previous years, Project Syndicate asked ...</td>\n",
       "      <td>As in previous years, Project Syndicate asked ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     de  \\\n",
       "0                        Steigt Gold auf 10.000 Dollar?   \n",
       "1     SAN FRANCISCO – Es war noch nie leicht, ein ra...   \n",
       "2     In letzter Zeit allerdings ist dies schwierige...   \n",
       "3     Erst letzten Dezember verfassten meine Kollege...   \n",
       "4                     Und es kam, wie es kommen musste.   \n",
       "...                                                 ...   \n",
       "9995  Following its targeted killing of Iran's secon...   \n",
       "9996  And that shift would occur at a time of growin...   \n",
       "9997  The targeted killing by the United States of o...   \n",
       "9998  After all, Iran and the US have already been a...   \n",
       "9999  As in previous years, Project Syndicate asked ...   \n",
       "\n",
       "                                                     en  \n",
       "0                                         $10,000 Gold?  \n",
       "1     SAN FRANCISCO – It has never been easy to have...  \n",
       "2     Lately, with gold prices up more than 300% ove...  \n",
       "3     Just last December, fellow economists Martin F...  \n",
       "4                                 Wouldn’t you know it?  \n",
       "...                                                 ...  \n",
       "9995  Following its targeted killing of Iran's secon...  \n",
       "9996  And that shift would occur at a time of growin...  \n",
       "9997  The targeted killing by the United States of o...  \n",
       "9998  After all, Iran and the US have already been a...  \n",
       "9999  As in previous years, Project Syndicate asked ...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.to_csv('./translation_data/en_de.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1de6c10-1f46-479d-8c49-1ff5f3b63376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 07:30:20.181258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 07:30:20.181280: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# candidate generation code\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c4e492-a4ad-4fd1-8f74-0556696274ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_en = \"The head of the United Nations says there is no military solution in Syria\"\n",
    "\n",
    "\n",
    "# translate from English to French (test)\n",
    "\"\"\"\n",
    "generated_tokens = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"],\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    #top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=10\n",
    "    \n",
    ")\n",
    "\"\"\"\n",
    "bs_cands = []\n",
    "def get_bs_candidates(inputs, refs, per_example=10, batch=8):\n",
    "    global bs_cands\n",
    "    bs_cands = []\n",
    "    tmpfile = open(\"./candoutputs/beamsearchcand\"+str(per_example)+\".jsonl\", \"w\")\n",
    "    for i in range(0, int(len(inputs)/batch)):\n",
    "        print(i*batch)\n",
    "        ins = inputs[i*batch:batch*(i+1)]\n",
    "        ref = refs[i*batch:batch*(i+1)]\n",
    "        model_inputs = tokenizer(ins, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            **model_inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"],\n",
    "            num_beams=per_example,\n",
    "            num_return_sequences=per_example,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "\n",
    "        )\n",
    "        allsco = generated_tokens.sequences_scores\n",
    "        allcands = tokenizer.batch_decode(generated_tokens.sequences, skip_special_tokens=True)\n",
    "        for j in range(0, batch):\n",
    "            tmp = {}\n",
    "            tmp['scores'] = list(allsco[j*per_example:per_example*(j+1)])\n",
    "            tmp['scores'] = [tensor.item() for tensor in tmp['scores']]\n",
    "            tmp['cands'] = list(allcands[j*per_example:per_example*(j+1)])\n",
    "            tmp['inp'] = inputs[i*batch+j]\n",
    "            tmp['ref'] = refs[i*batch+j]\n",
    "            #print(tmp)\n",
    "            bs_cands.append(tmp)\n",
    "            tmpfile.write(json.dumps(tmp))\n",
    "            tmpfile.write('\\n')\n",
    "            \n",
    "        # ensure space is cleared\n",
    "        del model_inputs\n",
    "        del generated_tokens\n",
    "        del allsco\n",
    "        del allcands\n",
    "    tmpfile.close()\n",
    "    return bs_cands\n",
    "\n",
    "nuc_cands = []\n",
    "def get_nucleus_candidates(inputs, refs, per_example=10, batch=8):\n",
    "    global nuc_cands\n",
    "    nuc_cands = []\n",
    "    tmpfile = open(\"./candoutputs/nucleuscand\"+str(per_example)+\".jsonl\", \"w\")\n",
    "    for i in range(0, int(len(inputs)/batch)):\n",
    "        print(i*batch)\n",
    "        ins = inputs[i*batch:batch*(i+1)]\n",
    "        ref = refs[i*batch:batch*(i+1)]\n",
    "        model_inputs = tokenizer(ins, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            **model_inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"],\n",
    "            do_sample=True, \n",
    "            max_length=50, \n",
    "            top_k=50, \n",
    "            top_p=0.6, \n",
    "            num_return_sequences=per_example,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        allsco = generated_tokens.sequences_scores\n",
    "        allcands = tokenizer.batch_decode(generated_tokens.sequences, skip_special_tokens=True)\n",
    "        for j in range(0, batch):\n",
    "            tmp = {}\n",
    "            tmp['scores'] = list(allsco[j*per_example:per_example*(j+1)])\n",
    "            tmp['scores'] = [tensor.item() for tensor in tmp['scores']]\n",
    "            tmp['cands'] = list(allcands[j*per_example:per_example*(j+1)])\n",
    "            tmp['inp'] = inputs[i*batch+j]\n",
    "            tmp['ref'] = refs[i*batch+j]\n",
    "            #print(tmp)\n",
    "            nuc_cands.append(tmp)\n",
    "            tmpfile.write(json.dumps(tmp))\n",
    "            tmpfile.write('\\n')\n",
    "            \n",
    "        # ensure space is cleared\n",
    "        del model_inputs\n",
    "        del generated_tokens\n",
    "        del allsco\n",
    "        del allcands\n",
    "    tmpfile.close()\n",
    "    return nuc_cands\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf696f5-b5e2-4324-a203-a24dfa826ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab0f59d-9a51-4cc6-a741-706400d2a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "160\n",
      "162\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "172\n",
      "174\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "202\n",
      "204\n",
      "206\n",
      "208\n",
      "210\n",
      "212\n",
      "214\n",
      "216\n",
      "218\n",
      "220\n",
      "222\n",
      "224\n",
      "226\n",
      "228\n",
      "230\n",
      "232\n",
      "234\n",
      "236\n",
      "238\n",
      "240\n",
      "242\n",
      "244\n",
      "246\n",
      "248\n",
      "250\n",
      "252\n",
      "254\n",
      "256\n",
      "258\n",
      "260\n",
      "262\n",
      "264\n",
      "266\n",
      "268\n",
      "270\n",
      "272\n",
      "274\n",
      "276\n",
      "278\n",
      "280\n",
      "282\n",
      "284\n",
      "286\n",
      "288\n",
      "290\n",
      "292\n",
      "294\n",
      "296\n",
      "298\n",
      "300\n",
      "302\n",
      "304\n",
      "306\n",
      "308\n",
      "310\n",
      "312\n",
      "314\n",
      "316\n",
      "318\n",
      "320\n",
      "322\n",
      "324\n",
      "326\n",
      "328\n",
      "330\n",
      "332\n",
      "334\n",
      "336\n",
      "338\n",
      "340\n",
      "342\n",
      "344\n",
      "346\n",
      "348\n",
      "350\n",
      "352\n",
      "354\n",
      "356\n",
      "358\n",
      "360\n",
      "362\n",
      "364\n",
      "366\n",
      "368\n",
      "370\n",
      "372\n",
      "374\n",
      "376\n",
      "378\n",
      "380\n",
      "382\n",
      "384\n",
      "386\n",
      "388\n",
      "390\n",
      "392\n",
      "394\n",
      "396\n",
      "398\n",
      "400\n",
      "402\n",
      "404\n",
      "406\n",
      "408\n",
      "410\n",
      "412\n",
      "414\n",
      "416\n",
      "418\n",
      "420\n",
      "422\n",
      "424\n",
      "426\n",
      "428\n",
      "430\n",
      "432\n",
      "434\n",
      "436\n",
      "438\n",
      "440\n",
      "442\n",
      "444\n",
      "446\n",
      "448\n",
      "450\n",
      "452\n",
      "454\n",
      "456\n",
      "458\n",
      "460\n",
      "462\n",
      "464\n",
      "466\n",
      "468\n",
      "470\n",
      "472\n",
      "474\n",
      "476\n",
      "478\n",
      "480\n",
      "482\n",
      "484\n",
      "486\n",
      "488\n",
      "490\n",
      "492\n",
      "494\n",
      "496\n",
      "498\n",
      "500\n",
      "502\n",
      "504\n",
      "506\n",
      "508\n",
      "510\n",
      "512\n",
      "514\n",
      "516\n",
      "518\n",
      "520\n",
      "522\n",
      "524\n",
      "526\n",
      "528\n",
      "530\n",
      "532\n",
      "534\n",
      "536\n",
      "538\n",
      "540\n",
      "542\n",
      "544\n",
      "546\n",
      "548\n",
      "550\n",
      "552\n",
      "554\n",
      "556\n",
      "558\n",
      "560\n",
      "562\n",
      "564\n",
      "566\n",
      "568\n",
      "570\n",
      "572\n",
      "574\n",
      "576\n",
      "578\n",
      "580\n",
      "582\n",
      "584\n",
      "586\n",
      "588\n",
      "590\n",
      "592\n",
      "594\n",
      "596\n",
      "598\n",
      "600\n",
      "602\n",
      "604\n",
      "606\n",
      "608\n",
      "610\n",
      "612\n",
      "614\n",
      "616\n",
      "618\n",
      "620\n",
      "622\n",
      "624\n",
      "626\n",
      "628\n",
      "630\n",
      "632\n",
      "634\n",
      "636\n",
      "638\n",
      "640\n",
      "642\n",
      "644\n",
      "646\n",
      "648\n",
      "650\n",
      "652\n",
      "654\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 2; 47.46 GiB total capacity; 42.50 GiB already allocated; 294.31 MiB free; 45.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_nucleus_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m nuc_cands\n\u001b[1;32m      3\u001b[0m get_nucleus_candidates(\u001b[38;5;28mlist\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m800\u001b[39m]), \u001b[38;5;28mlist\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m800\u001b[39m]), \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_nucleus_candidates\u001b[0;34m(inputs, refs, per_example, batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m ref \u001b[38;5;241m=\u001b[39m refs[i\u001b[38;5;241m*\u001b[39mbatch:batch\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     67\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(ins, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 68\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde_DE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     81\u001b[0m allsco \u001b[38;5;241m=\u001b[39m generated_tokens\u001b[38;5;241m.\u001b[39msequences_scores\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/generation_utils.py:1092\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m# interleave with `num_beams * num_return_sequences`\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1086\u001b[0m         input_ids,\n\u001b[1;32m   1087\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_beams \u001b[38;5;241m*\u001b[39m num_return_sequences,\n\u001b[1;32m   1088\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1090\u001b[0m     )\n\u001b[0;32m-> 1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_group_beam_gen_mode:\n\u001b[1;32m   1107\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/generation_utils.py:2106\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2104\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[1;32m   2105\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m-> 2106\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_warper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/generation_logits_process.py:93\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/generation_logits_process.py:196\u001b[0m, in \u001b[0;36mTopPLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 196\u001b[0m     sorted_logits, sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     cumulative_probs \u001b[38;5;241m=\u001b[39m sorted_logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 2; 47.46 GiB total capacity; 42.50 GiB already allocated; 294.31 MiB free; 45.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "get_nucleus_candidates(list(data['en'][:800]), list(data['de'][:800]), 40, 2)\n",
    "del nuc_cands\n",
    "get_nucleus_candidates(list(data['en'][:800]), list(data['de'][:800]), 50, 1)\n",
    "del nuc_cands\n",
    "get_bs_candidates(list(data['en'][:800]), list(data['de'][:800]), 10, 2)\n",
    "del bs_cands\n",
    "\n",
    "\n",
    "#print(generated_tokens)\n",
    "#print(len(generated_tokens.sequences_scores))\n",
    "#tokenizer.batch_decode(generated_tokens.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5189f7ca-ca8b-4130-9e30-5058826d4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46b6a20-aeb3-4a3b-9d66-cb0da9371757",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3305e7ef-9f0b-4d4c-8303-1680207f46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"myfile.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa052146-1f86-4ea8-a6f9-fd0f6bce6a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testd = {'yo':\"what is good buddy\"}\n",
    "\n",
    "file1.write(json.dumps(testd))\n",
    "file1.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffcfd65-ecdd-4816-8e9a-85a50a206d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1501f-4237-44ed-8c82-61a0f19634c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
