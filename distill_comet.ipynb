{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbdfd6d-d718-4eec-88b0-77789bf845d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 10:58:50.034370: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-03 10:58:50.034393: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "from rerank_score_cands_new import load_cands\n",
    "import numpy as np\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "#from distill_comet import XLMCometRegressor\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "xlm_tok = AutoTokenizer.from_pretrained('xlm-roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd4f3f2-85c7-452f-84f9-74205cb8627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMCometRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_rate=0.1):\n",
    "        # TODO should we be freezing layers?\n",
    "        super().__init__()\n",
    "        \n",
    "        self.xlmroberta = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "        # Num labels 1 should just indicate regression (?)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.xlmroberta.config.hidden_size, 1), \n",
    "        )\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        # don't finetune xlmroberta model\n",
    "        #with torch.no_grad():\n",
    "        word_rep, sentence_rep = self.xlmroberta(input_ids, attention_mask=attention_masks, encoder_attention_mask=attention_masks, return_dict=False)\n",
    "        # use the first <s> token as a CLS token, TODO experiment with using the sum of \n",
    "        # ensure padding not factored in\n",
    "        word_rep = word_rep*(input_ids>0).unsqueeze(-1)\n",
    "        outputs = self.regressor(torch.sum(word_rep, 1))\n",
    "        #print(\"Shape: \", outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959f8b23-872a-4e36-81f5-5ade199f12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing is everything\n",
    "# 1 is with distilled data lattice + beam\n",
    "# 2 is with just beam data? \n",
    "# ordered is the clean ranking ready set\n",
    "def load_cometqe_data(ind):\n",
    "    with open('processeddata/orderedmasks.pkl', 'rb') as f:\n",
    "        masks = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/orderedinps.pkl', 'rb') as f:\n",
    "        xinps = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/orderedlabels.pkl', 'rb') as f:\n",
    "        yinps = pickle.load(f)\n",
    "    return masks, xinps, yinps\n",
    "\n",
    "mdata, xdata, ydata = load_cometqe_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f07e35-1994-485e-ba44-c6fef5c57d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "cut = int(len(xdata)*.9)- int(len(xdata)*.9)%32\n",
    "xtrain, ytrain, mtrain = xdata[:cut], ydata[:cut], mdata[:cut]\n",
    "xtest, ytest, mtest = xdata[cut:], ydata[cut:], mdata[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a419403-ff57-4583-9f69-ee14771f2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mdata, xdata, ydata\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4729652-ffdb-4ce6-803c-15e17833f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torch.tensor(ytrain[:31]+[.8]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987705a3-5c97-47a4-903c-e3d0583f5a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5608487725257874,\n",
       " 0.6321543455123901,\n",
       " 0.6690516471862793,\n",
       " 0.66994708776474,\n",
       " 0.7340843081474304,\n",
       " 0.7379187941551208,\n",
       " 0.7536868453025818,\n",
       " 0.759560227394104,\n",
       " 0.7654272317886353,\n",
       " 0.7689279913902283,\n",
       " 0.7720431685447693,\n",
       " 0.7778377532958984,\n",
       " 0.7796866893768309,\n",
       " 0.7859623432159424,\n",
       " 0.7919414043426514,\n",
       " 0.7928602695465088,\n",
       " 0.7943693399429321,\n",
       " 0.8000797033309937,\n",
       " 0.8007832169532776,\n",
       " 0.8013631701469421,\n",
       " 0.8021320104598999,\n",
       " 0.8032323718070984,\n",
       " 0.8044326901435852,\n",
       " 0.8044593930244446,\n",
       " 0.8061469197273254,\n",
       " 0.8118272423744202,\n",
       " 0.8131868839263916,\n",
       " 0.8135043978691101,\n",
       " 0.817229688167572,\n",
       " 0.8339771032333374,\n",
       " 0.8789095878601074,\n",
       " 0.887186586856842]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24beb2d9-8520-4520-b8a2-e6d06aa30014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, masks):\n",
    "        assert len(sentences) == len(labels)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.labels[i], self.masks[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "def collate_custom(datafull):\n",
    "    #print(len(datafull[0]))\n",
    "    data = [torch.tensor(d[0]) for d in datafull]\n",
    "    masdata=  [d[2] for d in datafull]\n",
    "    labels = [d[1] for d in datafull]\n",
    "    max_len = max([x.squeeze().numel() for x in data])\n",
    "    data = [torch.nn.functional.pad(x, pad=(0, max_len - x.numel()), mode='constant', value=0) for x in data]\n",
    "    data = torch.stack(data).to(device)\n",
    "    # TODO just a normal mask for now\n",
    "    #masdata = [torch.ones_like(m) for m in masdata]\n",
    "    masdata = [torch.nn.functional.pad(x, pad=(0, max_len - x[0].numel(), 0, max_len - x[0].numel()), mode='constant', value=0) for x in masdata]\n",
    "    masdata = torch.stack(masdata).to(device)\n",
    "    return data, torch.tensor(labels).to(device), masdata\n",
    "\n",
    "testloader = DataLoader(RegressionDataset(xtest, ytest, mtest), batch_size=32, shuffle=False, collate_fn=collate_custom)\n",
    "trainloader = DataLoader(RegressionDataset(xtrain, ytrain, mtrain), batch_size=32, shuffle=False, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d91005-836e-415c-b943-a3b425e6b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinmax = 100000\n",
    "xtiny, ytiny, mtiny = xtrain[:tinmax], ytrain[:tinmax], mtrain[:tinmax]\n",
    "tinyloader = DataLoader(RegressionDataset(xtiny, ytiny, mtiny), batch_size=32, shuffle=True, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee20ab-5811-4500-abad-aa9eb20983ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe13842-e550-4128-840f-09615ce82dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a03299-cb46-4488-bbc4-2baafc36fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "def train(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2):\n",
    "    print(\"Total steps :\", epochs*len(train_dataloader))\n",
    "    best_loss = 1e10\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%1==0:\n",
    "            print(\"EPOCH \", epoch)\n",
    "            print(\"-----\")\n",
    "            print(best_loss)\n",
    "        model.train()\n",
    "        cbest = 1e10\n",
    "        lostot = 0\n",
    "        loscnt = 0\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            batch_inputs, batch_labels, batch_masks = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "            loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "            lostot+=loss\n",
    "            loscnt+=1\n",
    "            if step%500==0:\n",
    "                #print(loss)  \n",
    "                if loscnt>0:\n",
    "                    print(lostot/loscnt)\n",
    "                    cbest = min(float(lostot/loscnt), cbest)\n",
    "                    best_loss = min(best_loss, cbest)\n",
    "                    print(\"cbest, \", cbest)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        cbest = min(float(lostot/loscnt), cbest)\n",
    "        best_loss = min(best_loss, cbest)\n",
    "        print(\"cbest, \", cbest)\n",
    "        if cbest==best_loss:\n",
    "            torch.save(model.state_dict(), \"torchsaved/maskedminicomestim\"+str(epoch)+\".pt\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loss_function, tdataloader, device):\n",
    "    model.eval()\n",
    "    test_loss, test_r2 = [], []\n",
    "    preds = []\n",
    "    ind = 0\n",
    "    for batch in tdataloader:\n",
    "        batch_inputs, batch_labels,batch_masks = \\\n",
    "                                 tuple(b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "        preds.append(list(outputs.squeeze()))\n",
    "        test_loss.append(loss.item())\n",
    "        #r2 = r2_score(outputs, batch_labels)\n",
    "        #test_r2.append(r2.item())\n",
    "        if ind==10:\n",
    "            print(batch_labels)\n",
    "            print(outputs)\n",
    "        ind+=1\n",
    "    return test_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7fb56b-4ee2-44e2-86c2-b64aca12aeac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13f0fd3-f0e5-4494-8ac0-a017ccb71618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLMCometRegressor(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/maskedcont3.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "218d3e10-6a44-4909-97c1-ddbf3ad796f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskedcont1 has first round of optimizing loss\n",
    "# maskedcont2 more contrastive learning\n",
    "# maskedcont3 full on contrastive \n",
    "torch.save(model.state_dict(), \"torchsaved/maskedcont4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784319d1-d9ac-4524-9fc5-748f2d119255",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1918c74d-5efb-4a45-ba35-0fb4bb665fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmask = (torch.triu(torch.ones(32, 32))*2-torch.ones(32, 32))*-1\n",
    "vmask = vmask.to(device)\n",
    "mse = nn.MSELoss()\n",
    "def rank_loss(preds, golds):\n",
    "    totloss = 0\n",
    "    for i in range(1, len(preds)):\n",
    "        # for margin\n",
    "        margin = (golds - torch.roll(golds, i))*vmask[i]\n",
    "        diff = (preds - torch.roll(preds, i)-margin)*vmask[i]\n",
    "        diff[diff<0] = 0\n",
    "        totloss+=torch.sum(diff)\n",
    "    return totloss + mse(preds, golds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09d02e9c-2aed-4ab3-949b-45e56b64c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train_params(learn_r, epochs, loader, mod):\n",
    "    optimizer = AdamW(mod.parameters(),\n",
    "                      lr=learn_r,\n",
    "                      eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
    "                     num_warmup_steps=0, num_training_steps=epochs*len(loader))\n",
    "    model = train(mod, optimizer, scheduler, rank_loss, epochs, \n",
    "                  loader, device, clip_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c16f43a4-da47-478d-9872-f721771e4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps : 98440\n",
      "EPOCH  0\n",
      "-----\n",
      "10000000000.0\n",
      "tensor(0.6183, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6183037757873535\n",
      "tensor(1.1864, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6183037757873535\n",
      "tensor(1.2136, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6183037757873535\n",
      "tensor(1.1971, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6183037757873535\n",
      "tensor(1.2129, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6183037757873535\n",
      "cbest,  0.6183037757873535\n",
      "EPOCH  1\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.7793, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.77928227186203\n",
      "tensor(1.0859, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.77928227186203\n",
      "tensor(1.0957, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.77928227186203\n",
      "tensor(1.1028, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.77928227186203\n",
      "tensor(1.1142, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.77928227186203\n",
      "cbest,  0.77928227186203\n",
      "EPOCH  2\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.9297, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9297081232070923\n",
      "tensor(1.0247, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9297081232070923\n",
      "tensor(1.0703, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9297081232070923\n",
      "tensor(1.0751, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9297081232070923\n",
      "tensor(1.0830, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9297081232070923\n",
      "cbest,  0.9297081232070923\n",
      "EPOCH  3\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.6401, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6400976181030273\n",
      "tensor(0.9802, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6400976181030273\n",
      "tensor(1.0175, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6400976181030273\n",
      "tensor(1.0108, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6400976181030273\n",
      "tensor(1.0244, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6400976181030273\n",
      "cbest,  0.6400976181030273\n",
      "EPOCH  4\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.7944, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7943922877311707\n",
      "tensor(0.9588, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7943922877311707\n",
      "tensor(0.9859, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7943922877311707\n",
      "tensor(0.9879, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7943922877311707\n",
      "tensor(1.0006, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7943922877311707\n",
      "cbest,  0.7943922877311707\n",
      "EPOCH  5\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.8420, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8420398235321045\n",
      "tensor(0.9643, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8420398235321045\n",
      "tensor(1.0081, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8420398235321045\n",
      "tensor(1.0000, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8420398235321045\n",
      "tensor(1.0163, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8420398235321045\n",
      "cbest,  0.8420398235321045\n",
      "EPOCH  6\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.9586, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9586475491523743\n",
      "tensor(0.9643, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9586475491523743\n",
      "tensor(0.9822, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9586475491523743\n",
      "tensor(0.9818, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9586475491523743\n",
      "tensor(0.9902, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.9586475491523743\n",
      "cbest,  0.9586475491523743\n",
      "EPOCH  7\n",
      "-----\n",
      "0.6183037757873535\n",
      "tensor(0.4909, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.49085161089897156\n",
      "tensor(0.9227, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.49085161089897156\n",
      "tensor(0.9584, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.49085161089897156\n",
      "tensor(0.9550, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.49085161089897156\n",
      "tensor(0.9687, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.49085161089897156\n",
      "cbest,  0.49085161089897156\n",
      "EPOCH  8\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.5680, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.568020761013031\n",
      "tensor(0.9221, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.568020761013031\n",
      "tensor(0.9523, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.568020761013031\n",
      "tensor(0.9481, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.568020761013031\n",
      "tensor(0.9714, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.568020761013031\n",
      "cbest,  0.568020761013031\n",
      "EPOCH  9\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.5291, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5291076898574829\n",
      "tensor(0.9179, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5291076898574829\n",
      "tensor(0.9535, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5291076898574829\n",
      "tensor(0.9420, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5291076898574829\n",
      "tensor(0.9502, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5291076898574829\n",
      "cbest,  0.5291076898574829\n",
      "EPOCH  10\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.7855, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7854560017585754\n",
      "tensor(0.9107, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7854560017585754\n",
      "tensor(0.9609, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7854560017585754\n",
      "tensor(0.9492, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7854560017585754\n",
      "tensor(0.9625, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7854560017585754\n",
      "cbest,  0.7854560017585754\n",
      "EPOCH  11\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.7932, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7931955456733704\n",
      "tensor(0.9055, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7931955456733704\n",
      "tensor(0.9227, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7931955456733704\n",
      "tensor(0.9196, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7931955456733704\n",
      "tensor(0.9336, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7931955456733704\n",
      "cbest,  0.7931955456733704\n",
      "EPOCH  12\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.7742, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7742152214050293\n",
      "tensor(0.8834, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7742152214050293\n",
      "tensor(0.9282, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7742152214050293\n",
      "tensor(0.9199, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7742152214050293\n",
      "tensor(0.9312, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7742152214050293\n",
      "cbest,  0.7742152214050293\n",
      "EPOCH  13\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.5267, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5267380475997925\n",
      "tensor(0.9004, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5267380475997925\n",
      "tensor(0.9267, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5267380475997925\n",
      "tensor(0.9164, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5267380475997925\n",
      "tensor(0.9347, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5267380475997925\n",
      "cbest,  0.5267380475997925\n",
      "EPOCH  14\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.8189, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8188884258270264\n",
      "tensor(0.8932, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8188884258270264\n",
      "tensor(0.9095, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8188884258270264\n",
      "tensor(0.9104, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8188884258270264\n",
      "tensor(0.9176, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8188884258270264\n",
      "cbest,  0.8188884258270264\n",
      "EPOCH  15\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.8282, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8282430768013\n",
      "tensor(0.8927, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8282430768013\n",
      "tensor(0.9159, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8282430768013\n",
      "tensor(0.8981, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8282430768013\n",
      "tensor(0.9176, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.8282430768013\n",
      "cbest,  0.8282430768013\n",
      "EPOCH  16\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.6197, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6196838617324829\n",
      "tensor(0.8705, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6196838617324829\n",
      "tensor(0.9158, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6196838617324829\n",
      "tensor(0.9042, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6196838617324829\n",
      "tensor(0.9152, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6196838617324829\n",
      "cbest,  0.6196838617324829\n",
      "EPOCH  17\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.5557, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5556727051734924\n",
      "tensor(0.8724, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5556727051734924\n",
      "tensor(0.9070, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5556727051734924\n",
      "tensor(0.8953, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5556727051734924\n",
      "tensor(0.8955, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5556727051734924\n",
      "cbest,  0.5556727051734924\n",
      "EPOCH  18\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.5943, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5942503213882446\n",
      "tensor(0.8440, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5942503213882446\n",
      "tensor(0.8818, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5942503213882446\n",
      "tensor(0.8862, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5942503213882446\n",
      "tensor(0.8960, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5942503213882446\n",
      "cbest,  0.5942503213882446\n",
      "EPOCH  19\n",
      "-----\n",
      "0.49085161089897156\n",
      "tensor(0.3232, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.3231607675552368\n",
      "tensor(0.8619, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.3231607675552368\n",
      "tensor(0.8849, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.3231607675552368\n",
      "tensor(0.8819, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.3231607675552368\n",
      "tensor(0.9026, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.3231607675552368\n",
      "cbest,  0.3231607675552368\n",
      "EPOCH  20\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.5632, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5631653666496277\n",
      "tensor(0.8741, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5631653666496277\n",
      "tensor(0.8826, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5631653666496277\n",
      "tensor(0.8825, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5631653666496277\n",
      "tensor(0.8969, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5631653666496277\n",
      "cbest,  0.5631653666496277\n",
      "EPOCH  21\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.6577, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6576967835426331\n",
      "tensor(0.8423, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6576967835426331\n",
      "tensor(0.8860, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6576967835426331\n",
      "tensor(0.8810, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6576967835426331\n",
      "tensor(0.8871, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6576967835426331\n",
      "cbest,  0.6576967835426331\n",
      "EPOCH  22\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.7356, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7355753779411316\n",
      "tensor(0.8577, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7355753779411316\n",
      "tensor(0.8766, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7355753779411316\n",
      "tensor(0.8848, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7355753779411316\n",
      "tensor(0.8966, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7355753779411316\n",
      "cbest,  0.7355753779411316\n",
      "EPOCH  23\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.6940, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6939952373504639\n",
      "tensor(0.8475, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6939952373504639\n",
      "tensor(0.8839, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6939952373504639\n",
      "tensor(0.8753, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6939952373504639\n",
      "tensor(0.8838, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6939952373504639\n",
      "cbest,  0.6939952373504639\n",
      "EPOCH  24\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.7474, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7473641037940979\n",
      "tensor(0.8339, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7473641037940979\n",
      "tensor(0.8708, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7473641037940979\n",
      "tensor(0.8611, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7473641037940979\n",
      "tensor(0.8812, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7473641037940979\n",
      "cbest,  0.7473641037940979\n",
      "EPOCH  25\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.7649, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7649403810501099\n",
      "tensor(0.8637, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7649403810501099\n",
      "tensor(0.8788, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7649403810501099\n",
      "tensor(0.8742, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7649403810501099\n",
      "tensor(0.8846, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7649403810501099\n",
      "cbest,  0.7649403810501099\n",
      "EPOCH  26\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.5151, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5150803327560425\n",
      "tensor(0.8619, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5150803327560425\n",
      "tensor(0.8904, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5150803327560425\n",
      "tensor(0.8778, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5150803327560425\n",
      "tensor(0.8809, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5150803327560425\n",
      "cbest,  0.5150803327560425\n",
      "EPOCH  27\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.5140, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5140461325645447\n",
      "tensor(0.8644, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5140461325645447\n",
      "tensor(0.8837, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5140461325645447\n",
      "tensor(0.8731, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5140461325645447\n",
      "tensor(0.8826, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5140461325645447\n",
      "cbest,  0.5140461325645447\n",
      "EPOCH  28\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.5659, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5659205913543701\n",
      "tensor(0.8427, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5659205913543701\n",
      "tensor(0.8660, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5659205913543701\n",
      "tensor(0.8644, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5659205913543701\n",
      "tensor(0.8763, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5659205913543701\n",
      "cbest,  0.5659205913543701\n",
      "EPOCH  29\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.6382, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6381862759590149\n",
      "tensor(0.8503, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6381862759590149\n",
      "tensor(0.8663, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6381862759590149\n",
      "tensor(0.8635, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6381862759590149\n",
      "tensor(0.8666, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6381862759590149\n",
      "cbest,  0.6381862759590149\n",
      "EPOCH  30\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.7526, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.752559244632721\n",
      "tensor(0.8477, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.752559244632721\n",
      "tensor(0.8745, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.752559244632721\n",
      "tensor(0.8676, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.752559244632721\n",
      "tensor(0.8858, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.752559244632721\n",
      "cbest,  0.752559244632721\n",
      "EPOCH  31\n",
      "-----\n",
      "0.3231607675552368\n",
      "tensor(0.3033, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.30332455039024353\n",
      "tensor(0.8332, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.30332455039024353\n",
      "tensor(0.8622, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.30332455039024353\n",
      "tensor(0.8651, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.30332455039024353\n",
      "tensor(0.8750, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.30332455039024353\n",
      "cbest,  0.30332455039024353\n",
      "EPOCH  32\n",
      "-----\n",
      "0.30332455039024353\n",
      "tensor(0.4425, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.4425422251224518\n",
      "tensor(0.8147, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.4425422251224518\n",
      "tensor(0.8390, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.4425422251224518\n",
      "tensor(0.8439, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.4425422251224518\n",
      "tensor(0.8513, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.4425422251224518\n",
      "cbest,  0.4425422251224518\n",
      "EPOCH  33\n",
      "-----\n",
      "0.30332455039024353\n",
      "tensor(0.5633, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.563252866268158\n",
      "tensor(0.8586, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.563252866268158\n",
      "tensor(0.8737, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.563252866268158\n",
      "tensor(0.8632, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.563252866268158\n",
      "tensor(0.8740, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.563252866268158\n",
      "cbest,  0.563252866268158\n",
      "EPOCH  34\n",
      "-----\n",
      "0.30332455039024353\n",
      "tensor(0.2901, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.29009538888931274\n",
      "tensor(0.8302, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.29009538888931274\n",
      "tensor(0.8642, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.29009538888931274\n",
      "tensor(0.8525, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.29009538888931274\n",
      "tensor(0.8562, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.29009538888931274\n",
      "cbest,  0.29009538888931274\n",
      "EPOCH  35\n",
      "-----\n",
      "0.29009538888931274\n",
      "tensor(0.4660, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.46602940559387207\n",
      "tensor(0.8336, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.46602940559387207\n",
      "tensor(0.8612, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.46602940559387207\n",
      "tensor(0.8583, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.46602940559387207\n",
      "tensor(0.8681, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.46602940559387207\n",
      "cbest,  0.46602940559387207\n",
      "EPOCH  36\n",
      "-----\n",
      "0.29009538888931274\n",
      "tensor(0.6269, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6269484162330627\n",
      "tensor(0.8271, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6269484162330627\n",
      "tensor(0.8449, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6269484162330627\n",
      "tensor(0.8530, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6269484162330627\n",
      "tensor(0.8609, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6269484162330627\n",
      "cbest,  0.6269484162330627\n",
      "EPOCH  37\n",
      "-----\n",
      "0.29009538888931274\n",
      "tensor(0.7812, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7811852693557739\n",
      "tensor(0.8209, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7811852693557739\n",
      "tensor(0.8473, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7811852693557739\n",
      "tensor(0.8476, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7811852693557739\n",
      "tensor(0.8589, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.7811852693557739\n",
      "cbest,  0.7811852693557739\n",
      "EPOCH  38\n",
      "-----\n",
      "0.29009538888931274\n",
      "tensor(0.5248, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5248134732246399\n",
      "tensor(0.8447, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5248134732246399\n",
      "tensor(0.8589, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5248134732246399\n",
      "tensor(0.8612, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5248134732246399\n",
      "tensor(0.8703, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.5248134732246399\n",
      "cbest,  0.5248134732246399\n",
      "EPOCH  39\n",
      "-----\n",
      "0.29009538888931274\n",
      "tensor(0.6183, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6182942986488342\n",
      "tensor(0.8183, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6182942986488342\n",
      "tensor(0.8582, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6182942986488342\n",
      "tensor(0.8511, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6182942986488342\n",
      "tensor(0.8616, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "cbest,  0.6182942986488342\n",
      "cbest,  0.6182942986488342\n"
     ]
    }
   ],
   "source": [
    "run_model_train_params(1e-6, 40, trainloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "404c6094-5aab-42b5-9eb5-2f5a3ec50d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trainloader:\n",
    "    #print(t[1])\n",
    "    assert t[1][-1] == max(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2cf629fb-072b-4326-8af1-ccec6f11bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model \n",
    "model = XLMCometRegressor(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/maskedcont2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c96ce7-5520-4b13-92bf-58a2de69a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evaluate(model, rank_loss, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35907783-1f6f-4801-8ae5-d8e05bb80ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dede517-a94b-4e4e-a037-dac55a310ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abafa5-bf6a-45b4-b5c2-37d76474cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sloss = [math.sqrt(l) for l in loss[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4792d4-8a3b-4413-aef2-8c6ce3fa83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nval = [float(l) for l in loss[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68853bf4-ea68-4eb3-ab3e-6a6c6608c3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.48905109489051"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mind = []\n",
    "for l in loss[1]:\n",
    "    mind+=[l.index(max(l))]\n",
    "\n",
    "sum(mind)/len(mind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0028c47e-4fc8-4089-bf00-693ed030a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e6578-01b4-4ab4-87f0-69b68885ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(alldf['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1d335-f94b-4fd5-bd69-7f86d9e7269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old data loading stuff\n",
    "\n",
    "# data comes from WMT 2019 [I think], TODO validate exact year being used\n",
    "# data generated with gold reference given as hyp\n",
    "#golddf = pd.read_csv('./processeddata/golddata.csv')\n",
    "#golddf['inp'] = golddf['fr']\n",
    "#golddf['hyp'] = golddf['en']\n",
    "# data generated on candidates from beam search 50 and lattice (lots of bad)\n",
    "distill_df = pd.read_csv('distill_cometdata_1.csv')\n",
    "# data generated between random / unrelated sentences \n",
    "#rand_df = pd.read_csv('distill_cometdata_rand.csv')[:30000]\n",
    "# combine\n",
    "alldf = distill_df#.append(rand_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10bb4a-76b0-4ffd-af5d-7ad859d86a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted3 = alldf.sort_values(['ref', 'scores']).reset_index().drop(columns=['index', 'Unnamed: 0'])\n",
    "sorted3 = sorted3[sorted3[\"inp\"].str.contains(\"&#\")==False]\n",
    "sorted3 = sorted3[sorted3['inp'].str.len()>40]\n",
    "sorted3 = sorted3[sorted3['ref'].str.len()>40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f4f26-6305-4f95-a617-513f80cc2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dataframe, get sets of size BATCH that are ranked by score\n",
    "dlists = []\n",
    "prevs = sorted3['inp'].iloc[0]\n",
    "sofar = []\n",
    "for index, row in sorted3.iterrows():\n",
    "    if row['inp']==prevs:\n",
    "        sofar.append(row)\n",
    "    else:\n",
    "        dlists.append(sofar)\n",
    "        sofar = []\n",
    "        prevs = row['inp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca10ad6-ac72-4443-b2a3-d7e41d44fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0990b05-3a04-49d7-bd0f-9a771a947a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(dlist, best, batch):\n",
    "    nbatches = int(len(dlist)/batch)+1\n",
    "    res = []\n",
    "    for i in range(nbatches):\n",
    "        # get a random sorted sample, make sure best is in each batch\n",
    "        indices = random.sample(range(len(dlist)), batch-1)\n",
    "        res.append([dlist[i] for i in sorted(indices)]+[best])\n",
    "    return res\n",
    "\n",
    "def all_samples(dls, batch):\n",
    "    done = []\n",
    "    for d in dls:\n",
    "        done.extend(get_samples(d[:-1], d[-1], batch))\n",
    "    return done\n",
    "\n",
    "asamps = all_samples(dlists, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10d548-b10a-4dee-81f1-9a1e566260f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(asamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98760b9f-95a4-4523-a063-83e96237a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "for a in asamps:\n",
    "    done.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a161adb-f536-4d9d-a683-3dc8b8efca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = pd.DataFrame(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6162d52-44f2-4f79-9fe6-22e0c7b4338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = done.reset_index().drop(columns= ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb64f6-8c08-446d-b0cb-140a0f05634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "done.iloc[31+32*9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f350f2-6407-464a-ad05-4d71433f4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inputs from dataframe\n",
    "def get_inputs(inpdf):\n",
    "    xinp = []\n",
    "    yinp = []\n",
    "    maskinp = []\n",
    "    \n",
    "    for index, row in inpdf.iterrows():\n",
    "        if index%1000==0:\n",
    "            print(index)\n",
    "        #print(row['c1'], row['c2'])\n",
    "        # will need to make a custom mask (maybe) so that inputs from both languages are encoded separately\n",
    "        toktmp = xlm_tok(row['inp']).input_ids\n",
    "        lent = len(toktmp)\n",
    "        hyptmp = xlm_tok(row['hyp']).input_ids\n",
    "        toktmp.extend(hyptmp)\n",
    "        mask = torch.ones(len(toktmp), len(toktmp))\n",
    "        # should set upper left and bottom right quadrants to 1, mask other stuff\n",
    "        # TODO make different types of masks. \n",
    "        #mask[:lent, :lent] = 1\n",
    "        #mask[lent:, lent:] = 1\n",
    "        # make causal encoder-decoder mask\n",
    "        mask[:lent, lent:] = 0\n",
    "        mask[lent:, lent:] = torch.tril(mask[lent:, lent:])\n",
    "        xinp.append(toktmp)\n",
    "        maskinp.append(mask)\n",
    "        yinp.append(row['scores'])\n",
    "    return xinp, yinp, maskinp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa1228-da1f-4d64-8058-1c9c27abcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata, ydata, mdata = get_inputs(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ab69-47f1-4840-a4f9-758f167bcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(xdata)):\n",
    "    if len(xdata[i])>500:\n",
    "        del xdata[i]\n",
    "        del ydata[i]\n",
    "        del mdata[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840b719-e6fd-4818-afa6-10b3dbd9c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all as precaution\n",
    "def save_cometqe_data(md, xd, yd):\n",
    "    # save data into a pickle file\n",
    "    with open('processeddata/orderedmasks.pkl', 'wb') as f:\n",
    "        pickle.dump(md, f)\n",
    "\n",
    "    with open('processeddata/orderedinps.pkl', 'wb') as f:\n",
    "        pickle.dump(xd, f)\n",
    "\n",
    "    with open('processeddata/orderedlabels.pkl', 'wb') as f:\n",
    "        pickle.dump(yd, f)\n",
    "        \n",
    "#mdata, xdata, ydata = load_cometqe_data()\n",
    "        \n",
    "save_cometqe_data(mdata, xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd62a3-6a2d-463d-b89e-3379adcfe921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
