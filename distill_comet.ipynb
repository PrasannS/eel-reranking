{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbdfd6d-d718-4eec-88b0-77789bf845d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "from rerank_score_cands_new import load_cands\n",
    "import numpy as np\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from distill_comet import XLMCometRegressor\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "xlm_tok = AutoTokenizer.from_pretrained('xlm-roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959f8b23-872a-4e36-81f5-5ade199f12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing is everything\n",
    "# 1 is with distilled data lattice + beam\n",
    "# 2 is with just beam data? \n",
    "def load_cometqe_data(ind):\n",
    "    with open('processeddata/commasks'+str(ind)+'.pkl', 'rb') as f:\n",
    "        masks = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/cominps'+str(ind)+'.pkl', 'rb') as f:\n",
    "        xinps = pickle.load(f)\n",
    "\n",
    "    with open('processeddata/comlabels'+str(ind)+'.pkl', 'rb') as f:\n",
    "        yinps = pickle.load(f)\n",
    "    return masks, xinps, yinps\n",
    "\n",
    "mdata, xdata, ydata = load_cometqe_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f07e35-1994-485e-ba44-c6fef5c57d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "cut = int(len(xdata)*.9)\n",
    "xtrain, ytrain, mtrain = xdata[:cut], ydata[:cut], mdata[:cut]\n",
    "xtest, ytest, mtest = xdata[cut:], ydata[cut:], mdata[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a419403-ff57-4583-9f69-ee14771f2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mdata, xdata, ydata\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24beb2d9-8520-4520-b8a2-e6d06aa30014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, masks):\n",
    "        assert len(sentences) == len(labels)\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.labels[i], self.masks[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "def collate_custom(datafull):\n",
    "    #print(len(datafull[0]))\n",
    "    data = [torch.tensor(d[0]) for d in datafull]\n",
    "    masdata=  [d[2] for d in datafull]\n",
    "    labels = [d[1] for d in datafull]\n",
    "    max_len = max([x.squeeze().numel() for x in data])\n",
    "    data = [torch.nn.functional.pad(x, pad=(0, max_len - x.numel()), mode='constant', value=0) for x in data]\n",
    "    data = torch.stack(data).to(device)\n",
    "    # TODO just a normal mask for now\n",
    "    masdata = [torch.ones_like(m) for m in masdata]\n",
    "    masdata = [torch.nn.functional.pad(x, pad=(0, max_len - x[0].numel(), 0, max_len - x[0].numel()), mode='constant', value=0) for x in masdata]\n",
    "    masdata = torch.stack(masdata).to(device)\n",
    "    return data, torch.tensor(labels).to(device), masdata\n",
    "\n",
    "testloader = DataLoader(RegressionDataset(xtest, ytest, mtest), batch_size=32, shuffle=True, collate_fn=collate_custom)\n",
    "trainloader = DataLoader(RegressionDataset(xtrain, ytrain, mtrain), batch_size=32, shuffle=True, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d91005-836e-415c-b943-a3b425e6b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinmax = 100\n",
    "xtiny, ytiny, mtiny = xtrain[:tinmax], ytrain[:tinmax], mtrain[:tinmax]\n",
    "tinyloader = DataLoader(RegressionDataset(xtiny, ytiny, mtiny), batch_size=32, shuffle=True, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cfe13842-e550-4128-840f-09615ce82dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)\n",
    "epochs = 100\n",
    "total_steps = len(trainloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
    "                 num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_function = weighted_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b1a03299-cb46-4488-bbc4-2baafc36fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "def train(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2):\n",
    "    print(\"Total steps :\", total_steps)\n",
    "    best_loss = 1e10\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%1==0:\n",
    "            print(\"EPOCH \", epoch)\n",
    "            print(\"-----\")\n",
    "            print(best_loss)\n",
    "        model.train()\n",
    "        cbest = 1e10\n",
    "        lostot = 0\n",
    "        loscnt = 0\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            batch_inputs, batch_labels, batch_masks = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "            #print(outputs.squeeze().shape)\n",
    "            #print(batch_labels.squeeze().shape)\n",
    "            loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "            lostot+=loss\n",
    "            loscnt+=1\n",
    "            if step%500==0:\n",
    "                #print(loss)  \n",
    "                if loscnt>0:\n",
    "                    print(lostot/loscnt)\n",
    "                    cbest = min(float(lostot/loscnt), cbest)\n",
    "                    best_loss = min(best_loss, cbest)\n",
    "                    print(\"cbest, \", cbest)\n",
    "                    #if cbest==best_loss:\n",
    "                    #    torch.save(model.state_dict(),\"torchsaved/modcomestim\"+str(epoch)+\".pt\")\n",
    "            #best_loss = min(best_loss, float(loss))\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        cbest = min(float(lostot/loscnt), cbest)\n",
    "        best_loss = min(best_loss, cbest)\n",
    "        print(\"cbest, \", cbest)\n",
    "        if cbest==best_loss:\n",
    "            torch.save(model.state_dict(), \"torchsaved/maskedminicomestim\"+str(epoch)+\".pt\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loss_function, tdataloader, device):\n",
    "    model.eval()\n",
    "    test_loss, test_r2 = [], []\n",
    "    preds = []\n",
    "    ind = 0\n",
    "    for batch in tdataloader:\n",
    "        batch_inputs, batch_labels,batch_masks = \\\n",
    "                                 tuple(b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "        preds.extend(list(outputs.squeeze()))\n",
    "        test_loss.append(loss.item())\n",
    "        #r2 = r2_score(outputs, batch_labels)\n",
    "        #test_r2.append(r2.item())\n",
    "        if ind==10:\n",
    "            print(batch_labels)\n",
    "            print(outputs)\n",
    "        ind+=1\n",
    "    return test_loss, preds\n",
    "\n",
    "def r2_score(outputs, labels):\n",
    "    labels_mean = torch.mean(labels)\n",
    "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
    "    ss_res = torch.sum((labels - outputs) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cde44-0940-45fc-a39f-ecb9e761610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_train(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2):\n",
    "    print(\"Total steps :\", total_steps)\n",
    "    best_loss = 1e10\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%1==0:\n",
    "            print(\"EPOCH \", epoch)\n",
    "            print(\"-----\")\n",
    "            print(best_loss)\n",
    "        model.train()\n",
    "        cbest = 1e10\n",
    "        lostot = 0\n",
    "        loscnt = 0\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            batch_inputs, batch_labels, batch_masks = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "            #print(outputs.squeeze().shape)\n",
    "            #print(batch_labels.squeeze().shape)\n",
    "            loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "            lostot+=loss\n",
    "            loscnt+=1\n",
    "            if step%500==0:\n",
    "                #print(loss)  \n",
    "                if loscnt>0:\n",
    "                    print(lostot/loscnt)\n",
    "                    cbest = min(float(lostot/loscnt), cbest)\n",
    "                    best_loss = min(best_loss, cbest)\n",
    "                    print(\"cbest, \", cbest)\n",
    "                    if cbest==best_loss:\n",
    "                        torch.save(model.state_dict(),\"torchsaved/modcomestim\"+str(epoch)+\".pt\")\n",
    "            #best_loss = min(best_loss, float(loss))\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        cbest = min(float(lostot/loscnt), cbest)\n",
    "        best_loss = min(best_loss, cbest)\n",
    "        print(\"cbest, \", cbest)\n",
    "        if cbest==best_loss:\n",
    "            torch.save(model.state_dict(), \"torchsaved/modcomestim\"+str(epoch)+\".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaaeabd-4285-4ac6-9f48-5258b8b06660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankingLoss(score, summary_score=None, margin=0, gold_margin=0, gold_weight=1, no_gold=False, no_cand=False):\n",
    "    ones = torch.ones_like(score)\n",
    "    loss_func = torch.nn.MarginRankingLoss(0.0)\n",
    "    TotalLoss = loss_func(score, score, ones)\n",
    "    # candidate loss\n",
    "    n = score.size(1)\n",
    "    if not no_cand:\n",
    "        for i in range(1, n):\n",
    "            pos_score = score[:, :-i]\n",
    "            neg_score = score[:, i:]\n",
    "            pos_score = pos_score.contiguous().view(-1)\n",
    "            neg_score = neg_score.contiguous().view(-1)\n",
    "            ones = torch.ones_like(pos_score)\n",
    "            loss_func = torch.nn.MarginRankingLoss(margin * i)\n",
    "            loss = loss_func(pos_score, neg_score, ones)\n",
    "            TotalLoss += loss\n",
    "    if no_gold:\n",
    "        return TotalLoss\n",
    "    # gold summary loss\n",
    "    pos_score = summary_score.unsqueeze(-1).expand_as(score)\n",
    "    neg_score = score\n",
    "    pos_score = pos_score.contiguous().view(-1)\n",
    "    neg_score = neg_score.contiguous().view(-1)\n",
    "    ones = torch.ones_like(pos_score)\n",
    "    loss_func = torch.nn.MarginRankingLoss(gold_margin)\n",
    "    TotalLoss += gold_weight * loss_func(pos_score, neg_score, ones)\n",
    "    return TotalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf629fb-072b-4326-8af1-ccec6f11bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model \n",
    "model = XLMCometRegressor(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/comestim15.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46c96ce7-5520-4b13-92bf-58a2de69a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evaluate(model, torch.nn.MSELoss(), testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35907783-1f6f-4801-8ae5-d8e05bb80ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dede517-a94b-4e4e-a037-dac55a310ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28abafa5-bf6a-45b4-b5c2-37d76474cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sloss = [math.sqrt(l) for l in loss[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "33777a06-5501-45a1-b38e-9c86089438dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps : 84100\n",
      "EPOCH  0\n",
      "-----\n",
      "10000000000.0\n",
      "tensor(0.0330, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "cbest,  0.03302951902151108\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 3; 47.46 GiB total capacity; 16.93 GiB already allocated; 16.44 MiB free; 17.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [121]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      2\u001b[0m                   lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtinyloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device, clip_value)\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_inputs, batch_labels, batch_masks \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     17\u001b[0m                    \u001b[38;5;28mtuple\u001b[39m(b\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(outputs.squeeze().shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(batch_labels.squeeze().shape)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39msqueeze(), \n\u001b[1;32m     23\u001b[0m                  batch_labels\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36mXLMCometRegressor.forward\u001b[0;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_masks):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# don't finetune xlmroberta model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     word_rep, sentence_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxlmroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# use the first <s> token as a CLS token, TODO experiment with using the sum of \u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# outputs = self.regressor(torch.sum(word_rep, 1))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(torch\u001b[38;5;241m.\u001b[39msum(word_rep, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:843\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    836\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    837\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    838\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    842\u001b[0m )\n\u001b[0;32m--> 843\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    856\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:522\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    514\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 522\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    399\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m ):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ):\n\u001b[0;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/latenv/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:239\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    236\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 3; 47.46 GiB total capacity; 16.93 GiB already allocated; 16.44 MiB free; 17.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)\n",
    "model = train(model, optimizer, scheduler, nn.MSELoss(), 40, \n",
    "              tinyloader, device, clip_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c24ca-445c-4d04-8565-6a89ab8ae0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=5e-4,\n",
    "                  eps=1e-8)\n",
    "model = train(model, optimizer, scheduler, nn.MSELoss(), 2, \n",
    "              trainloader, device, clip_value=2)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)\n",
    "model = train(model, optimizer, scheduler, nn.MSELoss(), 10, \n",
    "              trainloader, device, clip_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4792d4-8a3b-4413-aef2-8c6ce3fa83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nval = [float(l) for l in loss[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e7c3dd6-a85f-4d18-b84c-4e9e968735d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23., 60., 75., 54., 24.,  7.,  2.,  4.,  0.,  1.]),\n",
       " array([0.04072549, 0.05284857, 0.06497165, 0.07709473, 0.0892178 ,\n",
       "        0.10134088, 0.11346396, 0.12558704, 0.13771012, 0.14983319,\n",
       "        0.16195627]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68853bf4-ea68-4eb3-ab3e-6a6c6608c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0028c47e-4fc8-4089-bf00-693ed030a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e6578-01b4-4ab4-87f0-69b68885ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(alldf['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1d335-f94b-4fd5-bd69-7f86d9e7269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old data loading stuff\n",
    "\n",
    "# data comes from WMT 2019 [I think], TODO validate exact year being used\n",
    "# data generated with gold reference given as hyp\n",
    "#golddf = pd.read_csv('./processeddata/golddata.csv')\n",
    "#golddf['inp'] = golddf['fr']\n",
    "#golddf['hyp'] = golddf['en']\n",
    "# data generated on candidates from beam search 50 and lattice (lots of bad)\n",
    "distill_df = pd.read_csv('distill_cometdata_1.csv')\n",
    "# data generated between random / unrelated sentences \n",
    "#rand_df = pd.read_csv('distill_cometdata_rand.csv')[:30000]\n",
    "# combine\n",
    "alldf = distill_df#.append(rand_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10bb4a-76b0-4ffd-af5d-7ad859d86a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted3 = alldf.sort_values(['ref', 'scores']).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f350f2-6407-464a-ad05-4d71433f4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inputs from dataframe\n",
    "def get_inputs(inpdf):\n",
    "    xinp = []\n",
    "    yinp = []\n",
    "    maskinp = []\n",
    "    \n",
    "    for index, row in inpdf.iterrows():\n",
    "        if index%1000==0:\n",
    "            print(index)\n",
    "        #print(row['c1'], row['c2'])\n",
    "        # will need to make a custom mask (maybe) so that inputs from both languages are encoded separately\n",
    "        toktmp = xlm_tok(row['inp']).input_ids\n",
    "        lent = len(toktmp)\n",
    "        hyptmp = xlm_tok(row['hyp']).input_ids\n",
    "        toktmp.extend(hyptmp)\n",
    "        mask = torch.ones(len(toktmp), len(toktmp))\n",
    "        # should set upper left and bottom right quadrants to 1, mask other stuff\n",
    "        # TODO make different types of masks. \n",
    "        #mask[:lent, :lent] = 1\n",
    "        #mask[lent:, lent:] = 1\n",
    "        # make causal encoder-decoder mask\n",
    "        mask[:lent, lent:] = 0\n",
    "        mask[lent:, lent:] = torch.tril(mask[lent:, lent:])\n",
    "        xinp.append(toktmp)\n",
    "        maskinp.append(mask)\n",
    "        yinp.append(row['scores'])\n",
    "    return xinp, yinp, maskinp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fa1228-da1f-4d64-8058-1c9c27abcdb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xdata, ydata, mdata \u001b[38;5;241m=\u001b[39m \u001b[43mget_inputs\u001b[49m(alldf)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "xdata, ydata, mdata = get_inputs(alldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ab69-47f1-4840-a4f9-758f167bcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(xdata)):\n",
    "    if len(xdata[i])>500:\n",
    "        del xdata[i]\n",
    "        del ydata[i]\n",
    "        del mdata[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840b719-e6fd-4818-afa6-10b3dbd9c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all as precaution\n",
    "def save_cometqe_data(md, xd, yd):\n",
    "    # save data into a pickle file\n",
    "    with open('processeddata/commasks2.pkl', 'wb') as f:\n",
    "        pickle.dump(md, f)\n",
    "\n",
    "    with open('processeddata/cominps2.pkl', 'wb') as f:\n",
    "        pickle.dump(xd, f)\n",
    "\n",
    "    with open('processeddata/comlabels2.pkl', 'wb') as f:\n",
    "        pickle.dump(yd, f)\n",
    "        \n",
    "#mdata, xdata, ydata = load_cometqe_data()\n",
    "        \n",
    "save_cometqe_data(mdata, xdata, ydata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
