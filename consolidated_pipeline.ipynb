{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0277c5-597c-40bf-9ef6-8916d34e067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_rerank as er\n",
    "import torch\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1301527-510a-4937-8e1b-04e84ed98188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2224514048"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in model, for french\n",
    "del model\n",
    "model = er.XLMCometEmbeds(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/divlat19.pt\"))\n",
    "model.eval()\n",
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "241836e0-2116-4891-8ff1-983630db4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daedc160-cab5-40d6-bdad-e48eb0c1ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"frtest_reversed/\"\n",
    "def test_graph_ind(ind, basedir):\n",
    "    g = pickle.load(open(basedir+str(ind), 'rb'))\n",
    "    if g['input'] in old['src']:\n",
    "        return None, None, None\n",
    "    try:\n",
    "        return g['input'], g['ref'], er.run_pipeline(g, model)\n",
    "    except:\n",
    "        return None, None, None\n",
    "    \n",
    "def get_all_preds(basedir):\n",
    "    l = len(os.listdir(basedir))\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        i, r, p = test_graph_ind(i, basedir)\n",
    "        res.append({\n",
    "            'src':i,\n",
    "            'hyp':p,\n",
    "            'ref':r\n",
    "        })\n",
    "        print(i)\n",
    "    res = pd.DataFrame(res)\n",
    "    res.to_csv(\"latfound\"+basedir[:-4]+\".csv\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccfec1-1237-4959-9db8-39c9590212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frpreds = get_all_preds(\"frtest_reversed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a9268-4d4c-4c76-8848-14d3c1900826",
   "metadata": {},
   "outputs": [],
   "source": [
    "depreds = get_all_preds(\"detest_reversed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a033cea6-d17a-4d5a-a192-e2f4f3fd0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"frtest_reversed/\"\n",
    "l = len(os.listdir(basedir))\n",
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693b273-1868-4eea-b570-4afaac3eacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(l):\n",
    "    i, r, p = test_graph_ind(i, basedir)\n",
    "    if i==None:\n",
    "        continue\n",
    "    res.append({\n",
    "        'src':i,\n",
    "        'hyp':p,\n",
    "        'ref':r\n",
    "    })\n",
    "    print(i)\n",
    "resdf = pd.DataFrame(res)\n",
    "frpreds = resdf\n",
    "resdf.to_csv(\"frenchlatpreds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed0c78d6-9dbb-47be-9c4c-4796f4fc943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMCometEmbeds(\n",
       "  (xlmroberta): XLMRobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "model = er.XLMCometEmbeds(drop_rate=0.1)\n",
    "model.load_state_dict(torch.load(\"./torchsaved/germanlat0.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd515659-0416-42a2-86da-3895978bba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"detest_reversed/\"\n",
    "l = len(os.listdir(basedir))\n",
    "res = []\n",
    "for i in range(l):\n",
    "    i, r, p = test_graph_ind(i, basedir)\n",
    "    res.append({\n",
    "        'src':i,\n",
    "        'hyp':p,\n",
    "        'ref':r\n",
    "    })\n",
    "    print(i)\n",
    "resdf = pd.DataFrame(res)\n",
    "depreds = resdf.to_csv(\"germanlatpreds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72698460-e7c3-4a22-8f00-82fbc73e08b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>hyp</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By the time Mao Zedong died, in 1976, the rura...</td>\n",
       "      <td>&lt;s&gt; By the time Mao Zedong died, in 1976, the ...</td>\n",
       "      <td>Als Mao Tse-Tung im Jahr 1976 starb, lag die W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(If the TTIP was opened to other economies – s...</td>\n",
       "      <td>&lt;s&gt; (If the TTIP was opened to other economies...</td>\n",
       "      <td>(Würde TTIP für andere Volkswirtschaften – wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On the other hand, China remains trapped by a ...</td>\n",
       "      <td>&lt;s&gt; On the other hand, China remains trapped b...</td>\n",
       "      <td>Auf der anderen Seite bleibt China seiner Verg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Simonyi wrote the Microsoft Word program, and...</td>\n",
       "      <td>&lt;s&gt; (Simonyi wrote the Microsoft Word program,...</td>\n",
       "      <td>(Simonyi entwickelte das Programm Microsoft Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Everyone agrees that Iran has the right to do ...</td>\n",
       "      <td>&lt;s&gt; Everyone agrees that Iran has the right to...</td>\n",
       "      <td>Alle sind sich einig, dass dem Iran das Recht ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>But this does not mean that we should just cut...</td>\n",
       "      <td>&lt;s&gt; But this does not mean that we should just...</td>\n",
       "      <td>Aber das heißt nicht, dass wir einfach auf all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>It might seem from these numbers that Europe h...</td>\n",
       "      <td>&lt;s&gt; It might seem from these numbers that Euro...</td>\n",
       "      <td>Aus diesen Zahlen könnte man ableiten, dass di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>In other words, the structures that companies ...</td>\n",
       "      <td>&lt;s&gt; In other words, the structures that compan...</td>\n",
       "      <td>Anders ausgedrückt: Die Strukturen, die Untern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>On October 24, you can stand up against this i...</td>\n",
       "      <td>&lt;s&gt; On October 24, you can stand up against th...</td>\n",
       "      <td>Am 24. Oktober besteht die Möglichkeit, gegen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>Perhaps most importantly, competition is a pow...</td>\n",
       "      <td>&lt;s&gt; Perhaps most importantly, competition is a...</td>\n",
       "      <td>Am wichtigsten ist vielleicht, dass der Wettbe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1077 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0     By the time Mao Zedong died, in 1976, the rura...   \n",
       "1     (If the TTIP was opened to other economies – s...   \n",
       "2     On the other hand, China remains trapped by a ...   \n",
       "3     (Simonyi wrote the Microsoft Word program, and...   \n",
       "4     Everyone agrees that Iran has the right to do ...   \n",
       "...                                                 ...   \n",
       "1072  But this does not mean that we should just cut...   \n",
       "1073  It might seem from these numbers that Europe h...   \n",
       "1074  In other words, the structures that companies ...   \n",
       "1075  On October 24, you can stand up against this i...   \n",
       "1076  Perhaps most importantly, competition is a pow...   \n",
       "\n",
       "                                                    hyp  \\\n",
       "0     <s> By the time Mao Zedong died, in 1976, the ...   \n",
       "1     <s> (If the TTIP was opened to other economies...   \n",
       "2     <s> On the other hand, China remains trapped b...   \n",
       "3     <s> (Simonyi wrote the Microsoft Word program,...   \n",
       "4     <s> Everyone agrees that Iran has the right to...   \n",
       "...                                                 ...   \n",
       "1072  <s> But this does not mean that we should just...   \n",
       "1073  <s> It might seem from these numbers that Euro...   \n",
       "1074  <s> In other words, the structures that compan...   \n",
       "1075  <s> On October 24, you can stand up against th...   \n",
       "1076  <s> Perhaps most importantly, competition is a...   \n",
       "\n",
       "                                                    ref  \n",
       "0     Als Mao Tse-Tung im Jahr 1976 starb, lag die W...  \n",
       "1     (Würde TTIP für andere Volkswirtschaften – wie...  \n",
       "2     Auf der anderen Seite bleibt China seiner Verg...  \n",
       "3     (Simonyi entwickelte das Programm Microsoft Wo...  \n",
       "4     Alle sind sich einig, dass dem Iran das Recht ...  \n",
       "...                                                 ...  \n",
       "1072  Aber das heißt nicht, dass wir einfach auf all...  \n",
       "1073  Aus diesen Zahlen könnte man ableiten, dass di...  \n",
       "1074  Anders ausgedrückt: Die Strukturen, die Untern...  \n",
       "1075  Am 24. Oktober besteht die Möglichkeit, gegen ...  \n",
       "1076  Am wichtigsten ist vielleicht, dass der Wettbe...  \n",
       "\n",
       "[1077 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8fbb45d-e007-49d0-a5c5-052e2f4efc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "frpreds = pd.read_csv(\"frenchlatpreds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc8106d2-9ebf-4bb9-8efa-3c1506c45410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> By the time Mao Zedong died, in 1976, the rural economy was a shambles.</s><s> de_DE Bei der Todestagszeit Mao Zedongs, im Jahr 1976, war die ländliche Wirtschaft in einem Schlamm.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depreds.loc[0]['hyp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e17e471f-3c24-4013-b944-0a4629e5baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_hyps(hyplist, cutoff):\n",
    "    res = []\n",
    "    for h in hyplist:\n",
    "        cind = h[3:].index(cutoff)+len(cutoff)+3\n",
    "        res.append(h[cind:])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f2eab03-5dda-47b4-aa6f-34d4dda63220",
   "metadata": {},
   "outputs": [],
   "source": [
    "frpreds['ahyp'] = get_act_hyps(frpreds['hyp'], \"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39931db3-4144-498d-b86d-5d4ac6b398c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src</th>\n",
       "      <th>hyp</th>\n",
       "      <th>ref</th>\n",
       "      <th>ahyp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Après tout, en tant que journaliste d'investig...</td>\n",
       "      <td>&lt;s&gt; Après tout, en tant que journaliste d'inve...</td>\n",
       "      <td>After all, as a campaigning investigative jour...</td>\n",
       "      <td>After all, as an investigative reporter in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Au fur et à mesure que l’Iran a cherché à éten...</td>\n",
       "      <td>&lt;s&gt; Au fur et à mesure que l’Iran a cherché à ...</td>\n",
       "      <td>As Iran seeks to assert its influence and inte...</td>\n",
       "      <td>As Iran sought to extend its influence and ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Une autre variable réside dans la question de ...</td>\n",
       "      <td>&lt;s&gt; Une autre variable réside dans la question...</td>\n",
       "      <td>Another variable is how much easier monetary p...</td>\n",
       "      <td>The other variable is the extent to which oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Il nous faut négocier un nouvel accord de part...</td>\n",
       "      <td>&lt;s&gt; Il nous faut négocier un nouvel accord de ...</td>\n",
       "      <td>A new Partnership and Cooperation Agreement (P...</td>\n",
       "      <td>We need to negotiate a new partnership and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Et lorsqu’une région du monde conçoit une meil...</td>\n",
       "      <td>&lt;s&gt; Et lorsqu’une région du monde conçoit une ...</td>\n",
       "      <td>And as one region in one part of the world des...</td>\n",
       "      <td>And when a region of the world devises better...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>839</td>\n",
       "      <td>Au moment même de lire ces lignes, des personn...</td>\n",
       "      <td>&lt;s&gt; Au moment même de lire ces lignes, des per...</td>\n",
       "      <td>As you read this, perfectly ordinary people so...</td>\n",
       "      <td>At the very time of reading these lines, perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>840</td>\n",
       "      <td>C’est ainsi que les médias traditionnels se so...</td>\n",
       "      <td>&lt;s&gt; C’est ainsi que les médias traditionnels s...</td>\n",
       "      <td>As a result, mainstream media are being margin...</td>\n",
       "      <td>As a result, the traditional media have progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>841</td>\n",
       "      <td>Un troisième impératif est de construire un mo...</td>\n",
       "      <td>&lt;s&gt; Un troisième impératif est de construire u...</td>\n",
       "      <td>A third imperative is building a safer and mor...</td>\n",
       "      <td>A third imperative is the building of a safer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>842</td>\n",
       "      <td>Une approche « portefeuille » du changement cl...</td>\n",
       "      <td>&lt;s&gt; Une approche « portefeuille » du changemen...</td>\n",
       "      <td>A “Portfolio” Approach to Climate Change</td>\n",
       "      <td>A \"portfolio \" approach to global climate cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>843</td>\n",
       "      <td>Au plan intérieur également, une méfiance géné...</td>\n",
       "      <td>&lt;s&gt; Au plan intérieur également, une méfiance ...</td>\n",
       "      <td>At home, too, massive distrust will further co...</td>\n",
       "      <td>Internally, too, widespread distrust will mak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                src  \\\n",
       "0             0  Après tout, en tant que journaliste d'investig...   \n",
       "1             1  Au fur et à mesure que l’Iran a cherché à éten...   \n",
       "2             2  Une autre variable réside dans la question de ...   \n",
       "3             3  Il nous faut négocier un nouvel accord de part...   \n",
       "4             4  Et lorsqu’une région du monde conçoit une meil...   \n",
       "..          ...                                                ...   \n",
       "839         839  Au moment même de lire ces lignes, des personn...   \n",
       "840         840  C’est ainsi que les médias traditionnels se so...   \n",
       "841         841  Un troisième impératif est de construire un mo...   \n",
       "842         842  Une approche « portefeuille » du changement cl...   \n",
       "843         843  Au plan intérieur également, une méfiance géné...   \n",
       "\n",
       "                                                   hyp  \\\n",
       "0    <s> Après tout, en tant que journaliste d'inve...   \n",
       "1    <s> Au fur et à mesure que l’Iran a cherché à ...   \n",
       "2    <s> Une autre variable réside dans la question...   \n",
       "3    <s> Il nous faut négocier un nouvel accord de ...   \n",
       "4    <s> Et lorsqu’une région du monde conçoit une ...   \n",
       "..                                                 ...   \n",
       "839  <s> Au moment même de lire ces lignes, des per...   \n",
       "840  <s> C’est ainsi que les médias traditionnels s...   \n",
       "841  <s> Un troisième impératif est de construire u...   \n",
       "842  <s> Une approche « portefeuille » du changemen...   \n",
       "843  <s> Au plan intérieur également, une méfiance ...   \n",
       "\n",
       "                                                   ref  \\\n",
       "0    After all, as a campaigning investigative jour...   \n",
       "1    As Iran seeks to assert its influence and inte...   \n",
       "2    Another variable is how much easier monetary p...   \n",
       "3    A new Partnership and Cooperation Agreement (P...   \n",
       "4    And as one region in one part of the world des...   \n",
       "..                                                 ...   \n",
       "839  As you read this, perfectly ordinary people so...   \n",
       "840  As a result, mainstream media are being margin...   \n",
       "841  A third imperative is building a safer and mor...   \n",
       "842          A “Portfolio” Approach to Climate Change    \n",
       "843  At home, too, massive distrust will further co...   \n",
       "\n",
       "                                                  ahyp  \n",
       "0     After all, as an investigative reporter in th...  \n",
       "1     As Iran sought to extend its influence and ad...  \n",
       "2     The other variable is the extent to which oth...  \n",
       "3     We need to negotiate a new partnership and co...  \n",
       "4     And when a region of the world devises better...  \n",
       "..                                                 ...  \n",
       "839   At the very time of reading these lines, perf...  \n",
       "840   As a result, the traditional media have progr...  \n",
       "841   A third imperative is the building of a safer...  \n",
       "842   A \"portfolio \" approach to global climate cha...  \n",
       "843   Internally, too, widespread distrust will mak...  \n",
       "\n",
       "[844 rows x 5 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "85c53317-3d89-4ee7-92f1-c7300d24d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comet_scores(hyps, srcs, refs):\n",
    "    cometqe_input = [{\"src\": src, \"mt\": mt, \"ref\":ref} for src, mt, ref in zip(srcs, hyps, refs)]\n",
    "    # sentence-level and corpus-level COMET\n",
    "    outputs = comet.predict(\n",
    "        cometqe_input, batch_size=32, progress_bar=True\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71cd529d-651b-4c35-be71-4a02b2215d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cometqe_dir = \"./cometqemodel\"\n",
    "# can alternatively use wmt21-comet-qe-mqm\n",
    "cometqe_model = \"wmt20-comet-qe-da\"\n",
    "cometmodel = \"wmt20-comet-da\"\n",
    "batch_size = 64\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb818c35-c952-4abe-8f40-36525db80650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb564d2a-71a8-45b5-94bd-fa6febff5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_path = download_model(cometmodel, \"./cometmodel\")\n",
    "comet = load_from_checkpoint(comet_path)\n",
    "comet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a95f2c3f-e859-40af-a1c8-46969d4b9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441afbe0-62a4-4e58-88d8-c1364bbb8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "scos = get_comet_scores(frpreds['ahyp'], frpreds['src'], frpreds['ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7cab774-12cf-4eec-b8db-074c56af3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "frpreds['scos'] = scos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b473785-da12-4209-a20a-12f9c6a3d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frpreds.to_csv(\"frenchlatpreds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "accf2281-f9ee-4281-8bb3-b6f146e38e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Already nearly everybody in the North Atlantic region has enough food to avoid hunger, enough clothing to stay warm, enough shelter to remain dry. And yet we want more, feel resentful when we do not get it, and are self-aware enough to know that luxuries turn into conveniences, and then into necessities – and that we are very good at inventing new luxuries after which to strive. '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frpreds[frpreds['scos']<0].loc[25]['ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33354134-a51e-48f2-9875-2f980a3479a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03813996849133665"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(depreds['scos'])/1077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4cd7acd6-4f7f-41a5-89b8-d3dff858f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a734ac70-7e88-48c0-8d65-5c3df8e21f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,   8.,  28.,  12.,  14.,  14.,  64., 110., 374., 217.]),\n",
       " array([-1.84165907, -1.55159155, -1.26152403, -0.97145652, -0.681389  ,\n",
       "        -0.39132148, -0.10125396,  0.18881356,  0.47888107,  0.76894859,\n",
       "         1.05901611]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(frpreds['scos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575048a-972d-4504-9b45-27f15329c34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
