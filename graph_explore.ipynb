{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bert_score import BERTScorer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import *\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import treebank #ptb tokenizer to convert for DAE stuff\n",
    "ptb_tok = treebank.TreebankWordTokenizer()\n",
    "detok = treebank.TreebankWordDetokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=False, model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "rng = np.random.default_rng(19680801)\n",
    "\n",
    "df = pd.read_csv('./maynez_docset.csv')\n",
    "\n",
    "# opening the json file\n",
    "with open('subgraph_meta/data.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only pair, with context, with after, all\n",
    "OPTIONS  = ['OP', 'C', 'AF', 'ALL']\n",
    "\n",
    "# method that gets a indexed lists for the pairs, ready for scoring\n",
    "def get_graph_pairs(pairs, option):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for p in pairs:\n",
    "        s1 = p['s1']\n",
    "        s2 = p['s2']\n",
    "        if (option == 'C') or (option == 'ALL'):\n",
    "            s1 = p['context']+s1\n",
    "            s2 = p['context']+s2\n",
    "        if (option == 'AF') or (option == 'ALL'):\n",
    "            s1 = s1+p['afterstr']\n",
    "            s2 = s2+p['afterstr']\n",
    "        if (s1.strip() == \"\") or (s2.strip() == \"\"):\n",
    "            continue\n",
    "        # this is a reverse of another example\n",
    "        if s1 in l2:\n",
    "            if l1[l2.index(s1)] == s2:\n",
    "                continue\n",
    "        l1.append(s1)\n",
    "        l2.append(s2)\n",
    "    return l1, l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(data.keys())\n",
    "\n",
    "def score_graph_ind(ind, tech):\n",
    "    strings1, strings2 = get_graph_pairs(data[keylist[ind]], tech)\n",
    "    try:\n",
    "        P, R, F1 = scorer.score(strings1, strings2)\n",
    "        #print(\"ID: \"+keylist[ind])\n",
    "        #print(F1.mean())\n",
    "        return F1\n",
    "    except:\n",
    "        return \n",
    "\n",
    "def get_all_graphdata(option):\n",
    "    f1s = []\n",
    "    for i in range(0, len(data.keys())):\n",
    "        g = score_graph_ind(i, option)\n",
    "        tmp = {}\n",
    "        tmp['id'] = i\n",
    "        tmp['f1mat'] = g\n",
    "        if g != None:\n",
    "            f1s.append(tmp)\n",
    "        else:\n",
    "            tmp['f1mat'] = [-1];\n",
    "            f1s.append(tmp)\n",
    "    return f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(data.keys())\n",
    "\n",
    "def vis_score_graph(ind, option):\n",
    "    \n",
    "    return vis_score_id(keylist[ind], option)\n",
    "\n",
    "def get_context(idval, dictkey):\n",
    "    newid = int(idval)\n",
    "    tmp = df.loc[df['bbcid']==newid]\n",
    "    return tmp[dictkey][tmp.first_valid_index()]\n",
    "\n",
    "def simcount(scores):\n",
    "    simcount = 0\n",
    "    THRESH = .7\n",
    "    for s in scores:\n",
    "        if s>THRESH:\n",
    "            simcount+=1\n",
    "    return simcount, len(scores)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stopwords(sraw):\n",
    "    word_tokens = word_tokenize(sraw)\n",
    "    \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def check_content (filt1, filt2):\n",
    "    stem1 = [stemmer.stem(f) for f in filt1]\n",
    "    stem2 = [stemmer.stem(f) for f in filt2]\n",
    "    if stem1==stem2:\n",
    "        return True\n",
    "\n",
    "def loose_stem(filt1, filt2):\n",
    "    stem1 = [stemmer.stem(f) for f in filt1]\n",
    "    stem2 = [stemmer.stem(f) for f in filt2]\n",
    "    for s in stem1:\n",
    "        if s in stem2:\n",
    "            return True\n",
    "\n",
    "def count_stop_stem(list1, list2):\n",
    "    stopmatches = []\n",
    "    stemmatches = []\n",
    "    loosematches = []\n",
    "    for i in range (0, len(list1)):\n",
    "        t1 = filter_stopwords(list1[i].replace(\"\\\"\", \"\"))\n",
    "        t2 = filter_stopwords(list2[i].replace(\"\\\"\", \"\"))\n",
    "        if t1==t2:\n",
    "            stopmatches.append(i)\n",
    "        elif check_content(t1, t2):\n",
    "            stemmatches.append(i)\n",
    "        elif loose_stem(t1, t2):\n",
    "            loosematches.append(i)\n",
    "    return stopmatches, stemmatches, loosematches\n",
    "\n",
    "def get_len_metrics(list1, list2):\n",
    "    lens1 = []\n",
    "    lens2 = []\n",
    "    for i in range(0, len(list1)):\n",
    "        lens1.append(len(list1[i].split()))\n",
    "        lens2.append(len(list2[i].split()))\n",
    "    lens1 = np.array(lens1)\n",
    "    lens2 = np.array(lens2)\n",
    "    avglen = (np.mean(lens1)+np.mean(lens2))/2\n",
    "    avgdiff = np.mean(np.absolute(lens2-lens1))\n",
    "    numzeros = len(lens1)+len(lens2) - (np.count_nonzero(lens1)+np.count_nonzero(lens2))\n",
    "    return avglen, avgdiff, numzeros\n",
    "\n",
    "def filter_stringlists (l1, l2):\n",
    "    count = 0\n",
    "    for i in range(0, len(l1)):\n",
    "        if l1[i].split==l2[i].split():\n",
    "            count+=1\n",
    "            del l1[i]\n",
    "            del l2[i]\n",
    "    #print(str(count)+\" WEIRD REPEATS REMOVED\")\n",
    "    return l1, l2\n",
    "\n",
    "keylist = list(data.keys()) \n",
    "\n",
    "\n",
    "def vis_score_id(idv, option):\n",
    "    ind = keylist.index(idv)\n",
    "    exampleid = keylist[ind]\n",
    "    # store metrics in dictionary\n",
    "    res = {}\n",
    "    res['id'] = exampleid\n",
    "    res['context'] = get_context(idv, 'summary')\n",
    "\n",
    "    #print(\"Context: \\n\"+get_context(idv, 'doc'))\n",
    "    strings1, strings2 = get_graph_pairs(data[keylist[ind]], option)\n",
    "    strings1, strings2 = filter_stringlists(strings1, strings2)\n",
    "    try: \n",
    "        scores = score_graph_ind(ind, option)\n",
    "        scount, tot = simcount(scores)\n",
    "    except:\n",
    "        scores = []\n",
    "        scount, tot = -1, len(strings1)\n",
    "    content, stem, loose_stem = count_stop_stem(strings1, strings2)\n",
    "    #identical except for stopwords or quotations\n",
    "    res['content_same'] = len(content)\n",
    "    #identical content words after stemming (that doesn't overlap w/ content_same)\n",
    "    res['stem_same'] = len(stem)\n",
    "    res['loose_stem'] = len(loose_stem)\n",
    "    #based on BERT-SCORE similarity\n",
    "    res['similar'] = scount\n",
    "    #metrics based on length\n",
    "    res['avglen'], res['avgdiff'], res['numzeros'] = get_len_metrics(strings1, strings2)\n",
    "    #total number of pairs\n",
    "    res['total'] = tot\n",
    "    res['scores'] = scores\n",
    "    res['strings1'] = strings1\n",
    "    res['strings2'] = strings2\n",
    "\n",
    "    return res\n",
    "\n",
    "def print_results(res):\n",
    "    for i in range(0, len(res['strings1'])):\n",
    "        print(str(res['scores'][i]))\n",
    "        print(res['strings1'][i])\n",
    "        print(res['strings2'][i])\n",
    "        print(\"----------\")\n",
    "\n",
    "def get_all_results(option):\n",
    "    reslist = []\n",
    "    for i in range(0, len(keylist)):\n",
    "        reslist.append(vis_score_graph(i, option))\n",
    "    return reslist\n",
    "\n",
    "#all_results = get_all_results('OP')\n",
    "#all_res_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(vis_score_graph(1, \"AF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lemma(tok, d2):\n",
    "    for d in d2:\n",
    "        if tok.lemma_==d.lemma_:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def analyze_pos(ind, item):\n",
    "    s1 = all_res_df.loc[ind]['strings1']\n",
    "    s2 = all_res_df.loc[ind]['strings2']\n",
    "    #print(s1[item])\n",
    "    # print(s2[item])\n",
    "    d1 = nlp(s1[item])\n",
    "    d2 = nlp(s2[item])\n",
    "    ncount1, vcount1 = 0, 0\n",
    "    sharedn, sharedv = 0, 0\n",
    "    for i in range(0, len(d1)):\n",
    "        if d1[i].is_stop:\n",
    "            continue\n",
    "        if \"VB\" in d1[i].tag_:\n",
    "            vcount1+=1\n",
    "            if check_lemma(d1[i], d2):\n",
    "                sharedv+=1\n",
    "        elif \"NN\" in d1[i].tag_:\n",
    "            ncount1+=1\n",
    "            if check_lemma(d1[i], d2):\n",
    "                sharedn+=1\n",
    "    ncount2, vcount2 = 0, 0\n",
    "    for i in range(0, len(d2)):\n",
    "        if d2[i].is_stop:\n",
    "            continue\n",
    "        if \"VB\" in d2[i].tag_:\n",
    "            vcount2+=1\n",
    "        elif \"NN\" in d2[i].tag_:\n",
    "            ncount2+=1\n",
    "    vals = (sharedn, sharedv, vcount1, ncount1, vcount2, ncount2)\n",
    "    res = []\n",
    "    for v in vals:\n",
    "        res.append(v)\n",
    "    return res\n",
    "\n",
    "def get_pos_info(ind):\n",
    "    length =  len(all_res_df.loc[ind]['strings1'])\n",
    "    if length==0:\n",
    "        return None\n",
    "    tot = analyze_pos(ind, 0)\n",
    "    if length==1:\n",
    "        return tot\n",
    "    for i in range(1,length):\n",
    "        tmp = analyze_pos(ind, i)\n",
    "        for j in range(0, len(tot)):\n",
    "            tot[j]+=tmp[j]\n",
    "    for j in range(0, len(tot)):\n",
    "            tot[j]=tot[j]/length\n",
    "    return tot\n",
    "\n",
    "pos_labels = ['sharedn', 'sharedv', 'vcount1', 'ncount1', 'vcount2', 'ncount2']\n",
    "#takes long to run\n",
    "def get_all_posinfo():\n",
    "    res = []\n",
    "    for i in range(0, all_res_df.shape[0]):\n",
    "        res.append (get_pos_info(i))\n",
    "    res = np.array(res)\n",
    "    for i in range(0, len(res)):\n",
    "        if res[i]==None:\n",
    "            res[i]=[-1,-1,-1,-1,-1,-1]\n",
    "    length = max(map(len, res))\n",
    "    y=np.array([xi+[None]*(length-len(xi)) for xi in res])\n",
    "    for i in range(0, len(pos_labels)):\n",
    "        all_res_df[pos_labels[i]] = y[:, i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df = pd.read_json('subgraph_meta/processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_res_df['cont_prop'] = all_res_df['content_same'] / all_res_df['total']\n",
    "#all_res_df['stemp_prop'] = all_res_df['stem_same'] / all_res_df['total']\n",
    "#get_all_posinfo()\n",
    "#all_res_df = all_res_df.fillna(-1)\n",
    "#all_res_df.loc[all_res_df['avgdiff']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff = all_res_df['ncount1']+all_res_df['ncount2']-all_res_df['sharedn']\n",
    "small = all_res_df.loc[all_res_df['total']<70]\n",
    "large = all_res_df.loc[all_res_df['total']>70]\n",
    "sns.histplot(data=small['sharedn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(data=large['sharedn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO come back to this later\n",
    "def convert_alldata_df():\n",
    "    res = []\n",
    "    for a in alldata:\n",
    "         res.append(np.mean(np.array(a['f1mat'])))\n",
    "    return res\n",
    "#all_res_df['sim_avgs'] = convert_alldata_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_res_df.to_json('subgraph_meta/processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_from_id(contextid):\n",
    "    return df.loc[df['bbcid']==int(contextid)].iloc[0]['doc']\n",
    "\n",
    "def tok_store_doc(doc, summ):\n",
    "    tmp = {}\n",
    "    tmp['doc'] = ptb_tok.tokenize(doc)\n",
    "    tmp['doc'] = detok.detokenize(tmp['doc'])\n",
    "    tmp['summ'] = ptb_tok.tokenize(summ)\n",
    "    tmp['summ'] = detok.detokenize(tmp['summ'])\n",
    "    return tmp\n",
    "\n",
    "def get_contexts_fullsums():\n",
    "    \n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    for index, row in all_res_df.iterrows():\n",
    "        docstr = get_context_from_id(row['id'])\n",
    "        for i in range(0, len(row['contexts'])):\n",
    "            #temporarily removing contexts, after\n",
    "            res1.append(tok_store_doc(docstr, row['s1s'][i]))#+row['afters'][i]))\n",
    "            res2.append(tok_store_doc(docstr, row['s2s'][i]))#+row['afters'][i]))\n",
    "        print(index)\n",
    "    return res1, res2\n",
    "\n",
    "#tok_tmp = ptb_tok.tokenize(text=all_res_df.loc[450]['afters'])\n",
    "#newstr = detok.detokenize(tok_tmp)\n",
    "#cont_fullsums = get_contexts_fullsums()\n",
    "#write_dae_input(cont_fullsums[1], 'onlypath2')\n",
    "#cont_fullsums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dae_input(fullsum, name):\n",
    "    with open(name+'.txt', 'w') as f:\n",
    "        for full in fullsum:\n",
    "            f.write(full['doc']+'\\n')\n",
    "            f.write(full['summ']+'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_context(cont):\n",
    "    cont = cont.replace(\"</s>\", \"\")\n",
    "    cont = cont.replace(\"-\", \"\")\n",
    "    return cont\n",
    "\n",
    "def get_conts_afts():\n",
    "    afters = []\n",
    "    contexts = []\n",
    "    s1s = []\n",
    "    s2s = []\n",
    "    for latid in all_res_df['id']:\n",
    "        alist = []\n",
    "        clist = []\n",
    "        s1l = []\n",
    "        s2l = []\n",
    "        pathlist = data[str(latid)]\n",
    "        for p in pathlist:\n",
    "            alist.append(p['afterstr'])\n",
    "            clist.append(clean_context(p['context']))\n",
    "            s1l.append(clean_context(p['s1']))\n",
    "            s2l.append(clean_context(p['s2']))\n",
    "        afters.append(alist)\n",
    "        contexts.append(clist)\n",
    "        s1s.append(s1l)\n",
    "        s2s.append(s2l)\n",
    "    return afters, contexts, s1s, s2s\n",
    "\n",
    "all_res_df['afters'], all_res_df['contexts'],all_res_df['s1s'], all_res_df['s2s']  = get_conts_afts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dae_input(fn):\n",
    "    linedata = []\n",
    "    docdata = []\n",
    "    i = 0\n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            if (i%3)==0:\n",
    "                docdata.append(line)\n",
    "            if (i-1)%3==0:\n",
    "                linedata.append(line)\n",
    "            i = i+1\n",
    "    return linedata, docdata\n",
    "\n",
    "fp1, docs1 = read_dae_input('fullpath1.txt')\n",
    "fp2, docs2 = read_dae_input('fullpath2-shortened.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1data = []\n",
    "p2data = []\n",
    "with open('path1-output.txt') as f:\n",
    "    p1data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('path2-output.txt') as f:\n",
    "    p2data = [json.loads(line) for line in f]\n",
    "\n",
    "dae_df_1 = pd.DataFrame(p1data)\n",
    "dae_df_2 = pd.DataFrame(p2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pred_avgs(predlist):\n",
    "    avgs = []\n",
    "    for plist in predlist:\n",
    "        avgs.append(sum(plist)/len(plist))\n",
    "    return avgs\n",
    "\n",
    "# indicates roughly how different they are as a measure of differences of factual arcs \n",
    "# rationed per l1 \n",
    "def get_fact_diffs(p1list, p2list):\n",
    "    listlen = min(len(p1list), len(p2list))\n",
    "    diffs = []\n",
    "    for i in range(0, listlen):\n",
    "        diffs.append(abs(sum(p2list[i])-sum(p1list[i]))/len(p1list[i]))\n",
    "    return diffs\n",
    "\n",
    "def get_conf_sum(clist):\n",
    "    tot = 0\n",
    "    for conf in clist:\n",
    "        tot+=conf[1]\n",
    "    return tot\n",
    "\n",
    "def get_conf_diffs(c1list, c2list):\n",
    "    listlen = min(len(c1list), len(c2list))\n",
    "    diffs = []\n",
    "    for i in range(0, listlen):\n",
    "        sum1 = get_conf_sum(c1list[i])\n",
    "        sum2 = get_conf_sum(c2list[i])\n",
    "        diffs.append(abs(sum1-sum2)/((sum1+sum2)*0.5))\n",
    "    return diffs\n",
    "\n",
    "sns.histplot(data=get_conf_diffs(dae_df_1['probs'], dae_df_2['probs']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline sanity check for the bertscorer\n",
    "def get_rand_stringlists(n):\n",
    "    slist = []\n",
    "\n",
    "    for i in range(0, n):\n",
    "        try:\n",
    "            docind = random.randint(0, len(all_res_df))\n",
    "            tl = all_res_df.loc[docind]\n",
    "            line = random.randint(0, len(tl['s1s']))\n",
    "            slist.append(tl['contexts'][line]+tl['s1s'][line]+tl['afters'][line])\n",
    "            #slist.append(tl['s1s'][line])\n",
    "        except:\n",
    "            i = i-1\n",
    "    return slist\n",
    "\n",
    "def do_random_baseline():\n",
    "    l1 = get_rand_stringlists(1000)\n",
    "    l2 = get_rand_stringlists(1000)\n",
    "    l1 = l1[:len(l2)]\n",
    "    l2 = l2[:len(l1)]\n",
    "    print(len(l1))\n",
    "    print(len(l2))\n",
    "    P, R, F1 = scorer.score(l1, l2)\n",
    "    return P, R, F1 \n",
    "        \n",
    "\n",
    "def update_df():\n",
    "    global dae_df_2\n",
    "    plist = []\n",
    "    rlist = []\n",
    "    f1list = []\n",
    "    s1list = []\n",
    "    s2list = []\n",
    "    P, R, F1 = scorer.score(fp1[:len(fp2)], fp2[:len(fp2)])\n",
    "    for index, row in dae_df_2.iterrows():\n",
    "        plist.append(float(P[index]))\n",
    "        rlist.append(float(R[index]))\n",
    "        f1list.append(float(F1[index]))\n",
    "        s1list.append(fp1[index])\n",
    "        s2list.append(fp2[index])\n",
    "    dae_df_2['P'], dae_df_2['R'], dae_df_2['F1'] = plist, rlist, f1list\n",
    "    dae_df_2['s1'], dae_df_2['s2'] = s1list, s2list\n",
    "#need to uncomment every time\n",
    "update_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"abcdef\"\n",
    "s[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str(ind, lis):\n",
    "    res = lis[ind]\n",
    "    for i in range(ind+1, len(lis)):\n",
    "        res = res+\" \"+lis[i]\n",
    "    return res\n",
    "\n",
    "# physically get different path strings from full path\n",
    "def find_indpaths(s1, s2):\n",
    "    l1 = s1.split()\n",
    "    l2 = s2.split()\n",
    "    start = 0\n",
    "    for i in range(0, min(len(l1), len(l2))):\n",
    "        if (l1[i]==l2[i]):\n",
    "            continue\n",
    "        start = i\n",
    "        break\n",
    "    l1 = l1[start:]\n",
    "    l2 = l2[start:]\n",
    "    for i in range(1, min(len(l1), len(l2))+1):\n",
    "        if l1[-i]==l2[-i]:\n",
    "            \"\"\n",
    "        else:\n",
    "            l1 = l1[:(-i+1)]\n",
    "            l2 = l2[:(-i+1)]\n",
    "            break\n",
    "    \n",
    "    #if len(l1)>1 or len(l2)>1:\n",
    "        #print(l1)    \n",
    "    return get_str(0, l1), get_str(0, l2)\n",
    "\n",
    "        \n",
    "def get_clean_table():\n",
    "    alldata = []\n",
    "    dae_a_list = dae_df_1['preds']\n",
    "    dae_b_list = dae_df_2['preds']\n",
    "    p = dae_df_2['P']\n",
    "    r = dae_df_2['R']\n",
    "    f1 = dae_df_2['F1']\n",
    "    for i in range(0, len(fp2)):\n",
    "        tmp = {}\n",
    "        tmp['doc'] = docs1[i]\n",
    "        assert docs1[i]==docs2[i]\n",
    "        tmp['full_a'] = fp1[i]\n",
    "        tmp['full_b'] = fp2[i]\n",
    "        tmp['invalid'] = False\n",
    "        if tmp['full_a']==tmp['full_b']:\n",
    "            print(\"BAD\")\n",
    "            tmp['invalid'] = True\n",
    "        tmp['short_a'], tmp['short_b'] = find_indpaths(tmp['full_a'], tmp['full_b']) \n",
    "        tmp['dae_a'] = dae_a_list[i]\n",
    "        tmp['dae_b'] = dae_b_list[i]\n",
    "        tmp['precision'] =  p[i]\n",
    "        tmp['recall'] = r[i]\n",
    "        tmp['f1']=  f1[i]\n",
    "        alldata.append(tmp)\n",
    "    return pd.DataFrame(alldata)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "table = get_clean_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(table.loc[table['invalid']==False]).to_csv(\"pathdata_table.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('pathdata_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(slist):\n",
    "    for i in range(0, len(slist)):\n",
    "        slist[i] = slist[i].replace(\"\\n\",\"\" )\n",
    "    slist = [line.strip() for line in slist]\n",
    "    return slist\n",
    "\n",
    "def clean_bertscore_out(table):\n",
    "    d = clean_str(table['doc'][:1])\n",
    "    fa = [clean_str(table['full_a'][:10])]\n",
    "    fb = [clean_str(table['full_b'][:10])]\n",
    "    return scorer.score(d, fa)\n",
    "\n",
    "clean_bertscore_out(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sum(P)/len(P))\n",
    "#print(sum(R)/len(R))\n",
    "#print(sum(F1)/len(F1))\n",
    "sentsum = dae_df_2['sent']+dae_df_1['sent'][:len(dae_df_2)]\n",
    "sns.scatterplot(x=dae_df_2['F1'], y=sentsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "spearmanr(np.array(dae_df_2['F1']), np.array(get_fact_diffs(dae_df_1['preds'], dae_df_2['preds'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_base = do_random_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(rand_base[0])/len(rand_base[0]))\n",
    "print(sum(rand_base[1])/len(rand_base[1]))\n",
    "print(sum(rand_base[2])/len(rand_base[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = \"fat boy.\"\n",
    "re.sub(r\"[^a-zA-Z0-9]+\", ' ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_df_1.loc[0]['arcs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affected_arcs(row, prunedletter):\n",
    "    refdf = dae_df_1\n",
    "    if prunedletter =='b':\n",
    "        refdf = dae_df_2\n",
    "    allarcs = refdf.loc[row['oldid']]['arcs']\n",
    "    pathwords = []\n",
    "    pathwords = re.sub(r\"[^a-zA-Z0-9]+\", ' ',row['short_'+prunedletter] ).split()\n",
    "    affected = {}\n",
    "    ind = 0\n",
    "    #print(allarcs)\n",
    "    for arc in allarcs:\n",
    "        for p in pathwords:\n",
    "            if p in arc:\n",
    "                affected[arc] = int(row['dae_'+prunedletter][1+ind*3])\n",
    "                break\n",
    "        ind+=1\n",
    "    return affected\n",
    "    \n",
    "def check_hallucination(row):\n",
    "    pwordsa = re.sub(r\"[^a-zA-Z0-9]+\", ' ',row['short_a'] ).split()\n",
    "    pwordsb = re.sub(r\"[^a-zA-Z0-9]+\", ' ',row['short_b'] ).split()\n",
    "    halluc = ''\n",
    "    #print(pwordsb)\n",
    "    #print(row['doc'])\n",
    "    for a in pwordsa:\n",
    "        if a not in row['doc']:\n",
    "            halluc = halluc+'a'\n",
    "            break\n",
    "            \n",
    "    for b in pwordsb:\n",
    "        if b not in row['doc']:\n",
    "            halluc = halluc+'b'\n",
    "            break\n",
    "    return halluc\n",
    "    \n",
    "\n",
    "# go through table, see which paths would get pruned / left if \n",
    "def see_dae_pruned():\n",
    "    pruned = []\n",
    "    table['oldid'] = table['Unnamed: 0']\n",
    "    table['graphind'] = table.groupby('doc', sort=False).ngroup()\n",
    "    for index, row in table.iterrows():\n",
    "        acnt = (row['dae_a']).count('0')\n",
    "        bcnt = (row['dae_b']).count('0')\n",
    "        tmp = {}\n",
    "        tmp = row\n",
    "        if acnt < bcnt:\n",
    "            tmp['dae_pruned'] = 'b'\n",
    "        elif bcnt < acnt:\n",
    "            tmp['dae_pruned'] = 'a'\n",
    "        else:\n",
    "            tmp['dae_pruned'] = '0'\n",
    "            \n",
    "        tmp['halluc'] = check_hallucination(row)\n",
    "        #if tmp['dae_pruned'] == '0' and tmp['halluc'] == '':\n",
    "        #    continue\n",
    "        #if tmp['dae_pruned']=='a' or 'a' in tmp['halluc']:\n",
    "        tmp['arcs_a'] = affected_arcs(row, 'a')\n",
    "        #if tmp['dae_pruned']=='a' or 'a' in tmp['halluc']:\n",
    "        tmp['arcs_b'] = affected_arcs(row, 'b')\n",
    "        pruned.append(tmp)\n",
    "        \n",
    "    ret = pd.DataFrame(pruned)\n",
    "    ret.drop(columns=['Unnamed: 0'])\n",
    "    return ret\n",
    "pruned = see_dae_pruned()\n",
    "pruned = pruned.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_prune = pruned.loc[pruned['dae_pruned']!='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5789360736996795, 0.35498937174445727]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dict_avg (arcs):\n",
    "    total = 0\n",
    "    for a in arcs.keys():\n",
    "        total+=arcs[a]\n",
    "    length = len(list(arcs.keys()))\n",
    "    if length==0:\n",
    "        return -1\n",
    "    return float(total)/length\n",
    "\n",
    "def zero_cnt (arcs):\n",
    "    total = 0\n",
    "    for a in arcs.keys():\n",
    "        if arcs[a]==0:\n",
    "            total+=1\n",
    "    length = len(list(arcs.keys()))\n",
    "    if length==0:\n",
    "        return -1\n",
    "    return float(total)\n",
    "\n",
    "def get_dae_arc_average(row):\n",
    "    cur = row['dae_pruned']\n",
    "    other = 'a'\n",
    "    if cur =='a':\n",
    "        other = 'b'\n",
    "    curarcs = row['arcs_'+cur]\n",
    "    otherarcs = row['arcs_'+other]\n",
    "    return dict_avg(curarcs), dict_avg(otherarcs)\n",
    "\n",
    "def get_dae_arc_coverage(row):\n",
    "    cur = row['dae_pruned']\n",
    "    other = 'a'\n",
    "    if cur =='a':\n",
    "        other = 'b'\n",
    "    curarcs = row['arcs_'+cur]\n",
    "    otherarcs = row['arcs_'+other]\n",
    "    curcnt = row['dae_'+cur].count('0')\n",
    "    othcnt = row['dae_'+other].count('0')\n",
    "    if othcnt ==0:\n",
    "        return zero_cnt(curarcs)/curcnt, 0\n",
    "    return zero_cnt(curarcs)/curcnt, zero_cnt(otherarcs)/othcnt\n",
    "\n",
    "\n",
    "def get_all_arc_avgs(dae_df):\n",
    "    total = [0, 0]\n",
    "    cnt = 0\n",
    "    for index, row in dae_df.iterrows():\n",
    "        avgs = get_dae_arc_coverage(row)\n",
    "        if avgs[1]>=0:\n",
    "            cnt+=1\n",
    "            total[0]+=avgs[0]\n",
    "            total[1]+=avgs[1]\n",
    "    total[0] = float(total[0])/cnt\n",
    "    total[1] = float(total[1])/cnt\n",
    "    return total\n",
    "    \n",
    "get_all_arc_avgs(dae_prune)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pruned.loc[pruned['dae_pruned'] == '0']\n",
    "tmp.loc[tmp['halluc']==''].to_csv('no_prune.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>full_a</th>\n",
       "      <th>full_b</th>\n",
       "      <th>invalid</th>\n",
       "      <th>short_a</th>\n",
       "      <th>short_b</th>\n",
       "      <th>dae_a</th>\n",
       "      <th>dae_b</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>oldid</th>\n",
       "      <th>graphind</th>\n",
       "      <th>dae_pruned</th>\n",
       "      <th>halluc</th>\n",
       "      <th>arcs_a</th>\n",
       "      <th>arcs_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The body of the man in his 70s, was found at t...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>False</td>\n",
       "      <td>home</td>\n",
       "      <td>house</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.995133</td>\n",
       "      <td>0.995132</td>\n",
       "      <td>0.995152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>{'[CLS] nmod : at [SEP] home [SEP] dead [SEP]'...</td>\n",
       "      <td>{'[CLS] nmod : at [SEP] house [SEP] dead [SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The body of the man in his 70s, was found at t...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>False</td>\n",
       "      <td>stabbed to death</td>\n",
       "      <td>found dead</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.902390</td>\n",
       "      <td>0.931311</td>\n",
       "      <td>0.917078</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>ab</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] stabbed [SEP...</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] found [SEP]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The body of the man in his 70s, was found at t...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>False</td>\n",
       "      <td>following the death of a man</td>\n",
       "      <td>after a man was found dead</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.899946</td>\n",
       "      <td>0.900133</td>\n",
       "      <td>0.900441</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ab</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The body of the man in his 70s, was found at t...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>False</td>\n",
       "      <td>following the death of a man</td>\n",
       "      <td>after a man was stabbed to death</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.905021</td>\n",
       "      <td>0.888849</td>\n",
       "      <td>0.897318</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>ab</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The body of the man in his 70s, was found at t...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "      <td>False</td>\n",
       "      <td>stabbed</td>\n",
       "      <td>dead</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.942055</td>\n",
       "      <td>0.945409</td>\n",
       "      <td>0.943956</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>ab</td>\n",
       "      <td>{'[CLS] xcomp [SEP] stabbed [SEP] found [SEP]'...</td>\n",
       "      <td>{'[CLS] xcomp [SEP] dead [SEP] found [SEP]': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>Birmingham academies Park View and Nansen Prim...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>False</td>\n",
       "      <td>face losing their Ofsted funding.</td>\n",
       "      <td>have been told they face losing their Ofsted f...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.958564</td>\n",
       "      <td>0.836212</td>\n",
       "      <td>0.895914</td>\n",
       "      <td>5457</td>\n",
       "      <td>71</td>\n",
       "      <td>b</td>\n",
       "      <td>ab</td>\n",
       "      <td>{'[CLS] nsubj [SEP] schools [SEP] losing [SEP]...</td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] schools [SEP] told [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>Birmingham academies Park View and Nansen Prim...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>False</td>\n",
       "      <td>have been told they may</td>\n",
       "      <td>could</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.828208</td>\n",
       "      <td>0.953978</td>\n",
       "      <td>0.889514</td>\n",
       "      <td>5462</td>\n",
       "      <td>71</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] schools [SEP] told [SE...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5413</th>\n",
       "      <td>Birmingham academies Park View and Nansen Prim...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>False</td>\n",
       "      <td>have been told they may</td>\n",
       "      <td>will</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.937926</td>\n",
       "      <td>0.884949</td>\n",
       "      <td>5463</td>\n",
       "      <td>71</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] schools [SEP] told [SE...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5414</th>\n",
       "      <td>Birmingham academies Park View and Nansen Prim...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>False</td>\n",
       "      <td>have been told they may</td>\n",
       "      <td>are to</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.844853</td>\n",
       "      <td>0.923623</td>\n",
       "      <td>0.883907</td>\n",
       "      <td>5464</td>\n",
       "      <td>71</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>{'[CLS] nsubjpass [SEP] schools [SEP] told [SE...</td>\n",
       "      <td>{'[CLS] nsubj [SEP] schools [SEP] are [SEP]': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>Birmingham academies Park View and Nansen Prim...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>Two schools at the centre of the socalled Troj...</td>\n",
       "      <td>False</td>\n",
       "      <td>are to</td>\n",
       "      <td>have been told they may</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0.923642</td>\n",
       "      <td>0.844814</td>\n",
       "      <td>0.883907</td>\n",
       "      <td>5465</td>\n",
       "      <td>71</td>\n",
       "      <td>b</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4644 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    doc  \\\n",
       "0     The body of the man in his 70s, was found at t...   \n",
       "3     The body of the man in his 70s, was found at t...   \n",
       "4     The body of the man in his 70s, was found at t...   \n",
       "5     The body of the man in his 70s, was found at t...   \n",
       "6     The body of the man in his 70s, was found at t...   \n",
       "...                                                 ...   \n",
       "5407  Birmingham academies Park View and Nansen Prim...   \n",
       "5412  Birmingham academies Park View and Nansen Prim...   \n",
       "5413  Birmingham academies Park View and Nansen Prim...   \n",
       "5414  Birmingham academies Park View and Nansen Prim...   \n",
       "5415  Birmingham academies Park View and Nansen Prim...   \n",
       "\n",
       "                                                 full_a  \\\n",
       "0     A man has been arrested on suspicion of murder...   \n",
       "3     A man has been arrested on suspicion of murder...   \n",
       "4     A man has been arrested on suspicion of murder...   \n",
       "5     A man has been arrested on suspicion of murder...   \n",
       "6     A man has been arrested on suspicion of murder...   \n",
       "...                                                 ...   \n",
       "5407  Two schools at the centre of the socalled Troj...   \n",
       "5412  Two schools at the centre of the socalled Troj...   \n",
       "5413  Two schools at the centre of the socalled Troj...   \n",
       "5414  Two schools at the centre of the socalled Troj...   \n",
       "5415  Two schools at the centre of the socalled Troj...   \n",
       "\n",
       "                                                 full_b  invalid  \\\n",
       "0     A man has been arrested on suspicion of murder...    False   \n",
       "3     A man has been arrested on suspicion of murder...    False   \n",
       "4     A man has been arrested on suspicion of murder...    False   \n",
       "5     A man has been arrested on suspicion of murder...    False   \n",
       "6     A man has been arrested on suspicion of murder...    False   \n",
       "...                                                 ...      ...   \n",
       "5407  Two schools at the centre of the socalled Troj...    False   \n",
       "5412  Two schools at the centre of the socalled Troj...    False   \n",
       "5413  Two schools at the centre of the socalled Troj...    False   \n",
       "5414  Two schools at the centre of the socalled Troj...    False   \n",
       "5415  Two schools at the centre of the socalled Troj...    False   \n",
       "\n",
       "                                short_a  \\\n",
       "0                                  home   \n",
       "3                      stabbed to death   \n",
       "4          following the death of a man   \n",
       "5          following the death of a man   \n",
       "6                               stabbed   \n",
       "...                                 ...   \n",
       "5407  face losing their Ofsted funding.   \n",
       "5412            have been told they may   \n",
       "5413            have been told they may   \n",
       "5414            have been told they may   \n",
       "5415                             are to   \n",
       "\n",
       "                                                short_b  \\\n",
       "0                                                 house   \n",
       "3                                            found dead   \n",
       "4                            after a man was found dead   \n",
       "5                      after a man was stabbed to death   \n",
       "6                                                  dead   \n",
       "...                                                 ...   \n",
       "5407  have been told they face losing their Ofsted f...   \n",
       "5412                                              could   \n",
       "5413                                               will   \n",
       "5414                                             are to   \n",
       "5415                            have been told they may   \n",
       "\n",
       "                                     dae_a  \\\n",
       "0                 [0, 0, 1, 1, 1, 1, 1, 1]   \n",
       "3                 [0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "4                    [0, 0, 1, 1, 1, 1, 1]   \n",
       "5                    [0, 0, 1, 1, 1, 1, 1]   \n",
       "6                 [0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "...                                    ...   \n",
       "5407     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5412  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5413  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5414  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5415  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                        dae_b  precision    recall        f1  \\\n",
       "0                    [0, 0, 1, 1, 1, 1, 1, 1]   0.995133  0.995132  0.995152   \n",
       "3                    [0, 0, 1, 1, 1, 1, 1, 1]   0.902390  0.931311  0.917078   \n",
       "4                    [0, 0, 1, 1, 1, 1, 1, 1]   0.899946  0.900133  0.900441   \n",
       "5                    [0, 0, 1, 0, 0, 0, 0, 0]   0.905021  0.888849  0.897318   \n",
       "6                    [0, 0, 1, 1, 1, 1, 1, 1]   0.942055  0.945409  0.943956   \n",
       "...                                       ...        ...       ...       ...   \n",
       "5407  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]   0.958564  0.836212  0.895914   \n",
       "5412           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   0.828208  0.953978  0.889514   \n",
       "5413           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   0.833829  0.937926  0.884949   \n",
       "5414     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   0.844853  0.923623  0.883907   \n",
       "5415     [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]   0.923642  0.844814  0.883907   \n",
       "\n",
       "      oldid  graphind dae_pruned halluc  \\\n",
       "0         0         0          0      a   \n",
       "3         3         0          a     ab   \n",
       "4         4         0          0     ab   \n",
       "5         5         0          b     ab   \n",
       "6         6         0          a     ab   \n",
       "...     ...       ...        ...    ...   \n",
       "5407   5457        71          b     ab   \n",
       "5412   5462        71          a          \n",
       "5413   5463        71          a          \n",
       "5414   5464        71          a          \n",
       "5415   5465        71          b          \n",
       "\n",
       "                                                 arcs_a  \\\n",
       "0     {'[CLS] nmod : at [SEP] home [SEP] dead [SEP]'...   \n",
       "3     {'[CLS] nsubjpass [SEP] man [SEP] stabbed [SEP...   \n",
       "4     {'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...   \n",
       "5     {'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...   \n",
       "6     {'[CLS] xcomp [SEP] stabbed [SEP] found [SEP]'...   \n",
       "...                                                 ...   \n",
       "5407  {'[CLS] nsubj [SEP] schools [SEP] losing [SEP]...   \n",
       "5412  {'[CLS] nsubjpass [SEP] schools [SEP] told [SE...   \n",
       "5413  {'[CLS] nsubjpass [SEP] schools [SEP] told [SE...   \n",
       "5414  {'[CLS] nsubjpass [SEP] schools [SEP] told [SE...   \n",
       "5415                                                NaN   \n",
       "\n",
       "                                                 arcs_b  \n",
       "0     {'[CLS] nmod : at [SEP] house [SEP] dead [SEP]...  \n",
       "3     {'[CLS] nsubjpass [SEP] man [SEP] found [SEP]'...  \n",
       "4     {'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...  \n",
       "5     {'[CLS] nsubjpass [SEP] man [SEP] arrested [SE...  \n",
       "6     {'[CLS] xcomp [SEP] dead [SEP] found [SEP]': 1...  \n",
       "...                                                 ...  \n",
       "5407  {'[CLS] nsubjpass [SEP] schools [SEP] told [SE...  \n",
       "5412                                                 {}  \n",
       "5413                                                 {}  \n",
       "5414  {'[CLS] nsubj [SEP] schools [SEP] are [SEP]': ...  \n",
       "5415                                                NaN  \n",
       "\n",
       "[4644 rows x 17 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f77846b0243d2dee26bbaa6fd0a0b34a7adea800a5063b4b91f2f98ac96800"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
