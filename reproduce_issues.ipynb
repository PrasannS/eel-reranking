{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0727f9cc-c1ca-4442-ba0c-def3f55a5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 06:50:44.727927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-24 06:50:44.727950: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from encode_utils.rerank_data import rerank_dist, rerank_single\n",
    "from encode_utils.efficient_rerank import get_effrerank_model, run_comstyle\n",
    "from encode_utils.sco_funct import weightaddprob, default_scofunct\n",
    "from encode_utils.mt_scores import get_scores_auto\n",
    "from encode_utils.new_flatten_lattice import get_dictlist\n",
    "from encode_utils.new_mask_utils import randomsingle, useall\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47014fe6-5fbb-4937-8a40-0672f03397b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Loading weights from /mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_44/checkpoints/epoch=9-step=40000.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze embeds\n"
     ]
    }
   ],
   "source": [
    "# setup / load model\n",
    "encodemod = get_effrerank_model(\"noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c306d18-ad31-4cc6-9d0c-913ecc68aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de7e1e2-2cd9-4b4c-8265-34b9664da198",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"outputs/graph_pickles/frtest_reversed/\"\n",
    "noun_explode = pd.read_csv(\"outputs/score_csvs/nounlargeexplodev1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5433d3e-cab4-488f-8e5e-185d81a1433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #3 - there's a difference between gold selected with standard re-rank and with EEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735866d8-d60e-4caa-99e0-98c6b1e9cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model\n",
    "def get_hyp_sco(inphyp, posids=None):\n",
    "    \n",
    "    tokens = xlm_tok(inphyp, return_tensors='pt').to(device)\n",
    "    tokens = tokens.input_ids\n",
    "    if posids is None: \n",
    "        positionids = None\n",
    "    else:\n",
    "        # get token at the end\n",
    "        positionids = torch.tensor(posids+[posids[-1]+1]).unsqueeze(0).to(device)\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(device)\n",
    "\n",
    "    toked_inp = xlm_tok([\"noun\"], return_tensors=\"pt\").to(device)\n",
    "    predout = encodemod(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    tmppred = predout['score']\n",
    "    #norm = predout['norm']\n",
    "    return tmppred\n",
    "\n",
    "# TODO do a validation that old score generation way and current have same bests\n",
    "def get_ind_result(ind):\n",
    "    graph = pickle.load(open(base+str(ind), 'rb'))\n",
    "    texplode = noun_explode[noun_explode['ref']==graph['ref']].reset_index()\n",
    "    # recalculate noun scores for all\n",
    "    cscos = []\n",
    "    for t in list(texplode['hyp']):\n",
    "        hs = get_hyp_sco(t)\n",
    "        cscos.append(torch.sum(get_hyp_sco(t)))#*(hs.shape[1]-1))\n",
    "    print(\"scodist - \", [float(f) for f in cscos])\n",
    "    print(\"max - \", float(max(cscos)))\n",
    "    bestpath , flattened, pnodes, mask, sents, posids, pred, _, \\\n",
    "            flnodes, dpath, beplist, besclist, totnodes, bsco = run_comstyle(graph, encodemod, default_scofunct, \"noun\", {'afunc':randomsingle}, True)\n",
    "    predhyp = bestpath[0][4:]\n",
    "    ph = get_hyp_sco(predhyp)\n",
    "    predsco = torch.sum(ph)#*(ph.shape[1]-1)\n",
    "    print(\"pred - \", float(predsco))\n",
    "    print(\"predhyp\")\n",
    "    return mask, sents, posids, pred, list(texplode['hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4274abc8-b780-4f9d-86e0-8dfef514e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scodist -  [0.37691840529441833, 0.3711412250995636, 0.7022441625595093, 0.33818960189819336, 0.3423137068748474, 0.3410169780254364, 0.674796462059021, 0.342754065990448, 0.6870383024215698, 0.6777890920639038, 0.36100950837135315, 0.3775254487991333]\n",
      "max -  0.7022441625595093\n",
      "pred -  0.7022441625595093\n",
      "predhyp\n"
     ]
    }
   ],
   "source": [
    "msk, snts, pids, scos, hyps = get_ind_result(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e79d750-ebbf-48e2-b0ff-a127eb21fd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To some extent, China is going to face obstacles on its way, and there is no guarantee that its leaders will admit their failures or adjust their strategy accordingly.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53daf3a9-d173-46a9-a2a5-cc55e680acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posmat = pids.expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "posver = posmat*msk\n",
    "inpmat = snts.expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "inpmat = inpmat*(msk>0)\n",
    "scomat = scos.squeeze(-1).expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "scomat = scomat*(msk>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d956f917-a94b-4f0b-bb4e-90ae61602b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28, 29, 30, 31, 32, 33, 34, 35, 36, 37,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "         30, 31, 32, 33, 34, 35, 36, 37, 38, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         38,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "         33, 34, 35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "         29, 30, 31, 32, 33, 34, 35, 36,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "         35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2]], device='cuda:2')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02d75e18-0e8e-431d-ab2a-11f6a9ea3d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0315, 0.0320, 0.0334, 0.3809, 0.0520, 0.9946, 0.2352, 0.2196,\n",
       "        0.1561, 1.0815, 0.3335, 0.4159, 0.3077, 1.5724, 0.1403, 0.4810, 0.4012,\n",
       "        0.4049, 0.5259, 2.2846, 0.6981, 0.4920, 2.6411, 0.7156, 0.7587, 0.6133,\n",
       "        3.0344, 0.8132, 0.7931, 1.2603, 0.4955, 3.8419, 1.0233, 0.5695, 0.3316,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scomat[35][:37]*torch.max(pids).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd1ce40c-25b8-4c55-a735-29010a969236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 37, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_hyp_sco(hyps[11])*36).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212fd249-77e2-4108-bc74-06f7f0b503d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> To a certain extent, China will encounter obstacles on its way, and there is no guarantee that its leaders will admit their failures or adjust their strategy accordingly.<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm_tok.decode(inpmat[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81430fcf-e53e-4b44-a8c9-99a00e0c4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(posver)):\n",
    "    for j in range(1, len(posver[0])):\n",
    "        val = posver[i][j]\n",
    "        if val == 0:\n",
    "            continue\n",
    "        if val <= posver[i][j-1]:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28ad7837-8a82-4bf2-aa03-4dc7d53271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask has no recomb\n",
    "assert torch.sum(msk!=torch.tril(msk))==0\n",
    "# no weird posids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b752eeb0-9cad-499b-9cab-66f292629cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #2 - graph may be getting corrupted during the process of conversion, make sure identical exploded even after conversion\n",
    "mb_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "\n",
    "# recursive method to sanity check first (assume no recomb, no cycles)\n",
    "def explode_path(ind, processed=False):\n",
    "    graph = pickle.load(open(base+str(ind), 'rb'))\n",
    "    allpaths = []\n",
    "    if processed:\n",
    "        fldict, flnodes = get_dictlist(graph, True)\n",
    "        explode_helper([], flnodes[0], allpaths, xlm_tok)\n",
    "    else:\n",
    "        explode_helper([], graph['root'], allpaths, mb_tok)\n",
    "        allpaths = [a[10:] for a in allpaths]\n",
    "    nexplode = noun_explode[noun_explode['ref']==graph['ref']].reset_index()\n",
    "    return allpaths, list(nexplode['hyp'])\n",
    "\n",
    "# helper for exploding paths\n",
    "def explode_helper(prevpath, node, apaths, tok):\n",
    "    prevpath.append(node.token_idx)\n",
    "    if len(node.nextlist)==0:\n",
    "        #print(prevpath)\n",
    "        apaths.append(mb_tok.decode(prevpath))\n",
    "    else:\n",
    "        for n in node.nextlist:\n",
    "            explode_helper(prevpath, n, apaths, tok)\n",
    "    prevpath.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf817f12-1688-4f8d-b7ce-d708faa47479",
   "metadata": {},
   "outputs": [],
   "source": [
    "apths = []\n",
    "explode_helper([], pnodes[0][0], apths, xlm_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d9a91-b08d-4cfa-b83f-d85be7f0e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #1 - changing pos-ids with identical strings changes token scores for all things\n",
    "# score check (at token level) the best option, and the new option (should be same until they get to non-identical tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5c399b-daa4-45e6-a25a-08c45e05be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model\n",
    "def get_hyp_sco_verb(inphyp, posids=None):\n",
    "    \n",
    "    tokens = xlm_tok(inphyp, return_tensors='pt').to(device)\n",
    "    tokens = tokens.input_ids\n",
    "\n",
    "    if posids is None: \n",
    "        # have to add 2 because of how xlm roberta works\n",
    "        positionids = torch.arange(len(tokens[0])).unsqueeze(0).to(device)+2\n",
    "        print(positionids)\n",
    "\n",
    "    elif -1 in posids:\n",
    "        positionids = None\n",
    "    else:\n",
    "        # get token at the end\n",
    "        positionids = posids\n",
    "        #torch.tensor(posids+[posids[-1]+1]).unsqueeze(0).to(device)\n",
    "        print(positionids)\n",
    "\n",
    "    # get a causal mask to use\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(device)\n",
    "    \n",
    "    # this is a dummy string (using CQE model architecture which requires an \"input\" pass)\n",
    "    toked_inp = xlm_tok([\"noun\"], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # run inputs through the model\n",
    "    predout = encodemod(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    \n",
    "    tmppred = predout['score']\n",
    "    return tmppred, tokens, positionids, tmpmask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e03b8a07-e363-48a2-bf84-ebefdf889adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4, 5, 6, 7]], device='cuda:2')\n",
      "tensor([[2, 3, 4, 5, 7, 7]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# get scores for identical strings, with different posids\n",
    "sco_default = get_hyp_sco_verb(\"I am a bat\")\n",
    "modpos = sco_default[2]\n",
    "# change last position id\n",
    "modpos[0][4] = 7\n",
    "sco_mod = get_hyp_sco_verb(\"I am a bat\", modpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e293d8eb-9073-419a-ac72-cec7699bd33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000],\n",
      "         [0.0042],\n",
      "         [0.0042],\n",
      "         [0.0043],\n",
      "         [0.0508],\n",
      "         [0.0214]]], device='cuda:2', grad_fn=<DivBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0042],\n",
      "         [0.0042],\n",
      "         [0.0043],\n",
      "         [0.0557],\n",
      "         [0.0211]]], device='cuda:2', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# these two are actually fine. There is some sort of issue in no-recomb cases\n",
    "print(sco_default[0])\n",
    "print(sco_mod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30eb11-0386-4c83-9627-7f40977c07f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
