{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727f9cc-c1ca-4442-ba0c-def3f55a5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from encode_utils.rerank_data import rerank_dist, rerank_single\n",
    "from encode_utils.efficient_rerank import get_effrerank_model, run_comstyle\n",
    "from encode_utils.sco_funct import weightaddprob, default_scofunct\n",
    "from encode_utils.mt_scores import get_scores_auto\n",
    "from encode_utils.new_flatten_lattice import get_dictlist\n",
    "from encode_utils.new_mask_utils import randomsingle, useall\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from generate_tables import metrics_mapping\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc70b9f8-b516-4be2-bdc6-234e4531281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE #5 - Tokenization Splitting not Happening Well With X-Sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bfc3f5-5e1a-4acb-b885-03c421ac3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsumcands = pd.read_csv(\"outputs/score_csvs/nounxsumbeam12v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce52f4f3-a309-4eaa-a0f5-d30ea501a080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deutsche Bank shares fell as much as 5% after reports the bank has failed to reach an agreement with US authorities over a $14bn (£11bn) fine.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsumcands['hyp'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a01a2c83-bac9-47d2-b1e0-010b6b68f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do this for everything\n",
    "mtb12 = pd.read_csv(\"outputs/predcsvs/noun_comstyle_xsum_randsing_8shot.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d621c35-ea51-4898-9c90-cc43e398440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testref = mtb12['ref'][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "041a57b1-bb4a-4cbc-a7f8-047b73ca394d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Plymouth Ar gyle midfielder Robbie Donald son has compared their League Two promotion push to Newcastle United's 2011 title success.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtb12['hyp'][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6f4f69d-828e-4c47-b0f0-decd62d45c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "bartsum_tok = AutoTokenizer.from_pretrained(\"facebook/bart-large-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e9b6d9d-22eb-4bf2-98b7-b49737c77473",
   "metadata": {},
   "outputs": [],
   "source": [
    "minis = bartsum_tok(testref).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ef24429-bcd1-4abc-9a07-0b21d0998a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plymouth Argyle midfielder Ryan Donaldson says League Two teams see the Pilgrims as a major scalp.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08baed8e-9a16-4d97-b108-fa94da818dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>P\n",
      "Plymouth Ar\n",
      "Argyle midfielder\n",
      "midfielder Ryan\n",
      "Ryan Donald\n",
      "Donaldson says\n",
      "says League\n",
      "League Two\n",
      "Two teams\n",
      "teams see\n",
      "see the\n",
      "the Pil\n",
      "Pilgrims as\n",
      "as a\n",
      "a major\n",
      "major scalp\n",
      "scalp.</s>\n"
     ]
    }
   ],
   "source": [
    "toks = [[m] for m in minis]\n",
    "words = []\n",
    "for i in range(1, len(toks)):\n",
    "    dec = bartsum_tok.decode(toks[i-1]+ toks[i]).strip()\n",
    "    #\n",
    "    if \" \" in dec or \"s>\" in dec:\n",
    "        print(dec)\n",
    "        words.append(bartsum_tok.decode(toks[i]))\n",
    "    else:\n",
    "        words[-1] = dec\n",
    "        toks[i] = toks[i-1]+ toks[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "63b55e4c-d8a2-49ac-b6a7-e955c2637ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[436, 538, 111443],\n",
       " [1172, 3432, 133],\n",
       " [4122, 28394, 56],\n",
       " [78201],\n",
       " [16692, 1681],\n",
       " [17378],\n",
       " [19175],\n",
       " [32964],\n",
       " [87199],\n",
       " [1957],\n",
       " [70],\n",
       " [13289, 113240, 7],\n",
       " [237],\n",
       " [10],\n",
       " [13036],\n",
       " [117906, 254, 5],\n",
       " [2]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f8e33891-8607-4c92-aab0-5a3d6f619131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'ly', 'mouth']\n",
      "['Ar', 'gy', 'le']\n",
      "['mid', 'field', 'er']\n",
      "['Ryan']\n",
      "['Donald', 'son']\n",
      "['says']\n",
      "['League']\n",
      "['Two']\n",
      "['teams']\n",
      "['see']\n",
      "['the']\n",
      "['Pil', 'grim', 's']\n",
      "['as']\n",
      "['a']\n",
      "['major']\n",
      "['scal', 'p', '.']\n",
      "['</s>']\n"
     ]
    }
   ],
   "source": [
    "for x in xtokens:\n",
    "    print([xlm_tok.decode(l) for l in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3784731-c523-427e-aff9-15568189bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtokens = []\n",
    "for w in words:\n",
    "    xtokens.append(xlm_tok(w).input_ids[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47014fe6-5fbb-4937-8a40-0672f03397b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Loading weights from /mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_43/checkpoints/epoch=3-step=130000.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze embeds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encodemod \u001b[38;5;241m=\u001b[39m \u001b[43mget_effrerank_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomstyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m xlm_tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m argsinp \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok\u001b[39m\u001b[38;5;124m'\u001b[39m:xlm_tok,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:encodemod,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:device\n\u001b[1;32m      7\u001b[0m }\n",
      "File \u001b[0;32m/mnt/data1/prasann/latticegen/lattice-generation/encode_utils/efficient_rerank.py:224\u001b[0m, in \u001b[0;36mget_effrerank_model\u001b[0;34m(keystr)\u001b[0m\n\u001b[1;32m    221\u001b[0m reflessmod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keystr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# TODO update to most updated model at a given time\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     reflessmod \u001b[38;5;241m=\u001b[39m \u001b[43mlfc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_43/checkpoints/epoch=3-step=130000.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m keystr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomnocause\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    226\u001b[0m     reflessmod \u001b[38;5;241m=\u001b[39m lfc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_38/checkpoints/epoch=3-step=140000.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:121\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 570 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/latclone/lib/python3.8/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encodemod = get_effrerank_model(\"comstyle\")\n",
    "xlm_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "argsinp = {\n",
    "    'tok':xlm_tok,\n",
    "    'model':encodemod,\n",
    "    'device':device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04cb392-ea97-4f60-8f70-426d98f5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model, given hypothesis and input source\n",
    "def get_hyp_sco(inphyp, inpsrc, args):\n",
    "    tok = args['tok']\n",
    "    dev = args['device']\n",
    "    model = args['model']\n",
    "\n",
    "    # calculate inputs\n",
    "    tokens = tok(inphyp, return_tensors='pt', truncation=True).to(dev)\n",
    "    tokens = tokens.input_ids\n",
    "    positionids = None\n",
    "    toked_inp = tok(inpsrc, return_tensors=\"pt\").to(dev)\n",
    "    # get causal mask\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(dev)\n",
    "    # run through model\n",
    "    predout = model(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    return predout['score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44a59ebd-2ece-46af-8330-bc15fd27bcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6200, device='cuda:1', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(get_hyp_sco(inex['hyp'], inex['src'], argsinp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd2feb-c894-4973-9c03-44aaf6734688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #4 - there's a difference between original generation and new generation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd6f5d6-4aa7-4571-aff4-414eccd127fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inex = mtb12.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aec3a68-8281-46fb-ba9f-547fd8cee4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpset = mtb12.loc[:3]\n",
    "tmpset = tmpset.rename(columns={'dupcqe':\"dco2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf1a707-f17a-42a8-9559-e6d55bf8935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>hyp</th>\n",
       "      <th>dcqeold</th>\n",
       "      <th>comet</th>\n",
       "      <th>cqe</th>\n",
       "      <th>posthoc</th>\n",
       "      <th>dco2</th>\n",
       "      <th>dupcqe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comme pour tout compromis, les parties prenant...</td>\n",
       "      <td>As with any compromise, the contending parties...</td>\n",
       "      <td>As with any compromise, stakeholders would hav...</td>\n",
       "      <td>0.672990</td>\n",
       "      <td>0.776076</td>\n",
       "      <td>0.526303</td>\n",
       "      <td>0.390460</td>\n",
       "      <td>0.640510</td>\n",
       "      <td>0.640568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comme pour tout compromis, les parties prenant...</td>\n",
       "      <td>As with any compromise, the contending parties...</td>\n",
       "      <td>As with any compromise, stakeholders would hav...</td>\n",
       "      <td>0.664692</td>\n",
       "      <td>0.766729</td>\n",
       "      <td>0.548467</td>\n",
       "      <td>0.393152</td>\n",
       "      <td>0.635732</td>\n",
       "      <td>0.635743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comme pour tout compromis, les parties prenant...</td>\n",
       "      <td>As with any compromise, the contending parties...</td>\n",
       "      <td>As with any compromise, stakeholders would hav...</td>\n",
       "      <td>0.636959</td>\n",
       "      <td>0.759802</td>\n",
       "      <td>0.545454</td>\n",
       "      <td>0.405204</td>\n",
       "      <td>0.610568</td>\n",
       "      <td>0.610579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comme pour tout compromis, les parties prenant...</td>\n",
       "      <td>As with any compromise, the contending parties...</td>\n",
       "      <td>As with any compromise, the stakeholders would...</td>\n",
       "      <td>0.667578</td>\n",
       "      <td>0.771617</td>\n",
       "      <td>0.517116</td>\n",
       "      <td>0.406245</td>\n",
       "      <td>0.643053</td>\n",
       "      <td>0.643061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  Comme pour tout compromis, les parties prenant...   \n",
       "1  Comme pour tout compromis, les parties prenant...   \n",
       "2  Comme pour tout compromis, les parties prenant...   \n",
       "3  Comme pour tout compromis, les parties prenant...   \n",
       "\n",
       "                                                 ref  \\\n",
       "0  As with any compromise, the contending parties...   \n",
       "1  As with any compromise, the contending parties...   \n",
       "2  As with any compromise, the contending parties...   \n",
       "3  As with any compromise, the contending parties...   \n",
       "\n",
       "                                                 hyp   dcqeold     comet  \\\n",
       "0  As with any compromise, stakeholders would hav...  0.672990  0.776076   \n",
       "1  As with any compromise, stakeholders would hav...  0.664692  0.766729   \n",
       "2  As with any compromise, stakeholders would hav...  0.636959  0.759802   \n",
       "3  As with any compromise, the stakeholders would...  0.667578  0.771617   \n",
       "\n",
       "        cqe   posthoc      dco2    dupcqe  \n",
       "0  0.526303  0.390460  0.640510  0.640568  \n",
       "1  0.548467  0.393152  0.635732  0.635743  \n",
       "2  0.545454  0.405204  0.610568  0.610579  \n",
       "3  0.517116  0.406245  0.643053  0.643061  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f23fae-69ac-4e26-bc73-07fba0e94d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Loading weights from /mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_43/checkpoints/epoch=3-step=130000.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Predicting DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOK TIME  4.18\n"
     ]
    }
   ],
   "source": [
    "metrics_mapping(\"dupcqe\", tmpset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de7e1e2-2cd9-4b4c-8265-34b9664da198",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"outputs/graph_pickles/frtest_reversed/\"\n",
    "noun_explode = pd.read_csv(\"outputs/score_csvs/nounlargeexplodev1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c47f805-d3e0-4aa0-9270-b2bba448a3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b22710-4c86-4300-9298-3eb95cb6460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallset = noun_explode.loc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92dac8f-03fe-4d62-8b56-1342e1a9edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4129421/615870310.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  smallset['utnounold'] = smallset['utnoun']\n"
     ]
    }
   ],
   "source": [
    "smallset['utnounold'] = smallset['utnoun']\n",
    "smallset = smallset.drop(columns=['utnoun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8092efb3-e03c-4e2e-b0ba-0b0f5179176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Loading weights from /mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_44/checkpoints/epoch=9-step=40000.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Predicting DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28],\n",
      "        [29],\n",
      "        [28],\n",
      "        [29]], device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOK TIME  3.75\n"
     ]
    }
   ],
   "source": [
    "metrics_mapping('utnoun', smallset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5927f07e-dc15-4a0d-a4c4-5a6885e3fb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>hyp</th>\n",
       "      <th>unique_nouns</th>\n",
       "      <th>utnounold</th>\n",
       "      <th>utnoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Après tout le Soudan est de la taille de l'Eur...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.702689</td>\n",
       "      <td>0.740255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Après tout le Soudan est de la taille de l'Eur...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>After all, the Sudan is the size of Western Eu...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.721806</td>\n",
       "      <td>0.759443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Après tout le Soudan est de la taille de l'Eur...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.688667</td>\n",
       "      <td>0.726136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Après tout le Soudan est de la taille de l'Eur...</td>\n",
       "      <td>After all, Sudan is the size of Western Europe...</td>\n",
       "      <td>After all, the Sudan is the size of Western Eu...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.708893</td>\n",
       "      <td>0.746611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                src  \\\n",
       "0           0  Après tout le Soudan est de la taille de l'Eur...   \n",
       "1           1  Après tout le Soudan est de la taille de l'Eur...   \n",
       "2           2  Après tout le Soudan est de la taille de l'Eur...   \n",
       "3           3  Après tout le Soudan est de la taille de l'Eur...   \n",
       "\n",
       "                                                 ref  \\\n",
       "0  After all, Sudan is the size of Western Europe...   \n",
       "1  After all, Sudan is the size of Western Europe...   \n",
       "2  After all, Sudan is the size of Western Europe...   \n",
       "3  After all, Sudan is the size of Western Europe...   \n",
       "\n",
       "                                                 hyp  unique_nouns  utnounold  \\\n",
       "0  After all, Sudan is the size of Western Europe...             7   0.702689   \n",
       "1  After all, the Sudan is the size of Western Eu...             7   0.721806   \n",
       "2  After all, Sudan is the size of Western Europe...             7   0.688667   \n",
       "3  After all, the Sudan is the size of Western Eu...             7   0.708893   \n",
       "\n",
       "     utnoun  \n",
       "0  0.740255  \n",
       "1  0.759443  \n",
       "2  0.726136  \n",
       "3  0.746611  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e1b603-1e60-47b0-9d09-9dd80c37daf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25, device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7594, device='cuda:1', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(get_hyp_sco(smallset['hyp'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5433d3e-cab4-488f-8e5e-185d81a1433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #3 - there's a difference between gold selected with standard re-rank and with EEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d2c041-f27e-4fd1-adf7-03827fd3c811",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xlm_tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     predout \u001b[38;5;241m=\u001b[39m model(toked_inp\u001b[38;5;241m.\u001b[39minput_ids, toked_inp\u001b[38;5;241m.\u001b[39mattention_mask, tokens, positionids, \\\n\u001b[1;32m     16\u001b[0m         tmpmask)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m argsinp \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mxlm_tok\u001b[49m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:encodemod,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:device\n\u001b[1;32m     23\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xlm_tok' is not defined"
     ]
    }
   ],
   "source": [
    "# get token level scores from model, given hypothesis and input source\n",
    "def get_hyp_sco(inphyp, inpsrc, args):\n",
    "    tok = args['tok']\n",
    "    dev = args['device']\n",
    "    model = args['model']\n",
    "\n",
    "    # calculate inputs\n",
    "    tokens = tok(inphyp, return_tensors='pt', truncation=True).to(dev)\n",
    "    tokens = tokens.input_ids\n",
    "    positionids = None\n",
    "    toked_inp = tok([inpsrc], return_tensors=\"pt\").to(dev)\n",
    "    # get causal mask\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(dev)\n",
    "    # run through model\n",
    "    predout = model(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    return predout['score']\n",
    "\n",
    "argsinp = {\n",
    "    'tok':xlm_tok,\n",
    "    'model':encodemod,\n",
    "    'device':device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735866d8-d60e-4caa-99e0-98c6b1e9cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model\n",
    "def get_hyp_sco(inphyp, posids=None):\n",
    "    \n",
    "    tokens = xlm_tok(inphyp, return_tensors='pt').to(device)\n",
    "    tokens = tokens.input_ids\n",
    "    if posids is None: \n",
    "        positionids = None\n",
    "    else:\n",
    "        # get token at the end\n",
    "        positionids = torch.tensor(posids+[posids[-1]+1]).unsqueeze(0).to(device)\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(device)\n",
    "\n",
    "    toked_inp = xlm_tok([\"noun\"], return_tensors=\"pt\").to(device)\n",
    "    predout = encodemod(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    tmppred = predout['score']\n",
    "    #norm = predout['norm']\n",
    "    return tmppred\n",
    "\n",
    "# TODO do a validation that old score generation way and current have same bests\n",
    "def get_ind_result(ind):\n",
    "    graph = pickle.load(open(base+str(ind), 'rb'))\n",
    "    texplode = noun_explode[noun_explode['ref']==graph['ref']].reset_index()\n",
    "    # recalculate noun scores for all\n",
    "    cscos = []\n",
    "    for t in list(texplode['hyp']):\n",
    "        hs = get_hyp_sco(t)\n",
    "        cscos.append(torch.sum(get_hyp_sco(t)))#*(hs.shape[1]-1))\n",
    "    print(\"scodist - \", [float(f) for f in cscos])\n",
    "    print(\"max - \", float(max(cscos)))\n",
    "    bestpath , flattened, pnodes, mask, sents, posids, pred, _, \\\n",
    "            flnodes, dpath, beplist, besclist, totnodes, bsco = run_comstyle(graph, encodemod, default_scofunct, \"noun\", {'afunc':randomsingle}, True)\n",
    "    predhyp = bestpath[0][4:]\n",
    "    ph = get_hyp_sco(predhyp)\n",
    "    predsco = torch.sum(ph)#*(ph.shape[1]-1)\n",
    "    print(\"pred - \", float(predsco))\n",
    "    print(\"predhyp\")\n",
    "    return mask, sents, posids, pred, list(texplode['hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274abc8-b780-4f9d-86e0-8dfef514e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk, snts, pids, scos, hyps = get_ind_result(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e79d750-ebbf-48e2-b0ff-a127eb21fd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To some extent, China is going to face obstacles on its way, and there is no guarantee that its leaders will admit their failures or adjust their strategy accordingly.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53daf3a9-d173-46a9-a2a5-cc55e680acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posmat = pids.expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "posver = posmat*msk\n",
    "inpmat = snts.expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "inpmat = inpmat*(msk>0)\n",
    "scomat = scos.squeeze(-1).expand(len(snts[0]), len(snts[0])).to(\"cpu\")\n",
    "scomat = scomat*(msk>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d956f917-a94b-4f0b-bb4e-90ae61602b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28, 29, 30, 31, 32, 33, 34, 35, 36, 37,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "         30, 31, 32, 33, 34, 35, 36, 37, 38, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         38,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "         33, 34, 35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "         29, 30, 31, 32, 33, 34, 35, 36,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "         35, 36, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2]], device='cuda:1')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02d75e18-0e8e-431d-ab2a-11f6a9ea3d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0315, 0.0320, 0.0334, 0.3809, 0.0520, 0.9946, 0.2352, 0.2196,\n",
       "        0.1561, 1.0815, 0.3335, 0.4159, 0.3077, 1.5724, 0.1403, 0.4810, 0.4012,\n",
       "        0.4049, 0.5259, 2.2846, 0.6981, 0.4920, 2.6411, 0.7156, 0.7587, 0.6133,\n",
       "        3.0344, 0.8132, 0.7931, 1.2603, 0.4955, 3.8419, 1.0233, 0.5695, 0.3316,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scomat[35][:37]*torch.max(pids).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd1ce40c-25b8-4c55-a735-29010a969236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 37, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_hyp_sco(hyps[11])*36).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212fd249-77e2-4108-bc74-06f7f0b503d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> To a certain extent, China will encounter obstacles on its way, and there is no guarantee that its leaders will admit their failures or adjust their strategy accordingly.<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm_tok.decode(inpmat[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81430fcf-e53e-4b44-a8c9-99a00e0c4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(posver)):\n",
    "    for j in range(1, len(posver[0])):\n",
    "        val = posver[i][j]\n",
    "        if val == 0:\n",
    "            continue\n",
    "        if val <= posver[i][j-1]:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28ad7837-8a82-4bf2-aa03-4dc7d53271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask has no recomb\n",
    "assert torch.sum(msk!=torch.tril(msk))==0\n",
    "# no weird posids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b752eeb0-9cad-499b-9cab-66f292629cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #2 - graph may be getting corrupted during the process of conversion, make sure identical exploded even after conversion\n",
    "mb_tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "\n",
    "# recursive method to sanity check first (assume no recomb, no cycles)\n",
    "def explode_path(ind, processed=False):\n",
    "    graph = pickle.load(open(base+str(ind), 'rb'))\n",
    "    allpaths = []\n",
    "    if processed:\n",
    "        fldict, flnodes = get_dictlist(graph, True)\n",
    "        explode_helper([], flnodes[0], allpaths, xlm_tok)\n",
    "    else:\n",
    "        explode_helper([], graph['root'], allpaths, mb_tok)\n",
    "        allpaths = [a[10:] for a in allpaths]\n",
    "    nexplode = noun_explode[noun_explode['ref']==graph['ref']].reset_index()\n",
    "    return allpaths, list(nexplode['hyp'])\n",
    "\n",
    "# helper for exploding paths\n",
    "def explode_helper(prevpath, node, apaths, tok):\n",
    "    prevpath.append(node.token_idx)\n",
    "    if len(node.nextlist)==0:\n",
    "        #print(prevpath)\n",
    "        apaths.append(mb_tok.decode(prevpath))\n",
    "    else:\n",
    "        for n in node.nextlist:\n",
    "            explode_helper(prevpath, n, apaths, tok)\n",
    "    prevpath.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf817f12-1688-4f8d-b7ce-d708faa47479",
   "metadata": {},
   "outputs": [],
   "source": [
    "apths = []\n",
    "explode_helper([], pnodes[0][0], apths, xlm_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d9a91-b08d-4cfa-b83f-d85be7f0e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #1 - changing pos-ids with identical strings changes token scores for all things\n",
    "# score check (at token level) the best option, and the new option (should be same until they get to non-identical tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5c399b-daa4-45e6-a25a-08c45e05be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model\n",
    "def get_hyp_sco_verb(inphyp, posids=None):\n",
    "    \n",
    "    tokens = xlm_tok(inphyp, return_tensors='pt').to(device)\n",
    "    tokens = tokens.input_ids\n",
    "\n",
    "    if posids is None: \n",
    "        # have to add 2 because of how xlm roberta works\n",
    "        positionids = torch.arange(len(tokens[0])).unsqueeze(0).to(device)+2\n",
    "        print(positionids)\n",
    "\n",
    "    elif -1 in posids:\n",
    "        positionids = None\n",
    "    else:\n",
    "        # get token at the end\n",
    "        positionids = posids\n",
    "        #torch.tensor(posids+[posids[-1]+1]).unsqueeze(0).to(device)\n",
    "        print(positionids)\n",
    "\n",
    "    # get a causal mask to use\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(device)\n",
    "    \n",
    "    # this is a dummy string (using CQE model architecture which requires an \"input\" pass)\n",
    "    toked_inp = xlm_tok([\"noun\"], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # run inputs through the model\n",
    "    predout = encodemod(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    \n",
    "    tmppred = predout['score']\n",
    "    return tmppred, tokens, positionids, tmpmask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e03b8a07-e363-48a2-bf84-ebefdf889adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4, 5, 6, 7]], device='cuda:1')\n",
      "tensor([[2, 3, 4, 5, 7, 7]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# get scores for identical strings, with different posids\n",
    "sco_default = get_hyp_sco_verb(\"I am a bat\")\n",
    "modpos = sco_default[2]\n",
    "# change last position id\n",
    "modpos[0][4] = 7\n",
    "sco_mod = get_hyp_sco_verb(\"I am a bat\", modpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e293d8eb-9073-419a-ac72-cec7699bd33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000],\n",
      "         [0.0042],\n",
      "         [0.0042],\n",
      "         [0.0043],\n",
      "         [0.0508],\n",
      "         [0.0214]]], device='cuda:1', grad_fn=<DivBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0042],\n",
      "         [0.0042],\n",
      "         [0.0043],\n",
      "         [0.0557],\n",
      "         [0.0211]]], device='cuda:1', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# these two are actually fine. There is some sort of issue in no-recomb cases\n",
    "print(sco_default[0])\n",
    "print(sco_mod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30eb11-0386-4c83-9627-7f40977c07f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
