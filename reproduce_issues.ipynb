{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0727f9cc-c1ca-4442-ba0c-def3f55a5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 08:01:17.905942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-22 08:01:17.905962: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from encode_utils.rerank_data import rerank_dist, rerank_single\n",
    "from encode_utils.efficient_rerank import get_effrerank_model, run_comstyle\n",
    "from encode_utils.sco_funct import weightaddprob, default_scofunct\n",
    "from encode_utils.mt_scores import get_scores_auto\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc04c28-b8c7-46f4-8afc-ba8531f83650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Loading weights from /mnt/data1/prasann/latticegen/lattice-generation/COMET/lightning_logs/version_44/checkpoints/epoch=9-step=40000.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze embeds\n"
     ]
    }
   ],
   "source": [
    "# setup / load model\n",
    "encodemod = get_effrerank_model(\"noun\")\n",
    "xlm_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d9a91-b08d-4cfa-b83f-d85be7f0e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue #1 - changing pos-ids with identical strings changes token scores for all things\n",
    "# score check (at token level) the best option, and the new option (should be same until they get to non-identical tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5c399b-daa4-45e6-a25a-08c45e05be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token level scores from model\n",
    "def get_hyp_sco_verb(inphyp, posids=None):\n",
    "    \n",
    "    tokens = xlm_tok(inphyp, return_tensors='pt').to(device)\n",
    "    tokens = tokens.input_ids\n",
    "\n",
    "    if posids is None: \n",
    "        # have to add 2 because of how xlm roberta works\n",
    "        positionids = torch.arange(len(tokens[0])).unsqueeze(0).to(device)+2\n",
    "        print(positionids)\n",
    "\n",
    "    elif -1 in posids:\n",
    "        positionids = None\n",
    "    else:\n",
    "        # get token at the end\n",
    "        positionids = posids\n",
    "        #torch.tensor(posids+[posids[-1]+1]).unsqueeze(0).to(device)\n",
    "        print(positionids)\n",
    "\n",
    "    # get a causal mask to use\n",
    "    tmpmask = torch.tril(torch.ones(len(tokens[0]), len(tokens[0]))).unsqueeze(0).to(device)\n",
    "    \n",
    "    # this is a dummy string (using CQE model architecture which requires an \"input\" pass)\n",
    "    toked_inp = xlm_tok([\"noun\"], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # run inputs through the model\n",
    "    predout = encodemod(toked_inp.input_ids, toked_inp.attention_mask, tokens, positionids, \\\n",
    "        tmpmask)\n",
    "    \n",
    "    tmppred = predout['score']\n",
    "    return tmppred, tokens, positionids, tmpmask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03b8a07-e363-48a2-bf84-ebefdf889adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4, 5, 6, 7]], device='cuda:0')\n",
      "tensor([[2, 3, 4, 5, 6, 6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# get scores for identical strings, with different posids\n",
    "sco_default = get_hyp_sco_verb(\"I am a bat\")\n",
    "modpos = sco_default[2]\n",
    "# change last position id\n",
    "modpos[0][5] = 6\n",
    "sco_mod = get_hyp_sco_verb(\"I am a bat\", modpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e293d8eb-9073-419a-ac72-cec7699bd33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000],\n",
      "         [0.0042],\n",
      "         [0.0042],\n",
      "         [0.0043],\n",
      "         [0.0508],\n",
      "         [0.0214]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0049],\n",
      "         [0.0049],\n",
      "         [0.0050],\n",
      "         [0.0593],\n",
      "         [0.0138]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# these two should be the same except for the last 2 tokens, but they aren't\n",
    "print(sco_default[0])\n",
    "print(sco_mod[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
